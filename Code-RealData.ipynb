{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634fc3c0",
   "metadata": {},
   "source": [
    "# Stock Price Forecasting with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcca1c",
   "metadata": {},
   "source": [
    "## 0. Preliminary workings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292dd48",
   "metadata": {},
   "source": [
    "### 0.1 Uploading necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6bb1572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dguse\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\dguse\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\dguse\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import norm, t, uniform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from linearmodels.panel import PanelOLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "from numpy.linalg import svd\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from group_lasso import GroupLasso\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c074dc",
   "metadata": {},
   "source": [
    "### 0.2.1 Defining few functions (may not be relevant anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20228b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fw1(x):\n",
    "    # Find the maximum location of a vector\n",
    "    maximum = np.max(x)\n",
    "    p = np.where(x == maximum)[0]\n",
    "    if len(p) > 1:\n",
    "        p = p[0]\n",
    "    return p\n",
    "\n",
    "def pls(X, y, A):\n",
    "    \"\"\"\n",
    "    Partial Least Squares (PLS) regression\n",
    "    \n",
    "    Parameters:\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values.\n",
    "    A : int\n",
    "        Number of components.\n",
    "        \n",
    "    Returns:\n",
    "    B : array, shape (n_features, A)\n",
    "        Coefficients for each component.\n",
    "    \"\"\"\n",
    "    # Initialize arrays to store intermediate results\n",
    "    s = X.T.dot(y)\n",
    "    R = np.zeros((X.shape[1], A))\n",
    "    TT = np.zeros((X.shape[0], A))\n",
    "    P = np.zeros((X.shape[1], A))\n",
    "    U = np.zeros((X.shape[0], A))\n",
    "    V = np.zeros((X.shape[1], A))\n",
    "    B = np.zeros((X.shape[1], A))\n",
    "    Q = np.zeros((1, A))\n",
    "\n",
    "    # Iterate through each component\n",
    "    for i in range(A):\n",
    "        # Calculate loading vector for X and Y\n",
    "        q = s.T.dot(s)\n",
    "        r = s.dot(q)\n",
    "        t = X.dot(r)\n",
    "        t = t - np.mean(t)\n",
    "        normt = np.sqrt(t.T.dot(t))\n",
    "        t = t / normt\n",
    "        r = r / normt\n",
    "        p = X.T.dot(t)\n",
    "        q = y.T.dot(t)\n",
    "        u = y * q\n",
    "        v = p\n",
    "\n",
    "        # Calculate deflation\n",
    "        if i > 0:\n",
    "            v = v - V[:, :i+1].dot(V[:, :i+1].T.dot(p))\n",
    "            u = u - TT[:, :i+1].dot(TT[:, :i+1].T.dot(u))\n",
    "        v = v / np.sqrt(v.T.dot(v))\n",
    "        s = s - v.dot(v.T.dot(s))\n",
    "\n",
    "        # Store results for current component\n",
    "        R[:, i] = r\n",
    "        TT[:, i] = t\n",
    "        P[:, i] = p\n",
    "        U[:, i] = u\n",
    "        V[:, i] = v\n",
    "        Q[:, i] = q\n",
    "\n",
    "    # Reconstruct the coefficient matrix B\n",
    "    for i in range(A - 1):\n",
    "        C = R[:, :i+1].dot(Q[:, :i+1].T)\n",
    "        B[:, i+1] = C[:, 0]\n",
    "\n",
    "    return B\n",
    "\n",
    "def soft_threshodl(groups, nc, w, mu):\n",
    "    \"\"\"\n",
    "    Soft thresholding operator\n",
    "    \n",
    "    Parameters:\n",
    "    groups : int\n",
    "        Not used in the function, placeholder for the MATLAB code.\n",
    "    nc : int\n",
    "        Not used in the function, placeholder for the MATLAB code.\n",
    "    w : array-like\n",
    "        Input array.\n",
    "    mu : float\n",
    "        Threshold parameter.\n",
    "    \n",
    "    Returns:\n",
    "    val : array-like\n",
    "        Soft thresholded array.\n",
    "    \"\"\"\n",
    "    val = np.sign(w) * np.maximum(np.abs(w) - mu, 0)\n",
    "    return val\n",
    "\n",
    "def lossh(y, yhat, mu):\n",
    "    \"\"\"\n",
    "    Loss function for proximalH\n",
    "    \n",
    "    Parameters:\n",
    "    y : array-like\n",
    "        True target values.\n",
    "    yhat : array-like\n",
    "        Predicted values.\n",
    "    mu : float\n",
    "        Threshold parameter.\n",
    "    \n",
    "    Returns:\n",
    "    m : float\n",
    "        Loss value.\n",
    "    \"\"\"\n",
    "    r = np.abs(yhat - y)\n",
    "    l = np.zeros(len(r))\n",
    "    ind = (r > mu)\n",
    "    l[ind] = 2 * mu * r[ind] - mu * mu\n",
    "    ind = (r <= mu)\n",
    "    l[ind] = r[ind] * r[ind]\n",
    "    m = np.mean(l)\n",
    "    return m\n",
    "\n",
    "def f_gradh(w, X, y, mu):\n",
    "    \"\"\"\n",
    "    Gradient of the loss function for proximalH\n",
    "    \n",
    "    Parameters:\n",
    "    w : array-like\n",
    "        Coefficients.\n",
    "    X : array-like\n",
    "        Training data.\n",
    "    y : array-like\n",
    "        Target values.\n",
    "    mu : float\n",
    "        Threshold parameter.\n",
    "    \n",
    "    Returns:\n",
    "    grad : array-like\n",
    "        Gradient.\n",
    "    \"\"\"\n",
    "    r = np.dot(X, w) - y\n",
    "    ind0 = np.where(np.abs(r) <= mu)[0]\n",
    "    ind1 = np.where(r > mu)[0]\n",
    "    indf1 = np.where(r < -mu)[0]\n",
    "    grad = np.dot(X[ind0, :].T, np.dot(X[ind0, :], w) - y[ind0]) + mu * np.dot(X[ind1, :].T, np.ones(len(ind1))) - mu * np.dot(X[indf1, :].T, np.ones(len(indf1)))\n",
    "    return grad\n",
    "\n",
    "def proximalH(groups, nc, xtest, mtrain, ytest, w, X, y, mu, tol, L, l2, func):\n",
    "    \"\"\"\n",
    "    Proximal operator using accelerated proximal gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    groups : int\n",
    "        Not used in the function, placeholder for the MATLAB code.\n",
    "    nc : int\n",
    "        Not used in the function, placeholder for the MATLAB code.\n",
    "    xtest : array-like\n",
    "        Test data.\n",
    "    mtrain : float\n",
    "        Mean of the training target values.\n",
    "    ytest : array-like\n",
    "        Test target values.\n",
    "    w : array-like\n",
    "        Initial guess of the coefficients.\n",
    "    X : array-like\n",
    "        Training data.\n",
    "    y : array-like\n",
    "        Target values.\n",
    "    mu : float\n",
    "        Threshold parameter.\n",
    "    tol : float\n",
    "        Tolerance parameter for convergence.\n",
    "    L : float\n",
    "        Lipschitz constant.\n",
    "    l2 : float\n",
    "        Regularization parameter.\n",
    "    func : function\n",
    "        Soft thresholding function.\n",
    "    \n",
    "    Returns:\n",
    "    a : array-like\n",
    "        Final coefficients after proximal gradient descent.\n",
    "    \"\"\"\n",
    "    dim = X.shape[1]\n",
    "    max_iter = 3000\n",
    "    gamma = 1 / L\n",
    "    l1 = l2\n",
    "    v = w.copy()\n",
    "    yhatbig1 = np.dot(xtest, w) + mtrain\n",
    "    r20 = lossh(yhatbig1, ytest, mu)\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        vold = v.copy()\n",
    "        w_perv = w.copy()\n",
    "        w = v - gamma * f_gradh(v, X, y, mu)\n",
    "        mu1 = l1 * gamma\n",
    "        w = func(groups, nc, w, mu1)\n",
    "        v = w + t / (t + 3) * (w - w_perv)\n",
    "        \n",
    "        if np.sum((v - vold) ** 2) < (np.sum(vold ** 2) * tol) or np.sum(np.abs(v - vold)) == 0:\n",
    "            break\n",
    "    \n",
    "    return v\n",
    "\n",
    "def proximal(groups, nc, XX, XY, tol, L, l2, func):\n",
    "    '''\n",
    "    groups: Group membership of features\n",
    "    nc: Number of classes\n",
    "    XX: Matrix of features\n",
    "    XY: Matrix of labels\n",
    "    tol: Tolerance for convergence\n",
    "    L: Lipschitz constant\n",
    "    l2: L2 regularization parameter\n",
    "    func: Proximal operator function\n",
    "    '''\n",
    "    dim = XX.shape[0]\n",
    "    max_iter = 30000\n",
    "    gamma = 1 / L\n",
    "    l1 = l2\n",
    "    w = np.zeros(dim)\n",
    "    v = w.copy()\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        vold = v.copy()\n",
    "        w_prev = w.copy()\n",
    "        w = v - gamma * f_grad(XX, XY, v)\n",
    "        w = func(groups, nc, w, l1 * gamma)\n",
    "        v = w + t / (t + 3) * (w - w_prev)\n",
    "        if (np.sum(np.power(v - vold, 2)) < (np.sum(np.power(vold, 2)) * tol)) or (np.sum(np.abs(v - vold)) == 0):\n",
    "            break\n",
    "\n",
    "    return v\n",
    "\n",
    "def f_grad(XX, XY, w):\n",
    "    \"\"\"\n",
    "    Gradient of the objective function.\n",
    "\n",
    "    Parameters:\n",
    "    XX (array): Design matrix.\n",
    "    XY (array): Target values.\n",
    "    w (array): Coefficients.\n",
    "\n",
    "    Returns:\n",
    "    grad (array): Gradient.\n",
    "    \"\"\"\n",
    "    grad = np.dot(XX, w) - XY\n",
    "    return grad\n",
    "\n",
    "def soft_threshodr(groups, nc, w, mu):\n",
    "    \"\"\"\n",
    "    Soft thresholding function for ridge regularization.\n",
    "\n",
    "    Parameters:\n",
    "    groups (array): Groups.\n",
    "    nc (int): Number of components.\n",
    "    w (array): Coefficients.\n",
    "    mu (float): Threshold parameter.\n",
    "\n",
    "    Returns:\n",
    "    val (array): Updated coefficients after soft thresholding.\n",
    "    \"\"\"\n",
    "    val = w / (1 + mu)\n",
    "    return val\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cut_knots_degree2(x, n, th):\n",
    "    '''\n",
    "    x: Input matrix\n",
    "    n: Degree of the polynomial\n",
    "    th: Threshold values for knot points\n",
    "    '''\n",
    "    a, b = x.shape  # Get the dimensions of the input matrix\n",
    "    resultfinal = np.zeros((a, b * (n + 1)))  # Initialize the final result matrix\n",
    "\n",
    "    for i in range(b):\n",
    "        xcut = x[:, i]  # Extract a column of the input matrix\n",
    "        xcutnona = np.copy(xcut)  # Create a copy of the column\n",
    "        xcutnona[np.isnan(xcutnona)] = 0  # Replace NaN values with 0\n",
    "        index = ((1 - 1 * np.isnan(xcut)) == 1)  # Find non-NaN indices\n",
    "\n",
    "        t = th[:, i]  # Get the threshold values for this column\n",
    "\n",
    "        x1 = xcutnona\n",
    "        resultfinal[:, (n + 1) * i - n] = x1 - np.mean(x1)  # Store the original values\n",
    "\n",
    "        x1 = np.power(xcutnona - t[0], 2)\n",
    "        resultfinal[:, (n + 1) * i - n + 1] = x1 - np.mean(x1)  # Store the squared differences with the first threshold\n",
    "\n",
    "        for j in range(n - 1):\n",
    "            x1 = np.power(xcutnona - t[j + 1], 2) * (xcutnona >= t[j + 1])  # Calculate squared differences with subsequent thresholds\n",
    "            resultfinal[:, (n + 1) * i - n + 1 + j] = x1 - np.mean(x1)  # Store the result in the final matrix\n",
    "\n",
    "    return resultfinal\n",
    "\n",
    "def soft_threshode(groups, nc, w, mu):\n",
    "    '''\n",
    "    groups: Group membership of features\n",
    "    nc: Number of classes\n",
    "    w: Input vector\n",
    "    mu: Threshold parameter\n",
    "    '''\n",
    "    # Apply soft thresholding operation element-wise\n",
    "    val = np.sign(w) * np.maximum(np.abs(w) - 0.5 * mu, 0) / (1 + 0.5 * mu)\n",
    "    \n",
    "    return val\n",
    "\n",
    "def soft_threshodg(groups, nc, w, mu):\n",
    "    '''\n",
    "    groups: Group membership of features\n",
    "    nc: Number of classes\n",
    "    w: Input vector\n",
    "    mu: Threshold parameter\n",
    "    '''\n",
    "    w1 = np.copy(w)  # Create a copy of the input vector\n",
    "    for i in range(1, nc + 1):  # Iterate over the number of classes\n",
    "        ind = (groups == i)  # Identify indices corresponding to the current class\n",
    "        wg = w1[ind]  # Extract the values of w corresponding to the current class\n",
    "        nn = len(wg)  # Get the number of elements in the current class\n",
    "        n2 = np.sqrt(np.sum(wg ** 2))  # Calculate the L2 norm of the current class\n",
    "        if n2 <= mu:  # Check if the L2 norm is less than or equal to the threshold parameter mu\n",
    "            w1[ind] = np.zeros(nn)  # Set the values of w corresponding to the current class to zero\n",
    "        else:\n",
    "            w1[ind] = wg - mu * wg / n2  # Apply the soft thresholding operation to the values of w corresponding to the current class\n",
    "    return w1  # Return the updated vector w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ee9c1",
   "metadata": {},
   "source": [
    "### 0.2.2. New set of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40a87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "# Huber objective function\n",
    "def huber(actual, predicted, xi):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, diff**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss)\n",
    "\n",
    "# Scoring Function\n",
    "# out-of-sample R squared\n",
    "def R_oos(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted).flatten()\n",
    "    predicted = np.where(predicted<0,0,predicted)\n",
    "    return 1 - (np.dot((actual-predicted),(actual-predicted)))/(np.dot(actual,actual))\n",
    "\n",
    "# Validation Function\n",
    "def val_fun(model, params: dict, X_trn, y_trn, X_vld, y_vld, illustration=True, sleep=0, is_NN=False):\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            if is_NN:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            else:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            best_mod = mod\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {best_ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "        else:\n",
    "            time.sleep(sleep)\n",
    "            if is_NN:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            else:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_mod = mod\n",
    "                best_param = param\n",
    "    if illustration:\n",
    "        print('\\n'+'#'*60)\n",
    "        print('Tuning process finished!!!')\n",
    "        print(f'The best setting is: {best_param}')\n",
    "        print(f'with R2oos {best_ros*100:.2f}% on validation set.')\n",
    "        print('#'*60)\n",
    "    return best_mod\n",
    "    \n",
    "\n",
    "# Pairwise Comparison\n",
    "# Diebold-Mariano test statistics\n",
    "\n",
    "# Evaluation Output\n",
    "def evaluate(actual, predicted, insample=False):\n",
    "    if insample:\n",
    "        print('*'*15+'In-Sample Metrics'+'*'*15)\n",
    "        print(f'The in-sample R2 is {r2_score(actual,predicted)*100:.2f}%')\n",
    "        print(f'The in-sample MSE is {mean_squared_error(actual,predicted):.3f}')\n",
    "    else:\n",
    "        print('*'*15+'Out-of-Sample Metrics'+'*'*15)\n",
    "        print(f'The out-of-sample R2 is {R_oos(actual,predicted)*100:.2f}%')\n",
    "        print(f'The out-of-sample MSE is {mean_squared_error(actual,predicted):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1a2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCRegressor:\n",
    "    \n",
    "    def __init__(self,n_PCs=1,loss='mse'):\n",
    "        self.n_PCs = n_PCs\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        X = np.array(X)\n",
    "        N,K = X.shape\n",
    "        y = np.array(y_trn).reshape((N,1))\n",
    "        self.mu = np.mean(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.std(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.where(self.sigma==0,1,self.sigma)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        pca = PCA()\n",
    "        X = pca.fit_transform(X)[:,:self.n_PCs]\n",
    "        self.pc_coef = pca.components_.T[:,:self.n_PCs]\n",
    "        if self.loss == 'mse':\n",
    "            self.model = LinearRegression().fit(X,y)\n",
    "        else:\n",
    "            self.model = HuberRegressor().fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X = np.array(X)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        X = X @ self.pc_coef\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633450df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse\n",
    "def mse(actual, predicted):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    return np.mean(resid**2)\n",
    "\n",
    "# huber objective function\n",
    "def huber(actual, predicted, xi):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, diff**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss)\n",
    "\n",
    "# proximal operator\n",
    "def prox(theta,lmd,rho,gamma):\n",
    "    return (1/(1+lmd*gamma*rho))*softhred(theta,(1-rho)*gamma*lmd)\n",
    "\n",
    "# soft-thresholding operator\n",
    "def softhred(x,mu):\n",
    "    x = np.where(np.abs(x)<=mu, 0, x)\n",
    "    x = np.where((np.abs(x)>mu) & (x>0), x-mu, x)\n",
    "    x = np.where((np.abs(x)>mu) & (x<0), x+mu, x)\n",
    "    return x\n",
    "\n",
    "# penalized mse\n",
    "def mse_pnl(theta, X, y, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    resid = y - X@theta.reshape((K,1))\n",
    "    return np.mean(resid**2) + lmd*(1-rho)*np.sum(np.abs(theta)) + 0.5*lmd*rho*np.sum(theta**2)\n",
    "\n",
    "# penalized huber objective function\n",
    "def huber_pnl(theta, X, y, xi, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    resid = y - X@theta.reshape((K,1))\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, resid**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss) + lmd*(1-rho)*np.sum(np.abs(theta)) + 0.5*lmd*rho*np.sum(theta**2)\n",
    "\n",
    "# gradient of mse\n",
    "def grad_mse(theta, X, y, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    grad = (X.T @ (y - X@theta))/N + lmd*(1-rho)*np.where(theta>0,1,-1) + lmd*rho*theta\n",
    "    return grad.flatten()\n",
    "\n",
    "# gradient of huber loss\n",
    "def grad_huber(theta, X, y, xi, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    resid = y - X@theta\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    try:\n",
    "        grad_m = X[ind_m].T @ (y[ind_m] - X[ind_m]@theta)\n",
    "    except:\n",
    "        grad_m = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_u = 2*xi* X[ind_u].T@np.ones((len(ind_u[0]),1))\n",
    "    except:\n",
    "        grad_u = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_l = -2*xi* X[ind_l].T@np.ones((len(ind_l[0]),1))\n",
    "    except:\n",
    "        grad_l = np.zeros((K,1))\n",
    "    grad = (grad_m+grad_u+grad_l)/N + lmd*(1-rho)*np.where(theta>0,1,-1) + lmd*rho*theta\n",
    "    return grad.flatten()\n",
    "\n",
    "# Elastic Net\n",
    "class ENet:\n",
    "    \n",
    "    def __init__(\n",
    "        self, lmd, rho, xi=1.35, loss='huber', random_state=None, fit_intercept=True\n",
    "    ):\n",
    "        self.lmd = lmd\n",
    "        self.rho = rho\n",
    "        self.xi = xi\n",
    "        self.random_state = random_state\n",
    "        self.fit_intercept = fit_intercept\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "            \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        K = X.shape[1]\n",
    "        X = np.array(X)\n",
    "        N = len(y)\n",
    "        y = np.array(y).reshape((N,1))\n",
    "        if self.fit_intercept:\n",
    "            K += 1\n",
    "            X = np.concatenate((np.ones((N,1)),X),axis=1)\n",
    "        # initialize theta\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "            theta = np.random.uniform(K)\n",
    "        else:\n",
    "            theta = np.zeros(K)\n",
    "        \n",
    "        if self.loss == 'huber':\n",
    "            res = minimize(\n",
    "                partial(huber_pnl, X=X, y=y, xi=self.xi, lmd=self.lmd, rho=self.rho), theta,\n",
    "                method='nelder-mead',\n",
    "                #method='BFGS',\n",
    "                #jac = partial(grad_huber, X=X, y=y, xi=self.xi, lmd=self.lmd, rho=self.rho),\n",
    "                options = {'disp': True}\n",
    "            )\n",
    "        else:\n",
    "            res = minimize(\n",
    "                partial(mse_pnl, X=X, y=y, lmd=self.lmd, rho=self.rho), theta, \n",
    "                method='nelder-mead',\n",
    "                #method='BFGS',\n",
    "                #jac = partial(grad_mse, X=X, y=y, lmd=self.lmd, rho=self.rho),\n",
    "                options = {'disp': True}\n",
    "            )\n",
    "        \n",
    "        self.theta = res.x.reshape((K,1))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        N = X.shape[0]\n",
    "        if self.fit_intercept:\n",
    "            X = np.concatenate((np.ones((N,1)),X),axis=1)\n",
    "        return X@self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6224fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def SplineTransform(data,knots=3):\n",
    "    spline_data = pd.DataFrame(np.ones((data.shape[0],1)),index=data.index,columns=['const'])\n",
    "    for i in data.columns:\n",
    "        i_dat = data.loc[:,i]\n",
    "        i_sqr = i_dat**2\n",
    "        i_cut, bins = pd.cut(i_dat, 3, right=True, ordered=True, retbins=True)\n",
    "        i_dum = pd.get_dummies(i_cut)\n",
    "        for j in np.arange(knots):\n",
    "            i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
    "        i_dum.columns = [f\"{i}_{k}\" for k in np.arange(1,knots+1)]\n",
    "        spline_data = pd.concat((spline_data,i_dat,i_dum),axis=1)\n",
    "    return spline_data\n",
    "\n",
    "class GLMRegression:\n",
    "    \n",
    "    def __init__(self,knots=3,lmd=1e-4,l1_reg=1e-4,random_state=12308):\n",
    "        self.knots = knots\n",
    "        self.lmd = lmd\n",
    "        self.random_state = random_state\n",
    "        self.l1_reg = l1_reg\n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        groups = [0]+flatten([list(np.repeat(i,self.knots+1))[:] for i in np.arange(1,X.shape[1]+1)])\n",
    "        X = SplineTransform(X)\n",
    "        self.mod = GroupLasso(\n",
    "            groups=groups,group_reg=self.lmd,l1_reg=self.l1_reg,\n",
    "            fit_intercept=False,random_state=self.random_state\n",
    "        )\n",
    "        self.mod = self.mod.fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X = SplineTransform(X)\n",
    "        return self.mod.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e85a1",
   "metadata": {},
   "source": [
    "### 0.3 Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = pd.read_csv('datashare.csv')\n",
    "r1 = pd.read_csv('funny_data.csv')\n",
    "some = pd.read_csv('small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df154ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1['DATE'] = pd.to_datetime(c1['DATE'],format='%Y%m%d')+pd.offsets.MonthEnd(0)\n",
    "r1['DATE'] = r1['date']\n",
    "r1['DATE'] = pd.to_datetime(r1['DATE'])+pd.offsets.MonthEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.to_csv('./Simu/Simu_p50/c1_1.csv', index=False)\n",
    "r1.to_csv('./Simu/Simu_p50/r1_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06800a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = pd.read_csv(\"./Simu/Simu_p50/c1_1.csv\")\n",
    "r1 = pd.read_csv(\"./Simu/Simu_p50/r1_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a02118",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.Categorical(c1['permno'])\n",
    "c1 = c1.set_index([\"permno\", \"DATE\"])\n",
    "c1['stock'] = stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3f1370",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.Categorical(r1[\"PERMNO\"])\n",
    "r1 = r1.set_index([\"PERMNO\", \"date\"])\n",
    "r1['stock'] = stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "066ce44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.index.names = ['permno', 'date']\n",
    "r1.index.names = ['permno', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "538a81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = c1.sort_index(level=['permno', 'date'])\n",
    "r1 = r1.sort_index(level=['permno', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fad9afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_date_index = pd.to_datetime(r1.index.get_level_values(level = \"date\"))\n",
    "new_index = pd.MultiIndex.from_tuples(zip(r1['stock'], new_date_index), names=['permno', 'date'])\n",
    "r1.index = new_index\n",
    "\n",
    "new_date_index = pd.to_datetime(c1.index.get_level_values(level = \"date\"))\n",
    "new_index = pd.MultiIndex.from_tuples(zip(c1['stock'], new_date_index), names=['permno', 'date'])\n",
    "c1.index = new_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26985922",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93c4fa",
   "metadata": {},
   "source": [
    "#### Set the prediod of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb81ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting = pd.to_datetime('1980-12-31')\n",
    "ending = pd.to_datetime('1990-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5c067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_small = c1[(c1.index.get_level_values(level = \"date\") >= starting) & (c1.index.get_level_values(level = \"date\") <= ending)]\n",
    "r1_small = r1[(r1.index.get_level_values(level = \"date\") >= starting) & (r1.index.get_level_values(level = \"date\") <= ending)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b9e00",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc0571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = c1_small.merge(r1_small, left_index=True, right_on=['permno', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26c2add",
   "metadata": {},
   "source": [
    "### 0.3.1 Cleaning the data from bad tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f33242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1['TICKER'] = pd.Categorical(b1['TICKER'])\n",
    "additional = pd.DataFrame(b1['TICKER'].value_counts())\n",
    "bad_spisok = additional[additional['count'] <= 5].index\n",
    "\n",
    "b1 = b1[~b1['TICKER'].isin(bad_spisok)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b5da551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalIndex(['MVICV', 'CMPX', 'MRCH', 'BTEL', 'TSIN', 'NEG', 'TCS',\n",
       "                  'KECO', 'TSIX', 'TCO',\n",
       "                  ...\n",
       "                  'TAREF', 'LCCQ', 'DGII', 'SYT', 'PRPL', 'SYSE', 'BDN',\n",
       "                  'DIND', 'NNH', 'MOU'],\n",
       "                 categories=['A', 'AA', 'AAA', 'AAC', ..., 'ZUSC', 'ZY', 'ZYAD', 'ZYMS'], ordered=False, dtype='category', name='TICKER', length=774)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_spisok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a42e18",
   "metadata": {},
   "source": [
    "### 0.4 Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d45ddb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mvel1        380\n",
       "beta       57369\n",
       "betasq     57369\n",
       "chmom      48063\n",
       "dolvol     54582\n",
       "           ...  \n",
       "ASKHI          0\n",
       "PRC         2359\n",
       "VOL        40752\n",
       "RET         2360\n",
       "SPREAD    279887\n",
       "Length: 104, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = b1.drop(columns = ['stock_x', 'stock_y', 'DATE', 'TICKER', 'RETX'])\n",
    "b1['RET'] = pd.to_numeric(b1['RET'], errors = \"coerce\")\n",
    "\n",
    "characteristics = list(set(b1.columns).difference({'SHROUT','mve0','sic2','prc', \"SICCD\"}))\n",
    "b1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86ce2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in enumerate(b1['RET']):\n",
    "    if isinstance(value, str):\n",
    "        b1.at[index, 'RET'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4149e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.2 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fill na with cross-sectional median\n",
    "for ch in characteristics:\n",
    "     b1[ch] = b1.groupby('date')[ch].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37ed4ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sic2'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ch in characteristics:\n",
    "     b1[ch] = b1[ch].fillna(0)\n",
    "    \n",
    "b1.columns[b1.isnull().sum()!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72998ab",
   "metadata": {},
   "source": [
    "### 0.5 Adjusting Industrical Classification variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea4ad17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sic_dummies(data_ch):\n",
    "    sic_dummies = pd.get_dummies(b1['sic2'].fillna(999).astype(int),prefix='sic').drop('sic_999',axis=1)\n",
    "    b1_d = pd.concat([b1,sic_dummies],axis=1)\n",
    "    b1_d.drop(['sic2'],inplace=True,axis=1)\n",
    "    b1_d.drop(['SICCD'],inplace=True,axis=1)\n",
    "    return b1_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34475b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    101\n",
       "bool        71\n",
       "int64        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_d = get_sic_dummies(b1)\n",
    "b1_d.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2923801",
   "metadata": {},
   "source": [
    "### 0.6 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61c28249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "index_co = b1_d.index\n",
    "features = list(set(b1_d.columns).difference({'permno','DATE','RET'}))\n",
    "X = MinMaxScaler((-1,1)).fit_transform(b1_d[features])\n",
    "X = pd.DataFrame(X, columns=features, index=index_co)\n",
    "y = b1_d['RET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f3c5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stdt_vld = starting + (ending - starting)/3\n",
    "stdt_tst = starting + 2*(ending - starting)/3\n",
    "\n",
    "X_trn = X[X.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "y_trn = y[y.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "\n",
    "X_vld = X[(X.index.get_level_values(level = \"date\") >= stdt_vld) & (X.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "y_vld = y[(y.index.get_level_values(level = \"date\") >= stdt_vld) & (y.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "\n",
    "X_tst = X[X.index.get_level_values(level = \"date\") >= stdt_tst]\n",
    "y_tst = y[y.index.get_level_values(level = \"date\") >= stdt_tst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519147f4",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068809c5",
   "metadata": {},
   "source": [
    "## 1.0 Linear Model (Old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "890cf012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                    RET   R-squared:                        0.0133\n",
      "Estimator:                   PanelOLS   R-squared (Between):             -0.0151\n",
      "No. Observations:              471892   R-squared (Within):               0.0145\n",
      "Date:                Wed, Apr 17 2024   R-squared (Overall):              0.0133\n",
      "Time:                        20:53:16   Log-likelihood                 1.813e+05\n",
      "Cov. Estimator:            Unadjusted                                           \n",
      "                                        F-statistic:                      62.758\n",
      "Entities:                       11089   P-value                           0.0000\n",
      "Avg Obs:                       42.555   Distribution:              F(101,471790)\n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       76.000   F-statistic (robust):             62.758\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                      76   Distribution:              F(101,471790)\n",
      "Avg Obs:                       6209.1                                           \n",
      "Min Obs:                       4739.0                                           \n",
      "Max Obs:                       7150.0                                           \n",
      "                                                                                \n",
      "                                Parameter Estimates                                \n",
      "===================================================================================\n",
      "                 Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-----------------------------------------------------------------------------------\n",
      "const               0.0861     0.0041     20.965     0.0000      0.0780      0.0941\n",
      "beta               -0.0003     0.0013    -0.2598     0.7950     -0.0030      0.0023\n",
      "rd_mve              0.0465     0.0065     7.1697     0.0000      0.0338      0.0592\n",
      "stdcf              -0.0002     0.0003    -0.4907     0.6236     -0.0008      0.0005\n",
      "convind            -0.0039     0.0008    -5.0178     0.0000     -0.0054     -0.0024\n",
      "stdacc              0.0003     0.0004     0.6614     0.5083     -0.0005      0.0011\n",
      "tang                0.0037     0.0030     1.2159     0.2240     -0.0023      0.0096\n",
      "rd_sale             0.0038     0.0009     4.0139     0.0001      0.0019      0.0056\n",
      "mom6m               0.0112     0.0020     5.7340     0.0000      0.0074      0.0150\n",
      "invest             -0.0117     0.0029    -4.0027     0.0001     -0.0175     -0.0060\n",
      "secured            -0.0055     0.0008    -7.0651     0.0000     -0.0070     -0.0039\n",
      "orgcap              0.1935     0.0347     5.5789     0.0000      0.1255      0.2615\n",
      "pchquick            0.0020     0.0013     1.5670     0.1171     -0.0005      0.0045\n",
      "chatoia            -0.0011     0.0015    -0.7655     0.4440     -0.0040      0.0018\n",
      "std_dolvol          0.0021     0.0008     2.5051     0.0122      0.0004      0.0037\n",
      "sgr              2.951e-05     0.0008     0.0387     0.9691     -0.0015      0.0015\n",
      "quick               0.0009     0.0004     2.3927     0.0167      0.0002      0.0016\n",
      "dolvol             -0.0034     0.0002    -16.672     0.0000     -0.0038     -0.0030\n",
      "pctacc          -9.798e-05   5.57e-05    -1.7591     0.0786     -0.0002   1.119e-05\n",
      "chmom               0.0020     0.0011     1.9191     0.0550  -4.335e-05      0.0041\n",
      "pchsaleinv          0.0015     0.0009     1.8116     0.0700     -0.0001      0.0032\n",
      "SPREAD              0.0050     0.0005     10.990     0.0000      0.0041      0.0059\n",
      "ill              8.419e-07     17.062  4.935e-08     1.0000     -33.441      33.441\n",
      "pchsale_pchxsga    -0.0012     0.0010    -1.1663     0.2435     -0.0031      0.0008\n",
      "pchcapx_ia       2.952e-05  1.719e-05     1.7172     0.0859  -4.174e-06   6.322e-05\n",
      "saleinv          3.139e-05  9.587e-06     3.2742     0.0011    1.26e-05   5.018e-05\n",
      "grcapx           8.257e-05     0.0001     0.7590     0.4478     -0.0001      0.0003\n",
      "retvol              0.0146     0.0287     0.5074     0.6119     -0.0417      0.0709\n",
      "roavol             -0.0234     0.0239    -0.9782     0.3280     -0.0702      0.0234\n",
      "mom1m              -0.0002     0.0022    -0.1068     0.9149     -0.0046      0.0042\n",
      "lgr                -0.0002     0.0005    -0.4884     0.6253     -0.0012      0.0007\n",
      "chcsho              0.0020     0.0009     2.2599     0.0238      0.0003      0.0038\n",
      "cinvest             0.0006     0.0008     0.6598     0.5094     -0.0011      0.0022\n",
      "turn               -0.0066     0.0007    -9.2890     0.0000     -0.0080     -0.0052\n",
      "idiovol            -0.1671     0.0120    -13.906     0.0000     -0.1906     -0.1435\n",
      "egr                 0.0003     0.0007     0.3979     0.6907     -0.0010      0.0016\n",
      "indmom             -0.0234     0.0010    -22.737     0.0000     -0.0254     -0.0214\n",
      "roic                0.0022     0.0011     2.0568     0.0397      0.0001      0.0042\n",
      "hire                0.0023     0.0013     1.7606     0.0783     -0.0003      0.0048\n",
      "salerec          4.628e-05  1.253e-05     3.6943     0.0002   2.173e-05   7.084e-05\n",
      "salecash        -1.992e-06  2.364e-06    -0.8424     0.3995  -6.626e-06   2.642e-06\n",
      "tb                 -0.0002     0.0002    -0.9237     0.3556     -0.0005      0.0002\n",
      "absacc             -0.0018     0.0042    -0.4271     0.6693     -0.0101      0.0065\n",
      "rd                  0.0007     0.0010     0.7206     0.4712     -0.0012      0.0026\n",
      "chempia            -0.0010     0.0008    -1.2478     0.2121     -0.0026      0.0006\n",
      "mom36m              0.0015     0.0004     4.1291     0.0000      0.0008      0.0022\n",
      "herf                0.0055     0.0023     2.3515     0.0187      0.0009      0.0101\n",
      "rsup               -0.0082     0.0017    -4.7386     0.0000     -0.0117     -0.0048\n",
      "chinv               0.0109     0.0072     1.5237     0.1276     -0.0031      0.0250\n",
      "cash               -0.0067     0.0027    -2.4198     0.0155     -0.0120     -0.0013\n",
      "aeavol              0.0011     0.0002     6.1332     0.0000      0.0007      0.0014\n",
      "mom12m           6.065e-06     0.0010     0.0061     0.9952     -0.0020      0.0020\n",
      "ep                  0.0051     0.0013     3.7984     0.0001      0.0025      0.0078\n",
      "mvel1           -2.392e-09  1.999e-10    -11.970     0.0000  -2.784e-09  -2.001e-09\n",
      "pchdepr            -0.0002     0.0006    -0.2979     0.7658     -0.0014      0.0010\n",
      "maxret             -0.0799     0.0098    -8.1853     0.0000     -0.0990     -0.0607\n",
      "cashdebt           -0.0005     0.0003    -1.4287     0.1531     -0.0011      0.0002\n",
      "PERMCO           1.208e-07  3.362e-08     3.5927     0.0003   5.489e-08   1.867e-07\n",
      "realestate          0.0250     0.0019     13.265     0.0000      0.0213      0.0287\n",
      "chpmia             -0.0001     0.0001    -1.2281     0.2194     -0.0003   7.517e-05\n",
      "HSICMG             -0.0012  5.153e-05    -23.697     0.0000     -0.0013     -0.0011\n",
      "cfp                 0.0013     0.0002     5.2174     0.0000      0.0008      0.0017\n",
      "chtx               -0.0389     0.0298    -1.3039     0.1923     -0.0974      0.0196\n",
      "ear                 0.0072     0.0057     1.2598     0.2077     -0.0040      0.0185\n",
      "sp                  0.0005     0.0001     5.0967     0.0000      0.0003      0.0007\n",
      "ps                  0.0008     0.0002     3.5655     0.0004      0.0004      0.0013\n",
      "pchgm_pchsale      -0.0009     0.0003    -2.6896     0.0072     -0.0016     -0.0002\n",
      "roaq                0.0519     0.0222     2.3385     0.0194      0.0084      0.0954\n",
      "ms                 -0.0008     0.0002    -3.2610     0.0011     -0.0013     -0.0003\n",
      "BIDLO              -0.0005  2.741e-05    -19.960     0.0000     -0.0006     -0.0005\n",
      "cfp_ia          -4.506e-05  2.141e-05    -2.1044     0.0353  -8.702e-05  -3.092e-06\n",
      "dy                  0.0145     0.0091     1.5879     0.1123     -0.0034      0.0325\n",
      "mve_ia           -3.07e-06  4.238e-07    -7.2450     0.0000  -3.901e-06   -2.24e-06\n",
      "VOL              5.696e-07  1.373e-08     41.497     0.0000   5.427e-07   5.965e-07\n",
      "lev                 0.0004   7.54e-05     4.9344     0.0000      0.0002      0.0005\n",
      "PRC                 0.0004  2.536e-05     14.999     0.0000      0.0003      0.0004\n",
      "bm                  0.0039     0.0004     9.8409     0.0000      0.0031      0.0047\n",
      "pricedelay         -0.0003     0.0003    -1.1677     0.2429     -0.0009      0.0002\n",
      "bm_ia              -0.0003     0.0001    -2.2971     0.0216     -0.0005  -4.245e-05\n",
      "pchsale_pchrect     0.0005     0.0005     1.0013     0.3167     -0.0005      0.0014\n",
      "nincr              -0.0007     0.0003    -2.9334     0.0034     -0.0012     -0.0002\n",
      "operprof            0.0026     0.0010     2.6775     0.0074      0.0007      0.0045\n",
      "zerotrade          -0.0014  8.293e-05    -16.394     0.0000     -0.0015     -0.0012\n",
      "gma                 0.0046     0.0016     2.9222     0.0035      0.0015      0.0076\n",
      "acc                 0.0080     0.0040     2.0181     0.0436      0.0002      0.0157\n",
      "currat             -0.0003     0.0003    -0.9311     0.3518     -0.0010      0.0004\n",
      "roeq                0.0279     0.0077     3.6184     0.0003      0.0128      0.0431\n",
      "pchcurrat          -0.0037     0.0016    -2.3467     0.0189     -0.0067     -0.0006\n",
      "age              3.041e-05  5.374e-05     0.5660     0.5714  -7.491e-05      0.0001\n",
      "grltnoa             0.0032     0.0036     0.9091     0.3633     -0.0038      0.0102\n",
      "sin                 0.0049     0.0033     1.4921     0.1357     -0.0015      0.0114\n",
      "agr                 0.0023     0.0015     1.5467     0.1219     -0.0006      0.0053\n",
      "betasq             -0.0004     0.0005    -0.7731     0.4395     -0.0015      0.0006\n",
      "std_turn            0.0008  9.758e-05     8.1698     0.0000      0.0006      0.0010\n",
      "pchsale_pchinvt    -0.0016     0.0008    -2.1641     0.0305     -0.0031     -0.0002\n",
      "depr               -0.0001     0.0023    -0.0580     0.9538     -0.0046      0.0043\n",
      "ASKHI               0.0002  2.648e-05     6.9611     0.0000      0.0001      0.0002\n",
      "securedind          0.0067     0.0006     12.042     0.0000      0.0056      0.0078\n",
      "divi               -0.0010     0.0019    -0.5155     0.6062     -0.0047      0.0028\n",
      "cashpr              -2e-05  5.099e-06    -3.9229     0.0001      -3e-05  -1.001e-05\n",
      "baspread            0.0350     0.0031     11.358     0.0000      0.0289      0.0410\n",
      "divo                0.0066     0.0018     3.6553     0.0003      0.0030      0.0101\n",
      "===================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PooledOLS - this is what I need to do. Plus add the Huber Func\n",
    "exog_vars = list(set(b1.columns).difference({'sic2','RET', \"SICCD\"}))\n",
    "exog = sm.add_constant(b1[exog_vars])\n",
    "mod = PanelOLS(b1.RET, exog, check_rank=False)\n",
    "pooled_res = mod.fit()\n",
    "print(pooled_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f7994e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some stocks and check how they behave. Check the data consostency and clear bad stocks (write them down in additional list.\n",
    "# Create a table with each bad ticker and associated problem (and check the paper for the advice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81854a",
   "metadata": {},
   "source": [
    "## 1.1 Linear Models (New)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4ca6c",
   "metadata": {},
   "source": [
    "### 1.1.1 OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a9854a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 4.80%\n",
      "The in-sample MSE is 0.027\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -0.05%\n",
      "The out-of-sample MSE is 3693715471123923456.000\n",
      "CPU times: total: 12.3 s\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Simple OLS with MSE as a loss function\n",
    "\n",
    "# OLS with all features\n",
    "OLS = LinearRegression().fit(X_trn,y_trn)\n",
    "evaluate(y_trn, OLS.predict(X_trn), insample=True)\n",
    "evaluate(y_tst, OLS.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b428ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 2.08%\n",
      "The in-sample MSE is 0.028\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -0.18%\n",
      "The out-of-sample MSE is 0.027\n",
      "CPU times: total: 38.1 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### OLS with a Huber loss function\n",
    "\n",
    "# OLS by Huber robust objective function with all features\n",
    "epsilon = np.max(((y_trn-OLS.predict(X_trn)).quantile(.999),1))\n",
    "OLS_H = HuberRegressor(epsilon=epsilon).fit(X_trn,y_trn)\n",
    "evaluate(y_trn, OLS_H.predict(X_trn), insample=True)\n",
    "evaluate(y_tst, OLS_H.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23de4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS with preselected size, bm, and momentum covariates\n",
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']\n",
    "OLS_3 = LinearRegression().fit(X_trn[features_3],y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e53195",
   "metadata": {},
   "source": [
    "### 1.1.2 PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22522759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'n_components': 1} finished.\n",
      "with out-of-sample R squared on validation set: -1.59834%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 5} finished.\n",
      "with out-of-sample R squared on validation set: -5.27974%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 10} finished.\n",
      "with out-of-sample R squared on validation set: -5.46342%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 50} finished.\n",
      "with out-of-sample R squared on validation set: -5.43570%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'n_components': 1}\n",
      "with R2oos -1.60% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 26.2 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {'n_components': [1, 5, 10, 50]}\n",
    "PLS = val_fun(PLSRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a36089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 2.17%\n",
      "The in-sample MSE is 0.028\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -2.34%\n",
      "The out-of-sample MSE is 0.027\n",
      "CPU times: total: 422 ms\n",
      "Wall time: 269 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pls_pred_is = PLS.predict(X_trn)\n",
    "pls_pred_os = PLS.predict(X_tst)\n",
    "evaluate(y_trn, pls_pred_is, insample=True) \n",
    "evaluate(y_tst, pls_pred_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0c721",
   "metadata": {},
   "source": [
    "### 1.1.3 PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09ec5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'loss': 'mse', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: -0.62917%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: -0.61700%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: -0.66151%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: -0.66961%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: -0.80923%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: -1.97006%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: -0.06920%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: 0.19665%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: 0.20444%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: 0.16046%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: 0.14019%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 0.03526%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'loss': 'huber', 'n_PCs': 5}\n",
      "with R2oos 0.20% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 3min 59s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {'n_PCs':[1,3,5,7,10,50],'loss':['mse','huber']}\n",
    "PCR = val_fun(PCRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,sleep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3921e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -0.94%\n",
      "The in-sample MSE is 0.029\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.21%\n",
      "The out-of-sample MSE is 0.027\n",
      "CPU times: total: 984 ms\n",
      "Wall time: 485 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, PCR.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, PCR.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651805eb",
   "metadata": {},
   "source": [
    "### 1.1.4 Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d36c2076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.027460\n",
      "         Iterations: 21202\n",
      "         Function evaluations: 22509\n",
      "CPU times: total: 43min 35s\n",
      "Wall time: 21min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "random.seed(12308)\n",
    "EN_my_mse_rdm = ENet(lmd=.01,rho=.5,loss='huber').fit(X_trn,y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a607409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.71%\n",
      "The in-sample MSE is 0.028\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -0.26%\n",
      "The out-of-sample MSE is 0.026\n",
      "CPU times: total: 281 ms\n",
      "Wall time: 283 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_my_mse_rdm.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, EN_my_mse_rdm.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441ee80",
   "metadata": {},
   "source": [
    "### 1.1.5 GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "977532cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: -3.05517%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: -3.08258%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.1}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 20min 28s\n",
      "Wall time: 15min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'knots':[3],\n",
    "    'lmd':[1e-4,1e-1],#list(np.linspace(1e-4,1e-1,10)),\n",
    "    'l1_reg':[1e-4,0]\n",
    "}\n",
    "GLM = val_fun(GLMRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d7b6c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -0.83%\n",
      "The in-sample MSE is 0.028\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.00%\n",
      "The out-of-sample MSE is 0.026\n",
      "CPU times: total: 1min 57s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, GLM.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, GLM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c13f2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns = [\"OLS + H\", \"OLS-3 + H\", \"PCR\", \"PLS\", \"Enet + H\", \"GLM + H\", \"RF\", \"GBRT + H\"], data=np.zeros((3,8)))\n",
    "output.index = [\"All\", \"Top 1000\", \"Bottom 1000\"]\n",
    "output.iloc[0,0] =r2_score(y_tst, OLS_H.predict(X_tst))\n",
    "output.iloc[0,2] = r2_score(y_tst, PCR.predict(X_tst))\n",
    "output.iloc[0,3] = r2_score(y_tst, PLS.predict(X_tst))\n",
    "output.iloc[0,4] = r2_score(y_tst, EN_my_mse_rdm.predict(X_tst))\n",
    "output.iloc[0,5] = r2_score(y_tst, GLM.predict(X_tst))\n",
    "\n",
    "output.iloc[0,1] = r2_score(y_tst, OLS_3.predict(X_tst[features_3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417f788",
   "metadata": {},
   "source": [
    "## 1.2 Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c5f113",
   "metadata": {},
   "source": [
    "### 1.2.0 Additional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efb18ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huber loss function for customized objective function\n",
    "\n",
    "# gradient of huber loss with respect to y_pred\n",
    "def grad_huber_obj(y_true, y_pred):\n",
    "    xi = 1.35 \n",
    "    # Though I do not want to make it hard-coded, lightgbm, behind the scene, evaluates the # of parameters\n",
    "    # of the objective function first, then pass according # of parameters. I tried to use partial to set \n",
    "    # the value of xi. It did not work.\n",
    "    # I refer the readers to the source code to have a better understanding of the issue:\n",
    "    # (https://github.com/microsoft/LightGBM/blob/master/python-package/lightgbm/sklearn.py)\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    N = len(y_true)\n",
    "    resid = y_true - y_pred\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    grad = np.zeros(N)\n",
    "    try:\n",
    "        grad[ind_m] = (-2*(y_true-y_pred))[ind_m]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grad[ind_u] = np.repeat(2*xi,N)[ind_u]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grad[ind_l] = np.repeat(-2*xi,N)[ind_l]\n",
    "    except:\n",
    "        pass\n",
    "    return grad/N\n",
    "\n",
    "# hessian of huber loss with respect to y_pred\n",
    "def hess_huber_obj(y_true, y_pred):\n",
    "    xi = 1.35\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    N = len(y_true)\n",
    "    resid = y_true - y_pred\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    hess = np.zeros(N)\n",
    "    try:\n",
    "        hess[ind_m] = np.repeat(2,N)[ind_m]\n",
    "    except:\n",
    "        pass\n",
    "    return hess/N\n",
    "\n",
    "# huber loss for lgbm\n",
    "def huber_obj(y_true, y_pred):\n",
    "    grad = grad_huber_obj(y_true, y_pred)\n",
    "    hess = hess_huber_obj(y_true, y_pred)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301dfe01",
   "metadata": {},
   "source": [
    "### 1.2.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85ea33ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.31521%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.21073%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.33682%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.82757%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.31735%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.14168%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'max_depth': 6, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308}\n",
      "with R2oos 1.32% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3, 6],\n",
    "    'max_features': [30, 50, 100],\n",
    "    'random_state': [12308]\n",
    "}\n",
    "RF = val_fun(RandomForestRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c706d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 15.23%\n",
      "The in-sample MSE is 0.024\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 1.46%\n",
      "The out-of-sample MSE is 0.026\n",
      "CPU times: total: 3.44 s\n",
      "Wall time: 3.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, RF.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, RF.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fd448",
   "metadata": {},
   "source": [
    "### 1.2.2 Gradient Boosting (XGBRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33014eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.46913%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -2.41942%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.87788%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.40689%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.13659%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.27731%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.58410%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -2.41942%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 3.88163%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.40689%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 4.56988%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.27731%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 4.40415%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.23562%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 5.74071%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.23562%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 6.35308%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.23562%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 14.06139%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.23562%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 17.90736%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.23562%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 19.82887%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.23562%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308}\n",
      "with R2oos 19.83% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective':[None, huber_obj],\n",
    "    'max_depth':[1,2],\n",
    "    'n_estimators':[500, 800, 1000],\n",
    "    'random_state':[12308],\n",
    "    'learning_rate':[.01,.1]\n",
    "}\n",
    "LGBM = val_fun(XGBRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81190723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 40.88%\n",
      "The in-sample MSE is 0.017\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 14.99%\n",
      "The out-of-sample MSE is 0.021\n",
      "CPU times: total: 5.56 s\n",
      "Wall time: 734 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, LGBM.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, LGBM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b48c057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[0,6] = r2_score(y_tst, RF.predict(X_tst))\n",
    "output.iloc[0,7] = r2_score(y_tst, LGBM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbed12f",
   "metadata": {},
   "source": [
    "## 1.3 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56a86d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized metrics\n",
    "# out-of-sample r squared for keras\n",
    "def R_oos_tf(y_true, y_pred):\n",
    "    resid = tf.square(y_true-y_pred)\n",
    "    denom = tf.square(y_true)\n",
    "    return 1 - tf.divide(tf.reduce_mean(resid),tf.reduce_mean(denom))\n",
    "\n",
    "# data standardization\n",
    "# please standardize the data if BatchNormalization is not used\n",
    "def standardize(X_trn, X_vld, X_tst):\n",
    "    mu_trn = np.mean(np.array(X_trn),axis=0).reshape((1,X_trn.shape[1]))\n",
    "    sigma_trn = np.std(np.array(X_trn),axis=0).reshape((1,X_trn.shape[1]))\n",
    "\n",
    "    X_trn_std = (np.array(X_trn)-mu_trn)/sigma_trn\n",
    "    X_vld_std = (np.array(X_vld)-mu_trn)/sigma_trn\n",
    "    X_tst_std = (np.array(X_tst)-mu_trn)/sigma_trn\n",
    "    return X_trn_std, X_vld_std, X_tst_std\n",
    "\n",
    "# NN class\n",
    "class NN:\n",
    "    \n",
    "    def __init__(\n",
    "        self, n_layers=1, loss='mse', l1=1e-5, l2=0, learning_rate=.01, BatchNormalization=True, patience=5,\n",
    "        epochs=100, batch_size=3000, verbose=1, random_state=12308, monitor='val_R_oos_tf', base_neurons=5\n",
    "    ):\n",
    "        self.n_layers = n_layers\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.learning_rate = learning_rate\n",
    "        self.BatchNormalization = BatchNormalization\n",
    "        self.patience = patience\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.monitor = monitor\n",
    "        self.base_neurons = base_neurons\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X_trn, y_trn, X_vld, y_vld):\n",
    "        # fix random seed for reproducibility\n",
    "        random.seed(self.random_state)\n",
    "        np.random.seed(self.random_state)\n",
    "        tf.random.set_seed(self.random_state)\n",
    "        \n",
    "        # model construction\n",
    "        mod = Sequential()\n",
    "        mod.add(Input(shape=(X_trn.shape[1],)))\n",
    "        \n",
    "        for i in np.arange(self.n_layers,0,-1):\n",
    "            if self.n_layers>self.base_neurons:\n",
    "                if self.n_layers == i:\n",
    "                    mod.add(Dense(2**i, activation='relu'))\n",
    "                else:\n",
    "                    mod.add(Dense(2**i, activation='relu', kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "            else:\n",
    "                if self.n_layers == i:\n",
    "                    mod.add(Dense(2**(self.base_neurons-(self.n_layers-i)), activation='relu'))\n",
    "                else:\n",
    "                    mod.add(Dense(2**(self.base_neurons-(self.n_layers-i)), \n",
    "                                  activation='relu', kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "            if self.BatchNormalization:\n",
    "                mod.add(BatchNormalization())\n",
    "        \n",
    "        mod.add(Dense(1, kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "        \n",
    "        # early stopping\n",
    "        earlystop = tf.keras.callbacks.EarlyStopping(monitor=self.monitor, patience=self.patience, mode = 'max')\n",
    "\n",
    "        # Adam solver\n",
    "        opt = Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # compile the model\n",
    "        mod.compile(loss=self.loss,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[R_oos_tf])\n",
    "\n",
    "        # fit the model\n",
    "        mod.fit(X_trn, np.array(y_trn).reshape((len(y_trn),1)), epochs=self.epochs, batch_size=self.batch_size, \n",
    "                callbacks=[earlystop], verbose=self.verbose, \n",
    "                validation_data=(X_vld,np.array(y_vld).reshape((len(y_vld),1))))\n",
    "        \n",
    "        self.model = mod\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=self.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41711c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.01158%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.28197%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.06509%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.37637%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.01207%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.08832%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.00263%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.44890%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 4min 47s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN1-Regression-[32(relu)-1(linear)]\n",
    "\n",
    "params = {\n",
    "    'n_layers': [1],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf'],\n",
    "}\n",
    "NN1 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d362e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -220.98727%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -4.04173%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -14.85559%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.61316%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -108.61961%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -2.79483%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -48.54019%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -70.14918%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos -1.61% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 5min 17s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [2],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN2 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "333b8520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -8.58787%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -5.80852%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.18354%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -10.64993%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -12.83201%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.32828%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.07567%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.43289%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 0.08% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 5min 42s\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [3],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN3 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05ebe489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -17.68690%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -10.78903%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.63800%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -3.99453%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -11.47770%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.82364%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.09036%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.65257%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos -1.09% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 6min 16s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [4],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN4 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9184fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.67422%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.03517%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.13011%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -2.56913%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.14736%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.78092%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.32471%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.09688%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 2694, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos -0.10% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 6min 38s\n",
      "Wall time: 4min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [5],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN5 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76f20bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['NN1'] = np.array([0,0,0])\n",
    "output['NN2'] = np.array([0,0,0])\n",
    "output['NN3'] = np.array([0,0,0])\n",
    "output['NN4'] = np.array([0,0,0])\n",
    "output['NN5'] = np.array([0,0,0])\n",
    "output.iloc[0,8] = r2_score(y_tst, NN1.predict(X_tst))\n",
    "output.iloc[0,9] = r2_score(y_tst, NN2.predict(X_tst))\n",
    "output.iloc[0,10] = r2_score(y_tst, NN3.predict(X_tst))\n",
    "output.iloc[0,11] = r2_score(y_tst, NN4.predict(X_tst))\n",
    "output.iloc[0,12] = r2_score(y_tst, NN5.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511b89a",
   "metadata": {},
   "source": [
    "## 1.3 Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d3c079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_top = b1_d.sort_values('mvel1',ascending=False).groupby('date').head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01f9800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_co = b1_top.index\n",
    "features = list(set(b1_top.columns).difference({'permno','DATE','RET'}))\n",
    "X = MinMaxScaler((-1,1)).fit_transform(b1_top[features])\n",
    "X = pd.DataFrame(X, columns=features, index=index_co)\n",
    "y = b1_top['RET']\n",
    "\n",
    "stdt_vld = starting + (ending - starting)/3\n",
    "stdt_tst = starting + 2*(ending - starting)/3\n",
    "\n",
    "X_trn = X[X.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "y_trn = y[y.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "\n",
    "X_vld = X[(X.index.get_level_values(level = \"date\") >= stdt_vld) & (X.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "y_vld = y[(y.index.get_level_values(level = \"date\") >= stdt_vld) & (y.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "\n",
    "X_tst = X[X.index.get_level_values(level = \"date\") >= stdt_tst]\n",
    "y_tst = y[y.index.get_level_values(level = \"date\") >= stdt_tst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c698b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'n_components': 1} finished.\n",
      "with out-of-sample R squared on validation set: -0.51006%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 5} finished.\n",
      "with out-of-sample R squared on validation set: -10.54365%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 10} finished.\n",
      "with out-of-sample R squared on validation set: -13.31591%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 50} finished.\n",
      "with out-of-sample R squared on validation set: -12.98592%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'n_components': 1}\n",
      "with R2oos -0.51% on validation set.\n",
      "############################################################\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.25155%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: -0.01178%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: -0.29222%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: -0.24719%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: -0.21309%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: -1.88298%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.60201%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: 0.66623%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: 0.33172%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: 0.33389%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: 0.37343%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: -0.69409%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'loss': 'huber', 'n_PCs': 3}\n",
      "with R2oos 0.67% on validation set.\n",
      "############################################################\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.009447\n",
      "         Iterations: 11051\n",
      "         Function evaluations: 11951\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: -2.78031%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: -2.77426%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.1}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "epsilon = np.max(((y_trn-OLS.predict(X_trn)).quantile(.999),1))\n",
    "OLS_H = HuberRegressor(epsilon=epsilon).fit(X_trn,y_trn)\n",
    "\n",
    "params = {'n_components': [1, 5, 10, 50]}\n",
    "PLS = val_fun(PLSRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)\n",
    "\n",
    "params = {'n_PCs':[1,3,5,7,10,50],'loss':['mse','huber']}\n",
    "PCR = val_fun(PCRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,sleep=3)\n",
    "\n",
    "EN_my_mse_rdm = ENet(lmd=.01,rho=.5,loss='huber').fit(X_trn,y_trn)\n",
    "\n",
    "params = {\n",
    "    'knots':[3],\n",
    "    'lmd':[1e-4,1e-1],#list(np.linspace(1e-4,1e-1,10)),\n",
    "    'l1_reg':[1e-4,0]\n",
    "}\n",
    "GLM = val_fun(GLMRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)\n",
    "\n",
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']\n",
    "OLS_3 = LinearRegression().fit(X_trn[features_3],y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5154272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[1,0] =r2_score(y_tst, OLS_H.predict(X_tst))\n",
    "output.iloc[1,2] = r2_score(y_tst, PCR.predict(X_tst))\n",
    "output.iloc[1,3] = r2_score(y_tst, PLS.predict(X_tst))\n",
    "output.iloc[1,4] = r2_score(y_tst, EN_my_mse_rdm.predict(X_tst))\n",
    "output.iloc[1,5] = r2_score(y_tst, GLM.predict(X_tst))\n",
    "\n",
    "output.iloc[1,1] = r2_score(y_tst, OLS_3.predict(X_tst[features_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07bfa454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.85469%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.25279%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.62587%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 3.00380%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 3.18136%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.43868%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'max_depth': 6, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308}\n",
      "with R2oos 3.18% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3, 6],\n",
    "    'max_features': [30, 50, 100],\n",
    "    'random_state': [12308]\n",
    "}\n",
    "RF = val_fun(RandomForestRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26fdb53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.00731%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -5.69424%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.12796%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.08824%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.28364%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.22563%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 8.48033%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -5.69424%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 11.09310%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.08824%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 12.48829%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.22563%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 5.21339%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.32103%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 7.44817%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.32103%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 8.45779%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.32103%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 26.22171%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.32103%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 29.60435%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.32103%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 30.29962%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.32103%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308}\n",
      "with R2oos 30.30% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective':[None, huber_obj],\n",
    "    'max_depth':[1,2],\n",
    "    'n_estimators':[500, 800, 1000],\n",
    "    'random_state':[12308],\n",
    "    'learning_rate':[.01,.1]\n",
    "}\n",
    "LGBM = val_fun(XGBRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b3662242",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[1,6] = r2_score(y_tst, RF.predict(X_tst))\n",
    "output.iloc[1,7] = r2_score(y_tst, LGBM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13ff717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.40921%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -3.43583%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.01586%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -2.16114%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.46837%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.10644%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.06452%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.19190%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 0.11% on validation set.\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN1-Regression-[32(relu)-1(linear)]\n",
    "\n",
    "params = {\n",
    "    'n_layers': [1],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf'],\n",
    "}\n",
    "NN1 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [2],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN2 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [3],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN3 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [4],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN4 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [5],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN5 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9cb7043",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[1,8] = r2_score(y_tst, NN1.predict(X_tst))\n",
    "output.iloc[1,9] = r2_score(y_tst, NN2.predict(X_tst))\n",
    "output.iloc[1,10] = r2_score(y_tst, NN3.predict(X_tst))\n",
    "output.iloc[1,11] = r2_score(y_tst, NN4.predict(X_tst))\n",
    "output.iloc[1,12] = r2_score(y_tst, NN5.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddb678",
   "metadata": {},
   "source": [
    "## 1.5 Bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ea07525",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_bot = b1_d.sort_values('mvel1',ascending=False).groupby('date').tail(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "64cc7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_co = b1_bot.index\n",
    "features = list(set(b1_bot.columns).difference({'permno','DATE','RET'}))\n",
    "X = MinMaxScaler((-1,1)).fit_transform(b1_bot[features])\n",
    "X = pd.DataFrame(X, columns=features, index=index_co)\n",
    "y = b1_bot['RET']\n",
    "\n",
    "stdt_vld = starting + (ending - starting)/3\n",
    "stdt_tst = starting + 2*(ending - starting)/3\n",
    "\n",
    "X_trn = X[X.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "y_trn = y[y.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "\n",
    "X_vld = X[(X.index.get_level_values(level = \"date\") >= stdt_vld) & (X.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "y_vld = y[(y.index.get_level_values(level = \"date\") >= stdt_vld) & (y.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "\n",
    "X_tst = X[X.index.get_level_values(level = \"date\") >= stdt_tst]\n",
    "y_tst = y[y.index.get_level_values(level = \"date\") >= stdt_tst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66690ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'n_components': 1} finished.\n",
      "with out-of-sample R squared on validation set: -4.94598%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 5} finished.\n",
      "with out-of-sample R squared on validation set: -2.12986%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 10} finished.\n",
      "with out-of-sample R squared on validation set: -2.96738%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 50} finished.\n",
      "with out-of-sample R squared on validation set: -3.07035%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'n_components': 5}\n",
      "with R2oos -2.13% on validation set.\n",
      "############################################################\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: -0.53070%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: -1.00642%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: -1.16852%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: -1.43518%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: -2.11008%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: -4.14291%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.01661%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: 0.01396%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: 0.02328%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: 0.02764%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: -0.05152%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: -0.17114%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'loss': 'huber', 'n_PCs': 7}\n",
      "with R2oos 0.03% on validation set.\n",
      "############################################################\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.061016\n",
      "         Iterations: 20988\n",
      "         Function evaluations: 22293\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: -2.70398%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: -2.79455%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.1}\n",
      "with R2oos 0.00% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "epsilon = np.max(((y_trn-OLS.predict(X_trn)).quantile(.999),1))\n",
    "OLS_H = HuberRegressor(epsilon=epsilon).fit(X_trn,y_trn)\n",
    "\n",
    "params = {'n_components': [1, 5, 10, 50]}\n",
    "PLS = val_fun(PLSRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)\n",
    "\n",
    "params = {'n_PCs':[1,3,5,7,10,50],'loss':['mse','huber']}\n",
    "PCR = val_fun(PCRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,sleep=3)\n",
    "\n",
    "EN_my_mse_rdm = ENet(lmd=.01,rho=.5,loss='huber').fit(X_trn,y_trn)\n",
    "\n",
    "params = {\n",
    "    'knots':[3],\n",
    "    'lmd':[1e-4,1e-1],#list(np.linspace(1e-4,1e-1,10)),\n",
    "    'l1_reg':[1e-4,0]\n",
    "}\n",
    "GLM = val_fun(GLMRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)\n",
    "\n",
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']\n",
    "OLS_3 = LinearRegression().fit(X_trn[features_3],y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9432d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[2,0] =r2_score(y_tst, OLS_H.predict(X_tst))\n",
    "output.iloc[2,2] = r2_score(y_tst, PCR.predict(X_tst))\n",
    "output.iloc[2,3] = r2_score(y_tst, PLS.predict(X_tst))\n",
    "output.iloc[2,4] = r2_score(y_tst, EN_my_mse_rdm.predict(X_tst))\n",
    "output.iloc[2,5] = r2_score(y_tst, GLM.predict(X_tst))\n",
    "\n",
    "output.iloc[2,1] = r2_score(y_tst, OLS_3.predict(X_tst[features_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "269e8cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.50883%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.51341%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.10074%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 3.46021%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 4.73532%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 6.03444%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'max_depth': 6, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308}\n",
      "with R2oos 6.03% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3, 6],\n",
    "    'max_features': [30, 50, 100],\n",
    "    'random_state': [12308]\n",
    "}\n",
    "RF = val_fun(RandomForestRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d2d05e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.53077%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.80458%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 3.36414%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.08669%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 3.90781%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.04311%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 6.84746%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.80458%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 10.24641%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.08669%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 11.83265%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.04311%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 9.77331%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.02944%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 11.64486%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.02944%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 12.53501%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.02944%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 27.60858%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.02944%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 30.12448%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.02944%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 30.61225%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x000001E787A3D040>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.02944%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308}\n",
      "with R2oos 30.61% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective':[None, huber_obj],\n",
    "    'max_depth':[1,2],\n",
    "    'n_estimators':[500, 800, 1000],\n",
    "    'random_state':[12308],\n",
    "    'learning_rate':[.01,.1]\n",
    "}\n",
    "LGBM = val_fun(XGBRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "340ac82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[2,6] = r2_score(y_tst, RF.predict(X_tst))\n",
    "output.iloc[2,7] = r2_score(y_tst, LGBM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "201073a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.00355%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.48623%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.33061%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.14776%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.01210%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.75231%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.09532%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.43358%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 500, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 1.43% on validation set.\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN1-Regression-[32(relu)-1(linear)]\n",
    "\n",
    "params = {\n",
    "    'n_layers': [1],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf'],\n",
    "}\n",
    "NN1 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [2],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN2 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [3],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN3 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [4],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN4 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [5],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN5 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1973656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[2,8] = r2_score(y_tst, NN1.predict(X_tst))\n",
    "output.iloc[2,9] = r2_score(y_tst, NN2.predict(X_tst))\n",
    "output.iloc[2,10] = r2_score(y_tst, NN3.predict(X_tst))\n",
    "output.iloc[2,11] = r2_score(y_tst, NN4.predict(X_tst))\n",
    "output.iloc[2,12] = r2_score(y_tst, NN5.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb45c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_excel(\"reality_here.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "684ca3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OLS + H</th>\n",
       "      <th>OLS-3 + H</th>\n",
       "      <th>PCR</th>\n",
       "      <th>PLS</th>\n",
       "      <th>Enet + H</th>\n",
       "      <th>GLM + H</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBRT + H</th>\n",
       "      <th>NN1</th>\n",
       "      <th>NN2</th>\n",
       "      <th>NN3</th>\n",
       "      <th>NN4</th>\n",
       "      <th>NN5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>-0.033009</td>\n",
       "      <td>-0.009846</td>\n",
       "      <td>-0.021733</td>\n",
       "      <td>-0.030201</td>\n",
       "      <td>-0.005345</td>\n",
       "      <td>-0.002751</td>\n",
       "      <td>0.014888</td>\n",
       "      <td>0.191139</td>\n",
       "      <td>-2.286733</td>\n",
       "      <td>-0.131319</td>\n",
       "      <td>-0.007220</td>\n",
       "      <td>-0.005772</td>\n",
       "      <td>-0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top 1000</th>\n",
       "      <td>-0.055939</td>\n",
       "      <td>-0.010582</td>\n",
       "      <td>-0.017403</td>\n",
       "      <td>-0.059468</td>\n",
       "      <td>-0.003677</td>\n",
       "      <td>-0.041385</td>\n",
       "      <td>0.026023</td>\n",
       "      <td>0.239203</td>\n",
       "      <td>-0.197176</td>\n",
       "      <td>-1.598856</td>\n",
       "      <td>-0.050293</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>-0.008123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom 1000</th>\n",
       "      <td>-0.025572</td>\n",
       "      <td>-0.009002</td>\n",
       "      <td>-0.012734</td>\n",
       "      <td>-0.201587</td>\n",
       "      <td>-0.004904</td>\n",
       "      <td>-0.001606</td>\n",
       "      <td>0.064284</td>\n",
       "      <td>0.263792</td>\n",
       "      <td>-0.204276</td>\n",
       "      <td>-0.801802</td>\n",
       "      <td>-0.002966</td>\n",
       "      <td>-0.001504</td>\n",
       "      <td>-0.000022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              OLS + H  OLS-3 + H       PCR       PLS  Enet + H   GLM + H  \\\n",
       "All         -0.033009  -0.009846 -0.021733 -0.030201 -0.005345 -0.002751   \n",
       "Top 1000    -0.055939  -0.010582 -0.017403 -0.059468 -0.003677 -0.041385   \n",
       "Bottom 1000 -0.025572  -0.009002 -0.012734 -0.201587 -0.004904 -0.001606   \n",
       "\n",
       "                   RF  GBRT + H       NN1       NN2       NN3       NN4  \\\n",
       "All          0.014888  0.191139 -2.286733 -0.131319 -0.007220 -0.005772   \n",
       "Top 1000     0.026023  0.239203 -0.197176 -1.598856 -0.050293 -0.001856   \n",
       "Bottom 1000  0.064284  0.263792 -0.204276 -0.801802 -0.002966 -0.001504   \n",
       "\n",
       "                  NN5  \n",
       "All         -0.000042  \n",
       "Top 1000    -0.008123  \n",
       "Bottom 1000 -0.000022  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75525910",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf50f3e",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd220f",
   "metadata": {},
   "source": [
    "### Old Code (Do not run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = 1  # setup \"Monte-Carlo\" number\n",
    "path = './Simu'  # set your own folder path \n",
    "datanum = 50 #Number of characteristics in the data (the only option is 50)\n",
    "dirstock = f\"{path}/Simu_p{datanum}/\"\n",
    "\n",
    "hh = [1] # Calculating monthly returns\n",
    "\n",
    "# hh = [1, 3, 6, 12]  # correspond to monthly, quarterly, half-year, and annually returns (have to but it into loop)\n",
    "\n",
    "title = f\"{path}/Simu_p{datanum}/Reg{hh[0]}\"\n",
    "\n",
    "if not os.path.exists(title) and MC == 1:\n",
    "    os.makedirs(title)\n",
    "titleB = f\"{title}/B\"\n",
    "if not os.path.exists(titleB) and MC == 1:\n",
    "    os.makedirs(titleB)\n",
    "if datanum == 50:\n",
    "    nump = 50\n",
    "\n",
    "M = 1 #Model number\n",
    "mu = 0.2 * np.sqrt(hh[0])\n",
    "tol = 10**(-10)\n",
    "mo = 1 \n",
    "nump = 50\n",
    "N = 200   # Number of CS tickers\n",
    "m = nump * 2   # Number of Characteristics\n",
    "T = 180   # Number of Time Periods\n",
    "\n",
    "per = np.tile(np.arange(1, N+1), T)\n",
    "time = np.repeat(np.arange(1, T+1), N)\n",
    "stdv = 0.05\n",
    "theta_w = 0.005\n",
    "\n",
    "# Read Files\n",
    "path1 = dirstock + 'c' + str(M) + '_' + str(M) + '.csv'\n",
    "path2 = dirstock + 'r' + str(mo) + '_' + str(M) + '.csv'\n",
    "c = np.genfromtxt(path1, delimiter=',')\n",
    "r1 = np.genfromtxt(path2, delimiter=',')\n",
    "\n",
    "# Add Some Elements\n",
    "daylen = np.repeat(N, T//3)\n",
    "daylen_test = daylen.copy()\n",
    "ind = np.arange(0, N*T//3)\n",
    "xtrain = c[ind]\n",
    "ytrain = r1[ind]\n",
    "trainper = per[ind]\n",
    "start_idx = math.floor(N * T / 3) + 1\n",
    "end_idx = math.floor(N * (T * 2 / 3 - hh[0] + 1))\n",
    "ind = list(range(start_idx, end_idx))\n",
    "xtest = c[ind]\n",
    "ytest = r1[ind]\n",
    "testper = per[ind]\n",
    "\n",
    "l1 = c.shape[0]\n",
    "l2 = len(r1)\n",
    "l3 = l2 - np.isnan(r1).sum()\n",
    "\n",
    "ind = np.arange(N*(2*T//3), min([l1, l2, l3]))\n",
    "xoos = c[ind]\n",
    "yoos = r1[ind]\n",
    "\n",
    "# Monthly Demean\n",
    "ytrain_demean = ytrain - np.mean(ytrain)\n",
    "ytest_demean = ytest - np.mean(ytest)\n",
    "mtrain = np.mean(ytrain)\n",
    "mtest = np.mean(ytest)\n",
    "\n",
    "# Calculate Sufficient Stats\n",
    "sd = np.zeros(xtrain.shape[1])\n",
    "for i in range(xtrain.shape[1]):\n",
    "    s = np.std(xtrain[:, i])\n",
    "    if s > 0:\n",
    "        xtrain[:, i] /= s\n",
    "        xtest[:, i] /= s\n",
    "        xoos[:, i] /= s\n",
    "        sd[i] = s\n",
    "\n",
    "XX = np.dot(xtrain.T, xtrain)\n",
    "U, S, V = np.linalg.svd(XX)\n",
    "L = S[0]\n",
    "Y = ytrain_demean\n",
    "XY = np.dot(xtrain.T, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS\n",
    "\n",
    "# Initialize arrays\n",
    "r2_oos = np.zeros(13)\n",
    "r2_is = np.zeros(13)\n",
    "modeln = 0\n",
    "groups = 0\n",
    "nc = 0\n",
    "\n",
    "# OLS+H\n",
    "modeln += 1\n",
    "\n",
    "clf = LinearRegression(fit_intercept=False)\n",
    "clf.fit(xtrain, ytrain_demean)\n",
    "b = clf.coef_\n",
    "\n",
    "func = soft_threshodl\n",
    "b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, 0, func)\n",
    "yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "print(f\"Simple OLS+H R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "# PCR\n",
    "modeln += 1\n",
    "ne = 30\n",
    "X = np.dot(xtrain.T, xtrain)\n",
    "pca_vec = V.T\n",
    "p1 = pca_vec[:, :ne]\n",
    "Z = np.dot(xtrain, p1)\n",
    "r = np.zeros((3, ne))\n",
    "B = np.zeros((xtrain.shape[1], ne))\n",
    "Y = ytrain_demean\n",
    "\n",
    "for j in range(ne - 1):\n",
    "    xx = Z[:, :j + 1]\n",
    "    b = np.dot(np.linalg.inv(np.dot(xx.T, xx)), np.dot(xx.T, Y))\n",
    "    b = np.dot(p1[:, :j + 1], b)\n",
    "\n",
    "    yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "    r[0, j] = 1 - np.sum(np.power(yhatbig1 - ytest, 2)) / np.sum(np.power(ytest - mtrain, 2))\n",
    "    yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "    r[1, j] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "    yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "    r[2, j] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "    B[:, j] = b\n",
    "\n",
    "b = np.zeros(xtest.shape[1])\n",
    "j = ne - 1\n",
    "yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "r[0, j] = 1 - np.sum(np.power(yhatbig1 - ytest, 2)) / np.sum(np.power(ytest - mtrain, 2))\n",
    "yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "r[1, j] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "r[2, j] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "B[:, j] = b\n",
    "\n",
    "r2_oos[modeln - 1] = r[1, fw1(r[0, :].tolist())]\n",
    "r2_is[modeln - 1] = r[2, fw1(r[0, :].tolist())]\n",
    "b = B[:, fw1(r[0, :].tolist())]\n",
    "pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "print(f\"PCR R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "# PLS\n",
    "modeln += 1\n",
    "B = pls(xtrain, ytrain_demean, 30)\n",
    "ne = 30\n",
    "r = np.zeros((3, ne))\n",
    "Y = ytrain_demean\n",
    "\n",
    "for j in range(ne):\n",
    "    b = B[:, j]\n",
    "    yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "    r[0, j] = 1 - np.sum(np.power(yhatbig1 - ytest, 2)) / np.sum(np.power(ytest - mtrain, 2))\n",
    "    yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "    r[1, j] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "    yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "    r[2, j] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "\n",
    "r2_oos[modeln - 1] = r[1, fw1(r[0, :].tolist())]\n",
    "r2_is[modeln - 1] = r[2, fw1(r[0, :].tolist())]\n",
    "b = B[:, fw1(r[0, :].tolist())]\n",
    "pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "print(f\"PLS R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "\n",
    "# Elastic Net\n",
    "modeln += 1\n",
    "lamv = np.arange(-2, 4.1, 0.1)\n",
    "alpha = 0.5\n",
    "r = np.zeros((3, len(lamv)))\n",
    "\n",
    "for j in range(len(lamv)):\n",
    "    l2 = 10 ** lamv[j]\n",
    "    func = soft_threshode\n",
    "    b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "    yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "    r[0, j] = 1 - np.sum(np.power(yhatbig1 - ytest, 2)) / np.sum(np.power(ytest - mtrain, 2))\n",
    "    yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "    r[1, j] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "    yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "    r[2, j] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "\n",
    "r2_oos[modeln - 1] = r[1, fw1(r[0, :].tolist())]\n",
    "r2_is[modeln - 1] = r[2, fw1(r[0, :].tolist())]\n",
    "l2 = 10 ** lamv[int(fw1(r[0]))]\n",
    "func = soft_threshode\n",
    "b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "np.savetxt(pathb, b, delimiter=',')\n",
    "print('Enet R2 :', np.round(r2_oos[modeln-1], 3))\n",
    "\n",
    "modeln += 1\n",
    "func = soft_threshode\n",
    "b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, l2, func)\n",
    "yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "r2_oos[modeln-1] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "r2_is[modeln-1] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "np.savetxt(pathb, b, delimiter=',')\n",
    "print('Enet+H R2 :', np.round(r2_oos[modeln-1], 3))\n",
    "\n",
    "# Group Lasso\n",
    "kn = 4\n",
    "th = np.zeros((kn, xtrain.shape[1]))\n",
    "th[1, :] = 0\n",
    "\n",
    "for i in range(xtrain.shape[1]):\n",
    "    th[:, i] = np.quantile(xtrain[:, i], np.arange(kn) / kn)\n",
    "\n",
    "xtrain = cut_knots_degree2(xtrain, kn, th)\n",
    "xtest = cut_knots_degree2(xtest, kn, th)\n",
    "xoos = cut_knots_degree2(xoos, kn, th)\n",
    "\n",
    "for i in range(xtrain.shape[1]):\n",
    "    s = np.std(xtrain[:, i])\n",
    "    if s > 0:\n",
    "        xtrain[:, i] = xtrain[:, i] / s\n",
    "        xtest[:, i] = xtest[:, i] / s\n",
    "        xoos[:, i] = xoos[:, i] / s\n",
    "\n",
    "Y = ytrain_demean\n",
    "XX = xtrain.T @ xtrain\n",
    "U, S, V = np.linalg.svd(XX)\n",
    "L = S[0]\n",
    "XY = xtrain.T @ Y\n",
    "\n",
    "modeln += 1\n",
    "lamv = np.arange(0.5, 3.1, 0.1)\n",
    "nc = XX.shape[1] // (kn + 1)\n",
    "groups = np.repeat(np.arange(1, nc + 1), kn + 1)\n",
    "r = np.zeros((3, len(lamv)))\n",
    "\n",
    "for j, lam in enumerate(lamv):\n",
    "    l2 = 10 ** lam\n",
    "    func = soft_threshodg\n",
    "    b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "    yhatbig1 = xtest @ b + mtrain\n",
    "    r[0, j] = 1 - np.sum((yhatbig1 - ytest) ** 2) / np.sum((ytest - mtrain) ** 2)\n",
    "    yhatbig1 = xoos @ b + mtrain\n",
    "    r[1, j] = 1 - np.sum((yhatbig1 - yoos) ** 2) / np.sum((yoos - mtrain) ** 2)\n",
    "    yhatbig1 = xtrain @ b + mtrain\n",
    "    r[2, j] = 1 - np.sum((yhatbig1 - ytrain) ** 2) / np.sum((ytrain - mtrain) ** 2)\n",
    "\n",
    "r2_oos[modeln] = r[1, np.int16(fw1(r[0, :]))]\n",
    "r2_is[modeln] = r[2, np.int16(fw1(r[0, :]))]\n",
    "l2 = 10 ** lamv[np.int16(fw1(r[0, :]))]\n",
    "\n",
    "func = soft_threshodg\n",
    "b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "pathb = f\"{title}/B/b\"\n",
    "pathb = f\"{pathb}_{mo}_{M}_{modeln}.csv\"\n",
    "np.savetxt(pathb, b, delimiter=\",\")\n",
    "\n",
    "print(\"Group Lasso R2:\", np.round(r2_oos[modeln], 3))\n",
    "\n",
    "modeln += 1\n",
    "func = soft_threshodg\n",
    "b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, l2, func)\n",
    "yhatbig1 = xoos @ b + mtrain\n",
    "r2_oos[modeln] = 1 - np.sum((yhatbig1 - yoos) ** 2) / np.sum((yoos - mtrain) ** 2)\n",
    "yhatbig1 = xtrain @ b + mtrain\n",
    "r2_is[modeln] = 1 - np.sum((yhatbig1 - ytrain) ** 2) / np.sum((ytrain - mtrain) ** 2)\n",
    "pathb = f\"{title}/B/b\"\n",
    "pathb = f\"{pathb}_{mo}_{M}_{modeln}.csv\"\n",
    "np.savetxt(pathb, b, delimiter=\",\")\n",
    "\n",
    "print(\"Group Lasso+H R2:\", np.round(r2_oos[modeln], 3))\n",
    "\n",
    "pathr = f\"{title}/roos\"\n",
    "pathr = f\"{pathr}_{mo}_{M}.csv\"\n",
    "np.savetxt(pathr, r2_oos, delimiter=\",\")\n",
    "\n",
    "pathr = f\"{title}/ris\"\n",
    "pathr = f\"{pathr}_{mo}_{M}.csv\"\n",
    "np.savetxt(pathr, r2_is, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fecf48",
   "metadata": {},
   "source": [
    "# Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e07279",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = 1  # setup MC number\n",
    "datanum = 50  # Or datanum = 100; separately run two cases\n",
    "path = './Simu'  # set your own folder path\n",
    "dirstock = os.path.join(path, f'SimuData_p{datanum}/')\n",
    "\n",
    "for hh in [1]:\n",
    "    # for hh in [1, 3, 6, 12]:  # correspond to monthly quarterly halfyear and annually returns\n",
    "    title = os.path.join(path, f'Simu_p{datanum}/Tree{hh}')\n",
    "\n",
    "    if not os.path.isdir(title) and MC == 1:\n",
    "        os.makedirs(title)\n",
    "\n",
    "    titleB = os.path.join(title, 'B')\n",
    "    if not os.path.isdir(titleB) and MC == 1:\n",
    "        os.makedirs(titleB)\n",
    "\n",
    "    if datanum == 50:\n",
    "        nump = 50\n",
    "    if datanum == 100:\n",
    "        nump = 100\n",
    "        \n",
    "        \n",
    "    for M in [MC]:\n",
    "        for mo in [1, 2]:\n",
    "            print(f\"### MCMC : {M}, Model : {mo} ###\")\n",
    "            N = 200  # Number of CS tickers\n",
    "            m = nump * 2  # Number of Characteristics\n",
    "            T = 180  # Number of Time Periods\n",
    "\n",
    "            per = np.tile(np.arange(1, N + 1), T)\n",
    "            time = np.repeat(np.arange(1, T + 1), N)\n",
    "            stdv = 0.05\n",
    "            theta_w = 0.005\n",
    "\n",
    "            # Read Files\n",
    "            path1 = f\"{dirstock}c{M}.csv\"\n",
    "            path2 = f\"{dirstock}r{mo}_{M}.csv\"\n",
    "            c = np.genfromtxt(path1, delimiter=',')\n",
    "            r1 = np.genfromtxt(path2, delimiter=',')\n",
    "\n",
    "            # Add Some Elements\n",
    "            daylen = np.tile(N, T // 3)\n",
    "            daylen_test = daylen\n",
    "            ind = np.arange(0, int(N * T / 3))\n",
    "            xtrain = c[ind, :]\n",
    "            ytrain = r1[ind]\n",
    "            trainper = per[ind]\n",
    "            ind = np.arange(int(N * T / 3), int(N * (T * 2 / 3 + 1)))\n",
    "            xtest = c[ind, :]\n",
    "            ytest = r1[ind]\n",
    "            testper = per[ind]\n",
    "\n",
    "            l1 = c.shape[0]\n",
    "            l2 = len(r1)\n",
    "            l3 = l2 - np.sum(np.isnan(r1))\n",
    "\n",
    "            ind = np.arange(int(N * T * 2 / 3), min(l1, l2, l3))\n",
    "            xoos = c[ind, :]\n",
    "            yoos = r1[ind]\n",
    "\n",
    "            # Monthly Demean\n",
    "            ytrain_demean = ytrain - np.mean(ytrain)\n",
    "            ytest_demean = ytest - np.mean(ytest)\n",
    "            mtrain = np.mean(ytrain)\n",
    "            mtest = np.mean(ytest)\n",
    "\n",
    "            # Start to train\n",
    "            r2_oos = np.zeros(3)  # OOS R2\n",
    "            r2_is = np.zeros(3)  # IS R2\n",
    "\n",
    "            # Random Forest\n",
    "            if nump == 50:\n",
    "                lamv = np.arange(10, 101, 10)\n",
    "            elif nump == 100:\n",
    "                lamv = np.arange(10, 201, 20)\n",
    "            ne = 100\n",
    "            lamc = [2, 4, 8, 16, 32]\n",
    "            r = np.zeros((len(lamv), len(lamc), 3))\n",
    "\n",
    "            #for n1 in (range(len(lamv))):\n",
    "             #   nf = lamv[n1]\n",
    "              #  for n2 in range(len(lamc)):\n",
    "               #     nn = lamc[n2]\n",
    "                #    clf = RandomForestRegressor(\n",
    "                 #       n_estimators=ne,\n",
    "                  #      max_features=nf,\n",
    "                   #     max_depth=nn\n",
    "                    #)\n",
    "                    #clf.fit(xtrain, ytrain)\n",
    "                    #yhatbig1 = clf.predict(xtest)\n",
    "                    #r[n1, n2, 0] = 1 - np.sum(np.power(yhatbig1 - ytest, 2)) / np.sum(\n",
    "                     #   np.power(ytest - mtrain, 2))\n",
    "                    #yhatbig1 = clf.predict(xoos)\n",
    "                    #r[n1, n2, 1] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(\n",
    "                     #   np.power(yoos - mtrain, 2))\n",
    "                    #yhatbig1 = clf.predict(xtrain)\n",
    "                    #r[n1, n2, 2] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(\n",
    "                     #   np.power(ytrain - mtrain, 2))\n",
    "            ytest_mtrain_sq = np.power(ytest - mtrain, 2)\n",
    "            yoos_mtrain_sq = np.power(yoos - mtrain, 2)\n",
    "            ytrain_mtrain_sq = np.power(ytrain - mtrain, 2)\n",
    "            for n1, nf in enumerate(lamv):\n",
    "                for n2, nn in enumerate(lamc):\n",
    "                    clf = RandomForestRegressor(\n",
    "                        n_estimators=ne,\n",
    "                        max_features=nf,\n",
    "                        max_depth=nn\n",
    "                    )\n",
    "                    clf.fit(xtrain, ytrain)\n",
    "\n",
    "                    # Predictions\n",
    "                    yhat_test = clf.predict(xtest)\n",
    "                    yhat_oos = clf.predict(xoos)\n",
    "                    yhat_train = clf.predict(xtrain)\n",
    "\n",
    "                    # Compute performance metrics\n",
    "                    r[n1, n2, 0] = 1 - np.sum(np.power(yhat_test - ytest, 2)) / np.sum(ytest_mtrain_sq)\n",
    "                    r[n1, n2, 1] = 1 - np.sum(np.power(yhat_oos - yoos, 2)) / np.sum(yoos_mtrain_sq)\n",
    "                    r[n1, n2, 2] = 1 - np.sum(np.power(yhat_train - ytrain, 2)) / np.sum(ytrain_mtrain_sq)\n",
    "\n",
    "            fw_2 = np.unravel_index(np.argmax(r[:, :, 0]), r[:, :, 0].shape)\n",
    "            r2_oos[0] = r[fw_2[0], fw_2[1], 1]\n",
    "            r2_is[0] = r[fw_2[0], fw_2[1], 2]\n",
    "            print(f\"RF R2 : {r2_oos[0]:.3f}\")\n",
    "            \n",
    "            \n",
    "            #GBRT\n",
    "            \n",
    "            lamv = np.arange(-1, 0.1, 0.2)\n",
    "            r = np.zeros((len(lamv), 50, 3))\n",
    "\n",
    "            for n1 in range(len(lamv)):\n",
    "                lr = 10 ** lamv[n1]\n",
    "                alpha = 2\n",
    "                ne = 50\n",
    "                clf = GradientBoostingRegressor(\n",
    "                    n_estimators=ne,\n",
    "                    learning_rate=lr,\n",
    "                    loss='ls',\n",
    "                    max_depth=2\n",
    "                )\n",
    "\n",
    "                clf.fit(xtrain, ytrain)\n",
    "                e = clf.staged_predict(xtest)\n",
    "                for i, pred in enumerate(e):\n",
    "                    r[n1, i, 0] = np.mean((pred - ytest) ** 2)\n",
    "\n",
    "                e = clf.staged_predict(xoos)\n",
    "                for i, pred in enumerate(e):\n",
    "                    r[n1, i, 1] = np.mean((pred - yoos) ** 2)\n",
    "\n",
    "                e = clf.staged_predict(xtrain)\n",
    "                for i, pred in enumerate(e):\n",
    "                    r[n1, i, 2] = np.mean((pred - ytrain) ** 2)\n",
    "\n",
    "            fw_2 = np.unravel_index(np.argmin(r[:, :, 0]), r[:, :, 0].shape)\n",
    "            err1 = np.mean((ytrain - mtrain) ** 2)\n",
    "            err2 = np.mean((yoos - mtrain) ** 2)\n",
    "            r2_oos[1] = 1 - r[fw_2[0], fw_2[1], 1] / err2\n",
    "            r2_is[1] = 1 - r[fw_2[0], fw_2[1], 2] / err1\n",
    "            print(f\"GBRT R2 : {r2_oos[1]:.3f}\")\n",
    "\n",
    "            # Save r2_oos and r2_is to files\n",
    "            pathr = f\"{title}/roos_{mo}_{M}.csv\"\n",
    "            np.savetxt(pathr, r2_oos.reshape(1, -1), delimiter=\",\")\n",
    "            pathr = f\"{title}/ris_{mo}_{M}.csv\"\n",
    "            np.savetxt(pathr, r2_is.reshape(1, -1), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ed04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = 1  # setup MC number\n",
    "datanum = 50  # Or datanum = 100; separately run two cases\n",
    "path = './Simu'  # set your own folder path\n",
    "dirstock = os.path.join(path, f'SimuData_p{datanum}/')\n",
    "hh = [1]\n",
    "mo = 1\n",
    "# for hh in [1, 3, 6, 12]:  # correspond to monthly quarterly halfyear and annually returns\n",
    "title = os.path.join(path, f'Simu_p{datanum}/Tree{hh}')\n",
    "\n",
    "if not os.path.isdir(title) and MC == 1:\n",
    "    os.makedirs(title)\n",
    "\n",
    "titleB = os.path.join(title, 'B')\n",
    "if not os.path.isdir(titleB) and MC == 1:\n",
    "    os.makedirs(titleB)\n",
    "\n",
    "if datanum == 50:\n",
    "    nump = 50\n",
    "if datanum == 100:\n",
    "    nump = 100\n",
    "N = 200  # Number of CS tickers\n",
    "m = nump * 2  # Number of Characteristics\n",
    "T = 180  # Number of Time Periods\n",
    "\n",
    "per = np.tile(np.arange(1, N + 1), T)\n",
    "time = np.repeat(np.arange(1, T + 1), N)\n",
    "stdv = 0.05\n",
    "theta_w = 0.005\n",
    "\n",
    "# Read Files\n",
    "path1 = f\"{dirstock}c{M}.csv\"\n",
    "path2 = f\"{dirstock}r{mo}_{M}.csv\"\n",
    "c = np.genfromtxt(path1, delimiter=',')\n",
    "r1 = np.genfromtxt(path2, delimiter=',')\n",
    "\n",
    "# Add Some Elements\n",
    "daylen = np.tile(N, T // 3)\n",
    "daylen_test = daylen\n",
    "ind = np.arange(0, int(N * T / 3))\n",
    "xtrain = c[ind, :]\n",
    "ytrain = r1[ind]\n",
    "trainper = per[ind]\n",
    "ind = np.arange(int(N * T / 3), int(N * (T * 2 / 3 + 1)))\n",
    "xtest = c[ind, :]\n",
    "ytest = r1[ind]\n",
    "testper = per[ind]\n",
    "\n",
    "l1 = c.shape[0]\n",
    "l2 = len(r1)\n",
    "l3 = l2 - np.sum(np.isnan(r1))\n",
    "\n",
    "ind = np.arange(int(N * T * 2 / 3), min(l1, l2, l3))\n",
    "xoos = c[ind, :]\n",
    "yoos = r1[ind]\n",
    "\n",
    "# Monthly Demean\n",
    "ytrain_demean = ytrain - np.mean(ytrain)\n",
    "ytest_demean = ytest - np.mean(ytest)\n",
    "mtrain = np.mean(ytrain)\n",
    "mtest = np.mean(ytest)\n",
    "\n",
    "# Start to train\n",
    "r2_oos = np.zeros(3)  # OOS R2\n",
    "r2_is = np.zeros(3)  # IS R2\n",
    "\n",
    "# Random Forest\n",
    "if nump == 50:\n",
    "    lamv = np.arange(10, 101, 10)\n",
    "elif nump == 100:\n",
    "    lamv = np.arange(10, 201, 20)\n",
    "ne = 100\n",
    "lamc = [2, 4, 8, 16, 32]\n",
    "r = np.zeros((len(lamv), len(lamc), 3))\n",
    "\n",
    "for n1 in tqdm(range(len(lamv))):\n",
    "    nf = lamv[n1]\n",
    "    for n2 in range(len(lamc)):\n",
    "        nn = lamc[n2]\n",
    "        clf = RandomForestRegressor(\n",
    "            n_estimators=ne,\n",
    "            max_features=nf,\n",
    "            max_depth=nn\n",
    "        )\n",
    "        clf.fit(xtrain, ytrain)\n",
    "        yhatbig1 = clf.predict(xtest)\n",
    "        r[n1, n2, 0] = 1 - np.sum(np.power(yhatbig1 - ytest, 2)) / np.sum(\n",
    "        np.power(ytest - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xoos)\n",
    "        r[n1, n2, 1] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(\n",
    "        np.power(yoos - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xtrain)\n",
    "        r[n1, n2, 2] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(\n",
    "        np.power(ytrain - mtrain, 2))\n",
    "\n",
    "fw_2 = np.unravel_index(np.argmax(r[:, :, 0]), r[:, :, 0].shape)\n",
    "r2_oos[0] = r[fw_2[0], fw_2[1], 1]\n",
    "r2_is[0] = r[fw_2[0], fw_2[1], 2]\n",
    "print(f\"RF R2 : {r2_oos[0]:.3f}\")\n",
    "\n",
    "# Save r2_oos and r2_is to files\n",
    "pathr = f\"{title}/roos_{mo}_{M}.csv\"\n",
    "np.savetxt(pathr, r2_oos.reshape(1, -1), delimiter=\",\")\n",
    "pathr = f\"{title}/ris_{mo}_{M}.csv\"\n",
    "np.savetxt(pathr, r2_is.reshape(1, -1), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816688a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting \n",
    "mo = 1\n",
    "\n",
    "lamv = np.arange(-1, 0.1, 0.2)\n",
    "r = np.zeros((len(lamv), 50, 3))\n",
    "\n",
    "for n1 in range(len(lamv)):\n",
    "    lr = 10 ** lamv[n1]\n",
    "    alpha = 2\n",
    "    ne = 50\n",
    "    clf = GradientBoostingRegressor(\n",
    "        n_estimators=ne,\n",
    "        learning_rate=lr,\n",
    "        loss='huber',\n",
    "        max_depth=2\n",
    "    )\n",
    "\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    e = clf.staged_predict(xtest)\n",
    "    for i, pred in enumerate(e):\n",
    "        r[n1, i, 0] = np.mean((pred - ytest) ** 2)\n",
    "\n",
    "    e = clf.staged_predict(xoos)\n",
    "    for i, pred in enumerate(e):\n",
    "        r[n1, i, 1] = np.mean((pred - yoos) ** 2)\n",
    "\n",
    "    e = clf.staged_predict(xtrain)\n",
    "    for i, pred in enumerate(e):\n",
    "        r[n1, i, 2] = np.mean((pred - ytrain) ** 2)\n",
    "\n",
    "fw_2 = np.unravel_index(np.argmin(r[:, :, 0]), r[:, :, 0].shape)\n",
    "err1 = np.mean((ytrain - mtrain) ** 2)\n",
    "err2 = np.mean((yoos - mtrain) ** 2)\n",
    "r2_oos[1] = 1 - r[fw_2[0], fw_2[1], 1] / err2\n",
    "r2_is[1] = 1 - r[fw_2[0], fw_2[1], 2] / err1\n",
    "print(f\"GBRT R2 : {r2_oos[1]:.3f}\")\n",
    "\n",
    "# Save r2_oos and r2_is to files\n",
    "pathr = f\"{title}/roos_{mo}_{M}.csv\"\n",
    "np.savetxt(pathr, r2_oos.reshape(1, -1), delimiter=\",\")\n",
    "pathr = f\"{title}/ris_{mo}_{M}.csv\"\n",
    "np.savetxt(pathr, r2_is.reshape(1, -1), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fee7a1",
   "metadata": {},
   "source": [
    "# Additional Manipulations to get top-1000 and bottom-1000 of simulated stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b55a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_c.shape, x_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated = pd.concat([x_c, x_b], ignore_index=True)\n",
    "#assume that the 4th column is the Market Value of the stock. Proceeding to arrange it in accordance to the column values\n",
    "updated = updated.sort_values(by=updated.columns[3], ascending=False)\n",
    "rtop_1 = updated.iloc[:,:1]\n",
    "ctop1 = updated.iloc[:,1:]\n",
    "\n",
    "rtop_1 = rtop_1.dropna()\n",
    "ctop1 = ctop1.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtop_1.to_csv('r1_1.csv', index=False, header=False)\n",
    "ctop1.to_csv('c1.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8224cbb",
   "metadata": {},
   "source": [
    "Same simulations for top 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa598ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = 1  # setup MC number\n",
    "datanum = 50  # Or datanum = 100; separately run two cases\n",
    "path = './Simu'  # set your own folder path    \n",
    "dirstock = f\"{path}/SimuData_d{datanum}/\"\n",
    "\n",
    "hh = [1]\n",
    "# hh = [1, 3, 6, 12]  # correspond to monthly, quarterly, half-year, and annually returns\n",
    "title = f\"{path}/Simu_p{datanum}/Reg{hh[0]}\"\n",
    "if not os.path.exists(title) and MC == 1:\n",
    "    os.makedirs(title)\n",
    "titleB = f\"{title}/B\"\n",
    "if not os.path.exists(titleB) and MC == 1:\n",
    "    os.makedirs(titleB)\n",
    "if datanum == 50:\n",
    "    nump = 50\n",
    "elif datanum == 100:\n",
    "    nump = 100\n",
    "mu = 0.2 * np.sqrt(hh[0])\n",
    "tol = 10**(-10)\n",
    "\n",
    "for M in range(1, 2):  # Assuming range for M\n",
    "    for mo in range(1, 2):  # Assuming range for mo\n",
    "        \n",
    "        print('### MCMC : {}, Model : {} ###'.format(M, mo))\n",
    "        \n",
    "        N = 200   # Number of CS tickers\n",
    "        m = nump * 2   # Number of Characteristics\n",
    "        T = 4  # Number of Time Periods\n",
    "        \n",
    "        per = np.tile(np.arange(1, N+1), T)\n",
    "        time = np.repeat(np.arange(1, T+1), N)\n",
    "        stdv = 0.05\n",
    "        theta_w = 0.005\n",
    "        \n",
    "        # Read Files\n",
    "        path1 = dirstock + 'c' + str(M) + '.csv'\n",
    "        path2 = dirstock + 'r' + str(mo) + '_' + str(M) + '.csv'\n",
    "        c = np.genfromtxt(path1, delimiter=',')\n",
    "        r1 = np.genfromtxt(path2, delimiter=',')\n",
    "        \n",
    "        # Add Some Elements\n",
    "        daylen = np.repeat(N, T//3)\n",
    "        daylen_test = daylen.copy()\n",
    "        ind = np.arange(0, N*T//3)\n",
    "        xtrain = c[ind]\n",
    "        ytrain = r1[ind]\n",
    "        trainper = per[ind]\n",
    "        start_idx = math.floor(N * T / 3) + 1\n",
    "        end_idx = math.floor(N * (T * 2 / 3 - hh[0] + 1))\n",
    "        ind = list(range(start_idx, end_idx))\n",
    "        xtest = c[ind]\n",
    "        ytest = r1[ind]\n",
    "        testper = per[ind]\n",
    "        \n",
    "        l1 = c.shape[0]\n",
    "        l2 = len(r1)\n",
    "        l3 = l2 - np.isnan(r1).sum()\n",
    "        \n",
    "        ind = np.arange(N*(2*T//3), min([l1, l2, l3]))\n",
    "        xoos = c[ind]\n",
    "        yoos = r1[ind]\n",
    "        \n",
    "        # Monthly Demean\n",
    "        ytrain_demean = ytrain - np.mean(ytrain)\n",
    "        ytest_demean = ytest - np.mean(ytest)\n",
    "        mtrain = np.mean(ytrain)\n",
    "        mtest = np.mean(ytest)\n",
    "        \n",
    "        # Calculate Sufficient Stats\n",
    "        sd = np.zeros(xtrain.shape[1])\n",
    "        for i in range(xtrain.shape[1]):\n",
    "            s = np.std(xtrain[:, i])\n",
    "            if s > 0:\n",
    "                xtrain[:, i] /= s\n",
    "                xtest[:, i] /= s\n",
    "                xoos[:, i] /= s\n",
    "                sd[i] = s\n",
    "        \n",
    "        XX = np.dot(xtrain.T, xtrain)\n",
    "        U, S, V = np.linalg.svd(XX)\n",
    "        L = S[0]\n",
    "        Y = ytrain_demean\n",
    "        XY = np.dot(xtrain.T, Y)\n",
    "        \n",
    "         \n",
    "        #OLS\n",
    "        \n",
    "        # Initialize arrays\n",
    "        r2_oos = np.zeros(13)\n",
    "        r2_is = np.zeros(13)\n",
    "        modeln = 0\n",
    "        groups = 0\n",
    "        nc = 0\n",
    "\n",
    "        # OLS\n",
    "        modeln += 1\n",
    "        clf = LinearRegression(fit_intercept=False)\n",
    "        clf.fit(xtrain, ytrain_demean)\n",
    "        yhatbig1 = clf.predict(xoos) + mtrain\n",
    "        r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xtrain) + mtrain\n",
    "        r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "        b = clf.coef_\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"Simple OLS R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "        # OLS+H\n",
    "        modeln += 1\n",
    "        func = soft_threshodl\n",
    "        b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, 0, func)\n",
    "        yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "        r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1[:1000] - yoos[:1000], 2)) / np.sum(np.power(yoos[:1000] - mtrain, 2))\n",
    "        yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "        r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1[:1000] - ytrain[:1000], 2)) / np.sum(np.power(ytrain[:1000] - mtrain, 2))\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"Simple OLS+H R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "        # PCR\n",
    "        modeln += 1\n",
    "        ne = 30\n",
    "        X = np.dot(xtrain.T, xtrain)\n",
    "        pca_vec = V.T\n",
    "        p1 = pca_vec[:, :ne]\n",
    "        Z = np.dot(xtrain, p1)\n",
    "        r = np.zeros((3, ne))\n",
    "        B = np.zeros((xtrain.shape[1], ne))\n",
    "        Y = ytrain_demean\n",
    "\n",
    "        for j in range(ne - 1):\n",
    "            xx = Z[:, :j + 1]\n",
    "            b = np.dot(np.linalg.inv(np.dot(xx.T, xx)), np.dot(xx.T, Y))\n",
    "            b = np.dot(p1[:, :j + 1], b)\n",
    "\n",
    "            yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "            r[0, j] = 1 - np.sum(np.power(yhatbig1[:1000] - ytest[:1000], 2)) / np.sum(np.power(ytest[:1000] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "            r[1, j] = 1 - np.sum(np.power(yhatbig1[:1000] - yoos[:1000], 2)) / np.sum(np.power(yoos[:1000] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "            r[2, j] = 1 - np.sum(np.power(yhatbig1[:1000] - ytrain[:1000], 2)) / np.sum(np.power(ytrain[:1000] - mtrain, 2))\n",
    "            B[:, j] = b\n",
    "\n",
    "        b = np.zeros(xtest.shape[1])\n",
    "        j = ne - 1\n",
    "        yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "        r[0, j] = 1 - np.sum(np.power(yhatbig1[:1000] - ytest[:1000], 2)) / np.sum(np.power(ytest[:1000] - mtrain, 2))\n",
    "        yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "        r[1, j] = 1 - np.sum(np.power(yhatbig1[:1000] - yoos[:1000], 2)) / np.sum(np.power(yoos[:1000] - mtrain, 2))\n",
    "        yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "        r[2, j] = 1 - np.sum(np.power(yhatbig1[:1000] - ytrain[:1000], 2)) / np.sum(np.power(ytrain[:1000] - mtrain, 2))\n",
    "        B[:, j] = b\n",
    "\n",
    "        r2_oos[modeln - 1] = r[1, fw1(r[0, :].tolist())]\n",
    "        r2_is[modeln - 1] = r[2, fw1(r[0, :].tolist())]\n",
    "        b = B[:, fw1(r[0, :].tolist())]\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"PCR R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "        # PLS\n",
    "        modeln += 1\n",
    "        B = pls(xtrain, ytrain_demean, 30)\n",
    "        ne = 30\n",
    "        r = np.zeros((3, ne))\n",
    "        Y = ytrain_demean\n",
    "\n",
    "        for j in range(ne):\n",
    "            b = B[:, j]\n",
    "            yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "            r[0, j] = 1 - np.sum(np.power(yhatbig1[:1000] - ytest[:1000], 2)) / np.sum(np.power(ytest[:1000] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "            r[1, j] = 1 - np.sum(np.power(yhatbig1[:1000] - yoos[:1000], 2)) / np.sum(np.power(yoos[:1000] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "            r[2, j] = 1 - np.sum(np.power(yhatbig1[:1000] - ytrain[:1000], 2)) / np.sum(np.power(ytrain[:1000] - mtrain, 2))\n",
    "\n",
    "        r2_oos[modeln - 1] = r[1, fw1(r[0, :].tolist())]\n",
    "        r2_is[modeln - 1] = r[2, fw1(r[0, :].tolist())]\n",
    "        b = B[:, fw1(r[0, :].tolist())]\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"PLS R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "            \n",
    "        # Elastic Net\n",
    "        modeln += 1\n",
    "        lamv = np.arange(-2, 4.1, 0.1)\n",
    "        alpha = 0.5\n",
    "        r = np.zeros((3, len(lamv)))\n",
    "\n",
    "        for j in range(len(lamv)):\n",
    "            l2 = 10 ** lamv[j]\n",
    "            func = soft_threshode\n",
    "            b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "            yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "            r[0, j] = 1 - np.sum(np.power(yhatbig1[:1000] - ytest[:1000], 2)) / np.sum(np.power(ytest[:1000] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "            r[1, j] = 1 - np.sum(np.power(yhatbig1[:1000] - yoos[:1000], 2)) / np.sum(np.power(yoos[:1000] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "            r[2, j] = 1 - np.sum(np.power(yhatbig1[:1000] - ytrain[:1000], 2)) / np.sum(np.power(ytrain[:1000] - mtrain, 2))\n",
    "\n",
    "        r2_oos[modeln - 1] = r[1, fw1(r[0, :].tolist())]\n",
    "        r2_is[modeln - 1] = r[2, fw1(r[0, :].tolist())]\n",
    "        l2 = 10 ** lamv[int(fw1(r[0]))]\n",
    "        func = soft_threshode\n",
    "        b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        np.savetxt(pathb, b, delimiter=',')\n",
    "        print('Enet R2 :', np.round(r2_oos[modeln-1], 3))\n",
    "\n",
    "        modeln += 1\n",
    "        func = soft_threshode\n",
    "        b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, l2, func)\n",
    "        yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "        r2_oos[modeln-1] = 1 - np.sum(np.power(yhatbig1[:1000] - yoos[:1000], 2)) / np.sum(np.power(yoos[:1000] - mtrain, 2))\n",
    "        yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "        r2_is[modeln-1] = 1 - np.sum(np.power(yhatbig1[:1000] - ytrain[:1000], 2)) / np.sum(np.power(ytrain[:1000] - mtrain, 2))\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        np.savetxt(pathb, b, delimiter=',')\n",
    "        print('Enet+H R2 :', np.round(r2_oos[modeln-1], 3))\n",
    "        \n",
    "        # Group Lasso\n",
    "        kn = 4\n",
    "        th = np.zeros((kn, xtrain.shape[1]))\n",
    "        th[1, :] = 0\n",
    "\n",
    "        for i in range(xtrain.shape[1]):\n",
    "            th[:, i] = np.quantile(xtrain[:, i], np.arange(kn) / kn)\n",
    "\n",
    "        xtrain = cut_knots_degree2(xtrain, kn, th)\n",
    "        xtest = cut_knots_degree2(xtest, kn, th)\n",
    "        xoos = cut_knots_degree2(xoos, kn, th)\n",
    "\n",
    "        for i in range(xtrain.shape[1]):\n",
    "            s = np.std(xtrain[:, i])\n",
    "            if s > 0:\n",
    "                xtrain[:, i] = xtrain[:, i] / s\n",
    "                xtest[:, i] = xtest[:, i] / s\n",
    "                xoos[:, i] = xoos[:, i] / s\n",
    "\n",
    "        Y = ytrain_demean\n",
    "        XX = xtrain.T @ xtrain\n",
    "        U, S, V = np.linalg.svd(XX)\n",
    "        L = S[0]\n",
    "        XY = xtrain.T @ Y\n",
    "\n",
    "        modeln += 1\n",
    "        lamv = np.arange(0.5, 3.1, 0.1)\n",
    "        nc = XX.shape[1] // (kn + 1)\n",
    "        groups = np.repeat(np.arange(1, nc + 1), kn + 1)\n",
    "        r = np.zeros((3, len(lamv)))\n",
    "\n",
    "        for j, lam in enumerate(lamv):\n",
    "            l2 = 10 ** lam\n",
    "            func = soft_threshodg\n",
    "            b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "            yhatbig1 = xtest @ b + mtrain\n",
    "            r[0, j] = 1 - np.sum((yhatbig1 - ytest) ** 2) / np.sum((ytest - mtrain) ** 2)\n",
    "            yhatbig1 = xoos @ b + mtrain\n",
    "            r[1, j] = 1 - np.sum((yhatbig1 - yoos) ** 2) / np.sum((yoos - mtrain) ** 2)\n",
    "            yhatbig1 = xtrain @ b + mtrain\n",
    "            r[2, j] = 1 - np.sum((yhatbig1 - ytrain) ** 2) / np.sum((ytrain - mtrain) ** 2)\n",
    "\n",
    "        r2_oos[modeln] = r[1, np.int16(fw1(r[0, :]))]\n",
    "        r2_is[modeln] = r[2, np.int16(fw1(r[0, :]))]\n",
    "        l2 = 10 ** lamv[np.int16(fw1(r[0, :]))]\n",
    "\n",
    "        func = soft_threshodg\n",
    "        b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "        pathb = f\"{title}/B/b\"\n",
    "        pathb = f\"{pathb}_{mo}_{M}_{modeln}.csv\"\n",
    "        np.savetxt(pathb, b, delimiter=\",\")\n",
    "\n",
    "        print(\"Group Lasso R2:\", np.round(r2_oos[modeln], 3))\n",
    "\n",
    "        modeln += 1\n",
    "        func = soft_threshodg\n",
    "        b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, l2, func)\n",
    "        yhatbig1 = xoos @ b + mtrain\n",
    "        r2_oos[modeln] = 1 - np.sum((yhatbig1[:1000] - yoos[:1000]) ** 2) / np.sum((yoos[:1000] - mtrain) ** 2)\n",
    "        yhatbig1 = xtrain @ b + mtrain\n",
    "        r2_is[modeln] = 1 - np.sum((yhatbig1[:1000] - ytrain[:1000]) ** 2) / np.sum((ytrain[:1000] - mtrain) ** 2)\n",
    "        pathb = f\"{title}/B/b\"\n",
    "        pathb = f\"{pathb}_{mo}_{M}_{modeln}.csv\"\n",
    "        np.savetxt(pathb, b, delimiter=\",\")\n",
    "\n",
    "        print(\"Group Lasso+H R2:\", np.round(r2_oos[modeln], 3))\n",
    "\n",
    "        pathr = f\"{title}/roos\"\n",
    "        pathr = f\"{pathr}_{mo}_{M}.csv\"\n",
    "        np.savetxt(pathr, r2_oos, delimiter=\",\")\n",
    "\n",
    "        pathr = f\"{title}/ris\"\n",
    "        pathr = f\"{pathr}_{mo}_{M}.csv\"\n",
    "        np.savetxt(pathr, r2_is, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a32dc",
   "metadata": {},
   "source": [
    "And for the bottom 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cdeb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = 1  # setup MC number\n",
    "datanum = 50  # Or datanum = 100; separately run two cases\n",
    "path = './Simu'  # set your own folder path    \n",
    "dirstock = f\"{path}/SimuData_d{datanum}/\"\n",
    "\n",
    "hh = [1]\n",
    "# hh = [1, 3, 6, 12]  # correspond to monthly, quarterly, half-year, and annually returns\n",
    "title = f\"{path}/Simu_p{datanum}/Reg{hh[0]}\"\n",
    "if not os.path.exists(title) and MC == 1:\n",
    "    os.makedirs(title)\n",
    "titleB = f\"{title}/B\"\n",
    "if not os.path.exists(titleB) and MC == 1:\n",
    "    os.makedirs(titleB)\n",
    "if datanum == 50:\n",
    "    nump = 50\n",
    "elif datanum == 100:\n",
    "    nump = 100\n",
    "mu = 0.2 * np.sqrt(hh[0])\n",
    "tol = 10**(-10)\n",
    "\n",
    "for M in range(1, 2):  # Assuming range for M\n",
    "    for mo in range(1, 2):  # Assuming range for mo\n",
    "        \n",
    "        print('### MCMC : {}, Model : {} ###'.format(M, mo))\n",
    "        \n",
    "        N = 200   # Number of CS tickers\n",
    "        m = nump * 2   # Number of Characteristics\n",
    "        T = 4  # Number of Time Periods\n",
    "        \n",
    "        per = np.tile(np.arange(1, N+1), T)\n",
    "        time = np.repeat(np.arange(1, T+1), N)\n",
    "        stdv = 0.05\n",
    "        theta_w = 0.005\n",
    "        \n",
    "        # Read Files\n",
    "        path1 = dirstock + 'c' + str(M) + '.csv'\n",
    "        path2 = dirstock + 'r' + str(mo) + '_' + str(M) + '.csv'\n",
    "        c = np.genfromtxt(path1, delimiter=',')\n",
    "        r1 = np.genfromtxt(path2, delimiter=',')\n",
    "        \n",
    "        # Add Some Elements\n",
    "        daylen = np.repeat(N, T//3)\n",
    "        daylen_test = daylen.copy()\n",
    "        ind = np.arange(0, N*T//3)\n",
    "        xtrain = c[ind]\n",
    "        ytrain = r1[ind]\n",
    "        trainper = per[ind]\n",
    "        start_idx = math.floor(N * T / 3) + 1\n",
    "        end_idx = math.floor(N * (T * 2 / 3 - hh[0] + 1))\n",
    "        ind = list(range(start_idx, end_idx))\n",
    "        xtest = c[ind]\n",
    "        ytest = r1[ind]\n",
    "        testper = per[ind]\n",
    "        \n",
    "        l1 = c.shape[0]\n",
    "        l2 = len(r1)\n",
    "        l3 = l2 - np.isnan(r1).sum()\n",
    "        \n",
    "        ind = np.arange(N*(2*T//3), min([l1, l2, l3]))\n",
    "        xoos = c[ind]\n",
    "        yoos = r1[ind]\n",
    "        \n",
    "        # Monthly Demean\n",
    "        ytrain_demean = ytrain - np.mean(ytrain)\n",
    "        ytest_demean = ytest - np.mean(ytest)\n",
    "        mtrain = np.mean(ytrain)\n",
    "        mtest = np.mean(ytest)\n",
    "        \n",
    "        # Calculate Sufficient Stats\n",
    "        sd = np.zeros(xtrain.shape[1])\n",
    "        for i in range(xtrain.shape[1]):\n",
    "            s = np.std(xtrain[:, i])\n",
    "            if s > 0:\n",
    "                xtrain[:, i] /= s\n",
    "                xtest[:, i] /= s\n",
    "                xoos[:, i] /= s\n",
    "                sd[i] = s\n",
    "        \n",
    "        XX = np.dot(xtrain.T, xtrain)\n",
    "        U, S, V = np.linalg.svd(XX)\n",
    "        L = S[0]\n",
    "        Y = ytrain_demean\n",
    "        XY = np.dot(xtrain.T, Y)\n",
    "        \n",
    "         \n",
    "        #OLS\n",
    "        \n",
    "        # Initialize arrays\n",
    "        r2_oos = np.zeros(13)\n",
    "        r2_is = np.zeros(13)\n",
    "        modeln = 0\n",
    "        groups = 0\n",
    "        nc = 0\n",
    "\n",
    "        # OLS\n",
    "        modeln += 1\n",
    "        clf = LinearRegression(fit_intercept=False)\n",
    "        clf.fit(xtrain, ytrain_demean)\n",
    "        yhatbig1 = clf.predict(xoos) + mtrain\n",
    "        r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xtrain) + mtrain\n",
    "        r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "        b = clf.coef_\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"Simple OLS R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "        # OLS+H\n",
    "        modeln += 1\n",
    "        func = soft_threshodl\n",
    "        b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, 0, func)\n",
    "        yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "        r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1[-1000:] - yoos[-1000:], 2)) / np.sum(np.power(yoos[-1000:]- mtrain, 2))\n",
    "        yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "        r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytrain[-1000:], 2)) / np.sum(np.power(ytrain[-1000:]- mtrain, 2))\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"Simple OLS+H R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "        # PCR\n",
    "        modeln += 1\n",
    "        ne = 30\n",
    "        X = np.dot(xtrain.T, xtrain)\n",
    "        pca_vec = V.T\n",
    "        p1 = pca_vec[:, :ne]\n",
    "        Z = np.dot(xtrain, p1)\n",
    "        r = np.zeros((3, ne))\n",
    "        B = np.zeros((xtrain.shape[1], ne))\n",
    "        Y = ytrain_demean\n",
    "\n",
    "        for j in range(ne - 1):\n",
    "            xx = Z[:, :j + 1]\n",
    "            b = np.dot(np.linalg.inv(np.dot(xx.T, xx)), np.dot(xx.T, Y))\n",
    "            b = np.dot(p1[:, :j + 1], b)\n",
    "\n",
    "            yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "            r[0, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytest[-1000:], 2)) / np.sum(np.power(ytest[-1000:] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "            r[1, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - yoos[-1000:], 2)) / np.sum(np.power(yoos[-1000:] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "            r[2, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytrain[-1000:], 2)) / np.sum(np.power(ytrain[-1000:] - mtrain, 2))\n",
    "            B[:, j] = b\n",
    "\n",
    "        b = np.zeros(xtest.shape[1])\n",
    "        j = ne - 1\n",
    "        yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "        r[0, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytest[-1000:], 2)) / np.sum(np.power(ytest[-1000:] - mtrain, 2))\n",
    "        yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "        r[1, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - yoos[-1000:], 2)) / np.sum(np.power(yoos[-1000:] - mtrain, 2))\n",
    "        yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "        r[2, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytrain[-1000:], 2)) / np.sum(np.power(ytrain[-1000:] - mtrain, 2))\n",
    "        B[:, j] = b\n",
    "\n",
    "        r2_oos[modeln - 1] = r[1, fw1(r[0, :].tolist())]\n",
    "        r2_is[modeln - 1] = r[2, fw1(r[0, :].tolist())]\n",
    "        b = B[:, fw1(r[0, :].tolist())]\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"PCR R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "        # PLS\n",
    "        modeln += 1\n",
    "        B = pls(xtrain, ytrain_demean, 30)\n",
    "        ne = 30\n",
    "        r = np.zeros((3, ne))\n",
    "        Y = ytrain_demean\n",
    "\n",
    "        for j in range(ne):\n",
    "            b = B[:, j]\n",
    "            yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "            r[0, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytest[-1000:], 2)) / np.sum(np.power(ytest[-1000:] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "            r[1, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - yoos[-1000:], 2)) / np.sum(np.power(yoos[-1000:] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "            r[2, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytrain[-1000:], 2)) / np.sum(np.power(ytrain[-1000:] - mtrain, 2))\n",
    "\n",
    "        r2_oos[modeln - 1] = r[1, fw1(r[0, :].tolist())]\n",
    "        r2_is[modeln - 1] = r[2, fw1(r[0, :].tolist())]\n",
    "        b = B[:, fw1(r[0, :].tolist())]\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"PLS R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "            \n",
    "        # Elastic Net\n",
    "        modeln += 1\n",
    "        lamv = np.arange(-2, 4.1, 0.1)\n",
    "        alpha = 0.5\n",
    "        r = np.zeros((3, len(lamv)))\n",
    "\n",
    "        for j in range(len(lamv)):\n",
    "            l2 = 10 ** lamv[j]\n",
    "            func = soft_threshode\n",
    "            b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "            yhatbig1 = np.dot(xtest, b) + mtrain\n",
    "            r[0, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytest[-1000:], 2)) / np.sum(np.power(ytest[-1000:] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "            r[1, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - yoos[-1000:], 2)) / np.sum(np.power(yoos[-1000:] - mtrain, 2))\n",
    "            yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "            r[2, j] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytrain[-1000:], 2)) / np.sum(np.power(ytrain[-1000:] - mtrain, 2))\n",
    "\n",
    "        r2_oos[modeln - 1] = r[1, fw1(r[0, :].tolist())]\n",
    "        r2_is[modeln - 1] = r[2, fw1(r[0, :].tolist())]\n",
    "        l2 = 10 ** lamv[int(fw1(r[0]))]\n",
    "        func = soft_threshode\n",
    "        b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        np.savetxt(pathb, b, delimiter=',')\n",
    "        print('Enet R2 :', np.round(r2_oos[modeln-1], 3))\n",
    "\n",
    "        modeln += 1\n",
    "        func = soft_threshode\n",
    "        b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, l2, func)\n",
    "        yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "        r2_oos[modeln-1] = 1 - np.sum(np.power(yhatbig1[-1000:] - yoos[-1000:], 2)) / np.sum(np.power(yoos[-1000:] - mtrain, 2))\n",
    "        yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "        r2_is[modeln-1] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytrain[-1000:], 2)) / np.sum(np.power(ytrain[-1000:] - mtrain, 2))\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        np.savetxt(pathb, b, delimiter=',')\n",
    "        print('Enet+H R2 :', np.round(r2_oos[modeln-1], 3))\n",
    "        \n",
    "        # Group Lasso\n",
    "        kn = 4\n",
    "        th = np.zeros((kn, xtrain.shape[1]))\n",
    "        th[1, :] = 0\n",
    "\n",
    "        for i in range(xtrain.shape[1]):\n",
    "            th[:, i] = np.quantile(xtrain[:, i], np.arange(kn) / kn)\n",
    "\n",
    "        xtrain = cut_knots_degree2(xtrain, kn, th)\n",
    "        xtest = cut_knots_degree2(xtest, kn, th)\n",
    "        xoos = cut_knots_degree2(xoos, kn, th)\n",
    "\n",
    "        for i in range(xtrain.shape[1]):\n",
    "            s = np.std(xtrain[:, i])\n",
    "            if s > 0:\n",
    "                xtrain[:, i] = xtrain[:, i] / s\n",
    "                xtest[:, i] = xtest[:, i] / s\n",
    "                xoos[:, i] = xoos[:, i] / s\n",
    "\n",
    "        Y = ytrain_demean\n",
    "        XX = xtrain.T @ xtrain\n",
    "        U, S, V = np.linalg.svd(XX)\n",
    "        L = S[0]\n",
    "        XY = xtrain.T @ Y\n",
    "\n",
    "        modeln += 1\n",
    "        lamv = np.arange(0.5, 3.1, 0.1)\n",
    "        nc = XX.shape[1] // (kn + 1)\n",
    "        groups = np.repeat(np.arange(1, nc + 1), kn + 1)\n",
    "        r = np.zeros((3, len(lamv)))\n",
    "\n",
    "        for j, lam in enumerate(lamv):\n",
    "            l2 = 10 ** lam\n",
    "            func = soft_threshodg\n",
    "            b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "            yhatbig1 = xtest @ b + mtrain\n",
    "            r[0, j] = 1 - np.sum((yhatbig1 - ytest) ** 2) / np.sum((ytest - mtrain) ** 2)\n",
    "            yhatbig1 = xoos @ b + mtrain\n",
    "            r[1, j] = 1 - np.sum((yhatbig1 - yoos) ** 2) / np.sum((yoos - mtrain) ** 2)\n",
    "            yhatbig1 = xtrain @ b + mtrain\n",
    "            r[2, j] = 1 - np.sum((yhatbig1 - ytrain) ** 2) / np.sum((ytrain - mtrain) ** 2)\n",
    "\n",
    "        r2_oos[modeln] = r[1, np.int16(fw1(r[0, :]))]\n",
    "        r2_is[modeln] = r[2, np.int16(fw1(r[0, :]))]\n",
    "        l2 = 10 ** lamv[np.int16(fw1(r[0, :]))]\n",
    "\n",
    "        func = soft_threshodg\n",
    "        b = proximal(groups, nc, XX, XY, tol, L, l2, func)\n",
    "        pathb = f\"{title}/B/b\"\n",
    "        pathb = f\"{pathb}_{mo}_{M}_{modeln}.csv\"\n",
    "        np.savetxt(pathb, b, delimiter=\",\")\n",
    "\n",
    "        print(\"Group Lasso R2:\", np.round(r2_oos[modeln], 3))\n",
    "\n",
    "        modeln += 1\n",
    "        func = soft_threshodg\n",
    "        b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, l2, func)\n",
    "        yhatbig1 = xoos @ b + mtrain\n",
    "        r2_oos[modeln] = 1 - np.sum((yhatbig1[-1000:] - yoos[-1000:]) ** 2) / np.sum((yoos[-1000:] - mtrain) ** 2)\n",
    "        yhatbig1 = xtrain @ b + mtrain\n",
    "        r2_is[modeln] = 1 - np.sum((yhatbig1[-1000:] - ytrain[-1000:]) ** 2) / np.sum((ytrain[-1000:] - mtrain) ** 2)\n",
    "        pathb = f\"{title}/B/b\"\n",
    "        pathb = f\"{pathb}_{mo}_{M}_{modeln}.csv\"\n",
    "        np.savetxt(pathb, b, delimiter=\",\")\n",
    "\n",
    "        print(\"Group Lasso+H R2:\", np.round(r2_oos[modeln], 3))\n",
    "\n",
    "        pathr = f\"{title}/roos\"\n",
    "        pathr = f\"{pathr}_{mo}_{M}.csv\"\n",
    "        np.savetxt(pathr, r2_oos, delimiter=\",\")\n",
    "\n",
    "        pathr = f\"{title}/ris\"\n",
    "        pathr = f\"{pathr}_{mo}_{M}.csv\"\n",
    "        np.savetxt(pathr, r2_is, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945af58",
   "metadata": {},
   "source": [
    "Now moving to boosing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade397fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = 1  # setup MC number\n",
    "datanum = 50  # Or datanum = 100; separately run two cases\n",
    "path = './Simu'  # set your own folder path\n",
    "dirstock = os.path.join(path, f'SimuData_d{datanum}/')\n",
    "hh = [1]\n",
    "mo = 1\n",
    "# for hh in [1, 3, 6, 12]:  # correspond to monthly quarterly halfyear and annually returns\n",
    "title = os.path.join(path, f'Simu_p{datanum}/Tree{hh}')\n",
    "\n",
    "if not os.path.isdir(title) and MC == 1:\n",
    "    os.makedirs(title)\n",
    "\n",
    "titleB = os.path.join(title, 'B')\n",
    "if not os.path.isdir(titleB) and MC == 1:\n",
    "    os.makedirs(titleB)\n",
    "\n",
    "if datanum == 50:\n",
    "    nump = 50\n",
    "if datanum == 100:\n",
    "    nump = 100\n",
    "N = 200  # Number of CS tickers\n",
    "m = nump * 2  # Number of Characteristics\n",
    "T = 180  # Number of Time Periods\n",
    "\n",
    "per = np.tile(np.arange(1, N + 1), T)\n",
    "time = np.repeat(np.arange(1, T + 1), N)\n",
    "stdv = 0.05\n",
    "theta_w = 0.005\n",
    "\n",
    "# Read Files\n",
    "path1 = f\"{dirstock}c{M}.csv\"\n",
    "path2 = f\"{dirstock}r{mo}_{M}.csv\"\n",
    "c = np.genfromtxt(path1, delimiter=',')\n",
    "r1 = np.genfromtxt(path2, delimiter=',')\n",
    "\n",
    "# Add Some Elements\n",
    "daylen = np.tile(N, T // 3)\n",
    "daylen_test = daylen\n",
    "ind = np.arange(0, int(N * T / 3))\n",
    "xtrain = c[ind, :]\n",
    "ytrain = r1[ind]\n",
    "trainper = per[ind]\n",
    "ind = np.arange(int(N * T / 3), int(N * (T * 2 / 3 + 1)))\n",
    "xtest = c[ind, :]\n",
    "ytest = r1[ind]\n",
    "testper = per[ind]\n",
    "\n",
    "l1 = c.shape[0]\n",
    "l2 = len(r1)\n",
    "l3 = l2 - np.sum(np.isnan(r1))\n",
    "\n",
    "ind = np.arange(int(N * T * 2 / 3), min(l1, l2, l3))\n",
    "xoos = c[ind, :]\n",
    "yoos = r1[ind]\n",
    "\n",
    "# Monthly Demean\n",
    "ytrain_demean = ytrain - np.mean(ytrain)\n",
    "ytest_demean = ytest - np.mean(ytest)\n",
    "mtrain = np.mean(ytrain)\n",
    "mtest = np.mean(ytest)\n",
    "\n",
    "# Start to train\n",
    "r2_oos = np.zeros(3)  # OOS R2\n",
    "r2_is = np.zeros(3)\n",
    "r3_oos = np.zeros(3) # IS R2\n",
    "\n",
    "# Random Forest\n",
    "if nump == 50:\n",
    "    lamv = np.arange(10, 101, 10)\n",
    "elif nump == 100:\n",
    "    lamv = np.arange(10, 201, 20)\n",
    "ne = 100\n",
    "lamc = [2, 4, 8, 16, 32]\n",
    "r = np.zeros((len(lamv), len(lamc), 3))\n",
    "rrr = np.zeros((len(lamv), len(lamc), 3))\n",
    "\n",
    "for n1 in tqdm(range(len(lamv))):\n",
    "    nf = lamv[n1]\n",
    "    for n2 in range(len(lamc)):\n",
    "        nn = lamc[n2]\n",
    "        clf = RandomForestRegressor(\n",
    "            n_estimators=ne,\n",
    "            max_features=nf,\n",
    "            max_depth=nn\n",
    "        )\n",
    "        clf.fit(xtrain, ytrain)\n",
    "        yhatbig1 = clf.predict(xtest)\n",
    "        \n",
    "        r[n1, n2, 0] = 1 - np.sum(np.power(yhatbig1[:1000] - ytest[:1000], 2)) / np.sum(\n",
    "        np.power(ytest[:1000] - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xoos)\n",
    "        r[n1, n2, 1] = 1 - np.sum(np.power(yhatbig1[:1000] - yoos[:1000], 2)) / np.sum(\n",
    "        np.power(yoos[:1000] - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xtrain)\n",
    "        r[n1, n2, 2] = 1 - np.sum(np.power(yhatbig1[:1000] - ytrain[:1000], 2)) / np.sum(\n",
    "        np.power(ytrain[:1000] - mtrain, 2))\n",
    "        \n",
    "        rrr[n1, n2, 0] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytest[-1000:], 2)) / np.sum(\n",
    "        np.power(ytest[-1000:] - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xoos)\n",
    "        rrr[n1, n2, 1] = 1 - np.sum(np.power(yhatbig1[-1000:] - yoos[-1000:], 2)) / np.sum(\n",
    "        np.power(yoos[-1000:] - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xtrain)\n",
    "        rrr[n1, n2, 2] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytrain[-1000:], 2)) / np.sum(\n",
    "        np.power(ytrain[-1000:] - mtrain, 2))\n",
    "\n",
    "fw_2 = np.unravel_index(np.argmax(r[:, :, 0]), r[:, :, 0].shape)\n",
    "fw_3 = np.unravel_index(np.argmax(rrr[:, :, 0]), rrr[:, :, 0].shape)\n",
    "\n",
    "r2_oos[0] = r[fw_2[0], fw_2[1], 1]\n",
    "r3_oos[0] = r[fw_3[0], fw_3[1], 1]\n",
    "print(f\"RF R2 for first 1000 : {r2_oos[0]:.3f}, RF R2 for last 1000 : {r3_oos[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea56b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting \n",
    "mo = 1\n",
    "\n",
    "lamv = np.arange(-1, 0.1, 0.2)\n",
    "r = np.zeros((len(lamv), 50, 3))\n",
    "rrr = np.zeros((len(lamv), 50, 3))\n",
    "\n",
    "for n1 in range(len(lamv)):\n",
    "    lr = 10 ** lamv[n1]\n",
    "    alpha = 2\n",
    "    ne = 50\n",
    "    clf = GradientBoostingRegressor(\n",
    "        n_estimators=ne,\n",
    "        learning_rate=lr,\n",
    "        loss='huber',\n",
    "        max_depth=2\n",
    "    )\n",
    "\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    e = clf.staged_predict(xtest)\n",
    "    for i, pred in enumerate(e):\n",
    "        r[n1, i, 0] = np.mean((pred - ytest) ** 2)\n",
    "\n",
    "    e = clf.staged_predict(xoos)\n",
    "    for i, pred in enumerate(e):\n",
    "        r[n1, i, 1] = np.mean((pred - yoos) ** 2)\n",
    "\n",
    "    e = clf.staged_predict(xtrain)\n",
    "    for i, pred in enumerate(e):\n",
    "        r[n1, i, 2] = np.mean((pred[:1000] - ytrain[:1000]) ** 2)\n",
    "        rrr[n1, i, 2] = np.mean((pred[-1000:] - ytrain[-1000:]) ** 2)\n",
    "        \n",
    "\n",
    "fw_2 = np.unravel_index(np.argmin(r[:, :, 0]), r[:, :, 0].shape)\n",
    "fw_3 = np.unravel_index(np.argmin(rrr[:, :, 0]), rrr[:, :, 0].shape)\n",
    "err2 = np.mean((yoos[:1000] - mtrain) ** 2)\n",
    "err3 = np.mean((yoos[-1000:] - mtrain) ** 2)\n",
    "r2_oos[1] = 1 - r[fw_2[0], fw_2[1], 1] / err2\n",
    "r3_oos[1] = 1 - r[fw_3[0], fw_3[1], 1] / err3\n",
    "print(f\"GBRT R2 for the first 1000 : {r2_oos[1]:.3f}, GBRT R2 for last 1000 : {r3_oos[1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92594d5",
   "metadata": {},
   "source": [
    "Special Case: OLS-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13fbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated = pd.concat([x_c, x_b], ignore_index=True)\n",
    "#assume that the 4th column is the Market Value of the stock. Proceeding to arrange it in accordance to the column values\n",
    "updated = updated.sort_values(by=updated.columns[3], ascending=False)\n",
    "rtop_1 = updated.iloc[:,:1]\n",
    "ctop1 = updated.iloc[:,1:4]\n",
    "\n",
    "rtop_1 = rtop_1.dropna()\n",
    "ctop1 = ctop1.dropna()\n",
    "\n",
    "rtop_1.to_csv('r1_1.csv', index=False, header=False)\n",
    "ctop1.to_csv('c1.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0069b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = 1\n",
    "datanum = 50  # Or datanum = 100; separately run two cases\n",
    "path = './Simu'  # set your own folder path    \n",
    "dirstock = f\"{path}/SimuData_z{datanum}/\"\n",
    "\n",
    "hh = [1]\n",
    "title = f\"{path}/Simu_p{datanum}/Reg{hh[0]}\"\n",
    "if not os.path.exists(title) and MC == 1:\n",
    "    os.makedirs(title)\n",
    "titleB = f\"{title}/B\"\n",
    "if not os.path.exists(titleB) and MC == 1:\n",
    "    os.makedirs(titleB)\n",
    "if datanum == 50:\n",
    "    nump = 50\n",
    "elif datanum == 100:\n",
    "    nump = 100\n",
    "mu = 0.2 * np.sqrt(hh[0])\n",
    "tol = 10**(-10)\n",
    "\n",
    "for M in range(1, 2):  # Assuming range for M\n",
    "    for mo in range(1, 2):  # Assuming range for mo\n",
    "        \n",
    "        print('### MCMC : {}, Model : {} ###'.format(M, mo))\n",
    "        \n",
    "        N = 200   # Number of CS tickers\n",
    "        m = nump * 2   # Number of Characteristics\n",
    "        T = 180   # Number of Time Periods\n",
    "        \n",
    "        per = np.tile(np.arange(1, N+1), T)\n",
    "        time = np.repeat(np.arange(1, T+1), N)\n",
    "        stdv = 0.05\n",
    "        theta_w = 0.005\n",
    "        \n",
    "        # Read Files\n",
    "        path1 = dirstock + 'c' + str(M) + '.csv'\n",
    "        path2 = dirstock + 'r' + str(mo) + '_' + str(M) + '.csv'\n",
    "        c = np.genfromtxt(path1, delimiter=',')\n",
    "        r1 = np.genfromtxt(path2, delimiter=',')\n",
    "        \n",
    "        # Add Some Elements\n",
    "        daylen = np.repeat(N, T//3)\n",
    "        daylen_test = daylen.copy()\n",
    "        ind = np.arange(0, N*T//3)\n",
    "        xtrain = c[ind]\n",
    "        ytrain = r1[ind]\n",
    "        trainper = per[ind]\n",
    "        start_idx = math.floor(N * T / 3) + 1\n",
    "        end_idx = math.floor(N * (T * 2 / 3 - hh[0] + 1))\n",
    "        ind = list(range(start_idx, end_idx))\n",
    "        xtest = c[ind]\n",
    "        ytest = r1[ind]\n",
    "        testper = per[ind]\n",
    "        \n",
    "        l1 = c.shape[0]\n",
    "        l2 = len(r1)\n",
    "        l3 = l2 - np.isnan(r1).sum()\n",
    "        \n",
    "        ind = np.arange(N*(2*T//3), min([l1, l2, l3]))\n",
    "        xoos = c[ind]\n",
    "        yoos = r1[ind]\n",
    "        \n",
    "        # Monthly Demean\n",
    "        ytrain_demean = ytrain - np.mean(ytrain)\n",
    "        ytest_demean = ytest - np.mean(ytest)\n",
    "        mtrain = np.mean(ytrain)\n",
    "        mtest = np.mean(ytest)\n",
    "        \n",
    "        # Calculate Sufficient Stats\n",
    "        sd = np.zeros(xtrain.shape[1])\n",
    "        for i in range(xtrain.shape[1]):\n",
    "            s = np.std(xtrain[:, i])\n",
    "            if s > 0:\n",
    "                xtrain[:, i] /= s\n",
    "                xtest[:, i] /= s\n",
    "                xoos[:, i] /= s\n",
    "                sd[i] = s\n",
    "        \n",
    "        XX = np.dot(xtrain.T, xtrain)\n",
    "        U, S, V = np.linalg.svd(XX)\n",
    "        L = S[0]\n",
    "        Y = ytrain_demean\n",
    "        XY = np.dot(xtrain.T, Y)\n",
    "        \n",
    "         \n",
    "        #OLS\n",
    "        \n",
    "        # Initialize arrays\n",
    "        r2_oos = np.zeros(13)\n",
    "        r2_is = np.zeros(13)\n",
    "        modeln = 0\n",
    "        groups = 0\n",
    "        nc = 0\n",
    "\n",
    "        # OLS\n",
    "        modeln += 1\n",
    "        clf = LinearRegression(fit_intercept=False)\n",
    "        clf.fit(xtrain, ytrain_demean)\n",
    "        yhatbig1 = clf.predict(xoos) + mtrain\n",
    "        r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xtrain) + mtrain\n",
    "        r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "        b = clf.coef_\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"Simple OLS R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "        # OLS+H\n",
    "        modeln += 1\n",
    "        func = soft_threshodl\n",
    "        b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, 0, func)\n",
    "        yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "        r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "        yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "        r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"Simple OLS+H R2: {r2_oos[modeln - 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24586276",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = 1\n",
    "datanum = 50  # Or datanum = 100; separately run two cases\n",
    "path = './Simu'  # set your own folder path    \n",
    "dirstock = f\"{path}/SimuData_z{datanum}/\"\n",
    "\n",
    "hh = [1]\n",
    "title = f\"{path}/Simu_p{datanum}/Reg{hh[0]}\"\n",
    "if not os.path.exists(title) and MC == 1:\n",
    "    os.makedirs(title)\n",
    "titleB = f\"{title}/B\"\n",
    "if not os.path.exists(titleB) and MC == 1:\n",
    "    os.makedirs(titleB)\n",
    "if datanum == 50:\n",
    "    nump = 50\n",
    "elif datanum == 100:\n",
    "    nump = 100\n",
    "mu = 0.2 * np.sqrt(hh[0])\n",
    "tol = 10**(-10)\n",
    "\n",
    "for M in range(1, 2):  # Assuming range for M\n",
    "    for mo in range(1, 2):  # Assuming range for mo\n",
    "        \n",
    "        print('### MCMC : {}, Model : {} ###'.format(M, mo))\n",
    "        \n",
    "        N = 200   # Number of CS tickers\n",
    "        m = nump * 2   # Number of Characteristics\n",
    "        T = 180   # Number of Time Periods\n",
    "        \n",
    "        per = np.tile(np.arange(1, N+1), T)\n",
    "        time = np.repeat(np.arange(1, T+1), N)\n",
    "        stdv = 0.05\n",
    "        theta_w = 0.005\n",
    "        \n",
    "        # Read Files\n",
    "        path1 = dirstock + 'c' + str(M) + '.csv'\n",
    "        path2 = dirstock + 'r' + str(mo) + '_' + str(M) + '.csv'\n",
    "        c = np.genfromtxt(path1, delimiter=',')\n",
    "        r1 = np.genfromtxt(path2, delimiter=',')\n",
    "        \n",
    "        # Add Some Elements\n",
    "        daylen = np.repeat(N, T//3)\n",
    "        daylen_test = daylen.copy()\n",
    "        ind = np.arange(0, N*T//3)\n",
    "        xtrain = c[ind]\n",
    "        ytrain = r1[ind]\n",
    "        trainper = per[ind]\n",
    "        start_idx = math.floor(N * T / 3) + 1\n",
    "        end_idx = math.floor(N * (T * 2 / 3 - hh[0] + 1))\n",
    "        ind = list(range(start_idx, end_idx))\n",
    "        xtest = c[ind]\n",
    "        ytest = r1[ind]\n",
    "        testper = per[ind]\n",
    "        \n",
    "        l1 = c.shape[0]\n",
    "        l2 = len(r1)\n",
    "        l3 = l2 - np.isnan(r1).sum()\n",
    "        \n",
    "        ind = np.arange(N*(2*T//3), min([l1, l2, l3]))\n",
    "        xoos = c[ind]\n",
    "        yoos = r1[ind]\n",
    "        \n",
    "        # Monthly Demean\n",
    "        ytrain_demean = ytrain - np.mean(ytrain)\n",
    "        ytest_demean = ytest - np.mean(ytest)\n",
    "        mtrain = np.mean(ytrain)\n",
    "        mtest = np.mean(ytest)\n",
    "        \n",
    "        # Calculate Sufficient Stats\n",
    "        sd = np.zeros(xtrain.shape[1])\n",
    "        for i in range(xtrain.shape[1]):\n",
    "            s = np.std(xtrain[:, i])\n",
    "            if s > 0:\n",
    "                xtrain[:, i] /= s\n",
    "                xtest[:, i] /= s\n",
    "                xoos[:, i] /= s\n",
    "                sd[i] = s\n",
    "        \n",
    "        XX = np.dot(xtrain.T, xtrain)\n",
    "        U, S, V = np.linalg.svd(XX)\n",
    "        L = S[0]\n",
    "        Y = ytrain_demean\n",
    "        XY = np.dot(xtrain.T, Y)\n",
    "        \n",
    "         \n",
    "        #OLS\n",
    "        \n",
    "        # Initialize arrays\n",
    "        r2_oos = np.zeros(13)\n",
    "        r2_is = np.zeros(13)\n",
    "        modeln = 0\n",
    "        groups = 0\n",
    "        nc = 0\n",
    "\n",
    "        # OLS\n",
    "        modeln += 1\n",
    "        clf = LinearRegression(fit_intercept=False)\n",
    "        clf.fit(xtrain, ytrain_demean)\n",
    "        yhatbig1 = clf.predict(xoos) + mtrain\n",
    "        r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1[:1000] - yoos[:1000], 2)) / np.sum(np.power(yoos[:1000] - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xtrain) + mtrain\n",
    "        r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1[:1000] - ytrain[:1000], 2)) / np.sum(np.power(ytrain[:1000] - mtrain, 2))\n",
    "        b = clf.coef_\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"Simple OLS R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "        # OLS+H\n",
    "        modeln += 1\n",
    "        func = soft_threshodl\n",
    "        b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, 0, func)\n",
    "        yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "        r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1[:1000] - yoos[:1000], 2)) / np.sum(np.power(yoos[:1000] - mtrain, 2))\n",
    "        yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "        r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1[:1000] - ytrain[:1000], 2)) / np.sum(np.power(ytrain[:1000] - mtrain, 2))\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"Simple OLS+H R2: {r2_oos[modeln - 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f80a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MC = 1\n",
    "datanum = 50  # Or datanum = 100; separately run two cases\n",
    "path = './Simu'  # set your own folder path    \n",
    "dirstock = f\"{path}/SimuData_z{datanum}/\"\n",
    "\n",
    "hh = [1]\n",
    "title = f\"{path}/Simu_p{datanum}/Reg{hh[0]}\"\n",
    "if not os.path.exists(title) and MC == 1:\n",
    "    os.makedirs(title)\n",
    "titleB = f\"{title}/B\"\n",
    "if not os.path.exists(titleB) and MC == 1:\n",
    "    os.makedirs(titleB)\n",
    "if datanum == 50:\n",
    "    nump = 50\n",
    "elif datanum == 100:\n",
    "    nump = 100\n",
    "mu = 0.2 * np.sqrt(hh[0])\n",
    "tol = 10**(-10)\n",
    "\n",
    "for M in range(1, 2):  # Assuming range for M\n",
    "    for mo in range(1, 2):  # Assuming range for mo\n",
    "        \n",
    "        print('### MCMC : {}, Model : {} ###'.format(M, mo))\n",
    "        \n",
    "        N = 200   # Number of CS tickers\n",
    "        m = nump * 2   # Number of Characteristics\n",
    "        T = 180   # Number of Time Periods\n",
    "        \n",
    "        per = np.tile(np.arange(1, N+1), T)\n",
    "        time = np.repeat(np.arange(1, T+1), N)\n",
    "        stdv = 0.05\n",
    "        theta_w = 0.005\n",
    "        \n",
    "        # Read Files\n",
    "        path1 = dirstock + 'c' + str(M) + '.csv'\n",
    "        path2 = dirstock + 'r' + str(mo) + '_' + str(M) + '.csv'\n",
    "        c = np.genfromtxt(path1, delimiter=',')\n",
    "        r1 = np.genfromtxt(path2, delimiter=',')\n",
    "        \n",
    "        # Add Some Elements\n",
    "        daylen = np.repeat(N, T//3)\n",
    "        daylen_test = daylen.copy()\n",
    "        ind = np.arange(0, N*T//3)\n",
    "        xtrain = c[ind]\n",
    "        ytrain = r1[ind]\n",
    "        trainper = per[ind]\n",
    "        start_idx = math.floor(N * T / 3) + 1\n",
    "        end_idx = math.floor(N * (T * 2 / 3 - hh[0] + 1))\n",
    "        ind = list(range(start_idx, end_idx))\n",
    "        xtest = c[ind]\n",
    "        ytest = r1[ind]\n",
    "        testper = per[ind]\n",
    "        \n",
    "        l1 = c.shape[0]\n",
    "        l2 = len(r1)\n",
    "        l3 = l2 - np.isnan(r1).sum()\n",
    "        \n",
    "        ind = np.arange(N*(2*T//3), min([l1, l2, l3]))\n",
    "        xoos = c[ind]\n",
    "        yoos = r1[ind]\n",
    "        \n",
    "        # Monthly Demean\n",
    "        ytrain_demean = ytrain - np.mean(ytrain)\n",
    "        ytest_demean = ytest - np.mean(ytest)\n",
    "        mtrain = np.mean(ytrain)\n",
    "        mtest = np.mean(ytest)\n",
    "        \n",
    "        # Calculate Sufficient Stats\n",
    "        sd = np.zeros(xtrain.shape[1])\n",
    "        for i in range(xtrain.shape[1]):\n",
    "            s = np.std(xtrain[:, i])\n",
    "            if s > 0:\n",
    "                xtrain[:, i] /= s\n",
    "                xtest[:, i] /= s\n",
    "                xoos[:, i] /= s\n",
    "                sd[i] = s\n",
    "        \n",
    "        XX = np.dot(xtrain.T, xtrain)\n",
    "        U, S, V = np.linalg.svd(XX)\n",
    "        L = S[0]\n",
    "        Y = ytrain_demean\n",
    "        XY = np.dot(xtrain.T, Y)\n",
    "        \n",
    "         \n",
    "        #OLS\n",
    "        \n",
    "        # Initialize arrays\n",
    "        r2_oos = np.zeros(13)\n",
    "        r2_is = np.zeros(13)\n",
    "        modeln = 0\n",
    "        groups = 0\n",
    "        nc = 0\n",
    "\n",
    "        # OLS\n",
    "        modeln += 1\n",
    "        clf = LinearRegression(fit_intercept=False)\n",
    "        clf.fit(xtrain, ytrain_demean)\n",
    "        yhatbig1 = clf.predict(xoos) + mtrain\n",
    "        r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - yoos, 2)) / np.sum(np.power(yoos - mtrain, 2))\n",
    "        yhatbig1 = clf.predict(xtrain) + mtrain\n",
    "        r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1 - ytrain, 2)) / np.sum(np.power(ytrain - mtrain, 2))\n",
    "        b = clf.coef_\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"Simple OLS R2: {r2_oos[modeln - 1]:.3f}\")\n",
    "\n",
    "        # OLS+H\n",
    "        modeln += 1\n",
    "        func = soft_threshodl\n",
    "        b = proximalH(groups, nc, xtest, mtrain, ytest, b, xtrain, ytrain_demean, mu, tol, L, 0, func)\n",
    "        yhatbig1 = np.dot(xoos, b) + mtrain\n",
    "        r2_oos[modeln - 1] = 1 - np.sum(np.power(yhatbig1[-1000:] - yoos[-1000:], 2)) / np.sum(np.power(yoos[-1000:] - mtrain, 2))\n",
    "        yhatbig1 = np.dot(xtrain, b) + mtrain\n",
    "        r2_is[modeln - 1] = 1 - np.sum(np.power(yhatbig1[-1000:] - ytrain[-1000:], 2)) / np.sum(np.power(ytrain[-1000:] - mtrain, 2))\n",
    "        pathb = f\"{title}/B/b{mo}_{M}_{modeln}.csv\"\n",
    "        pd.DataFrame(b).to_csv(pathb, index=False, header=False)\n",
    "        print(f\"Simple OLS+H R2: {r2_oos[modeln - 1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
