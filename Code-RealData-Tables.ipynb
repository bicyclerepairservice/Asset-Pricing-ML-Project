{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634fc3c0",
   "metadata": {},
   "source": [
    "# Stock Price Forecasting with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcca1c",
   "metadata": {},
   "source": [
    "## 0. Preliminary workings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292dd48",
   "metadata": {},
   "source": [
    "### 0.1 Uploading necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6bb1572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dguse\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\dguse\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\dguse\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import norm, t, uniform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from linearmodels.panel import PanelOLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "from numpy.linalg import svd\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from group_lasso import GroupLasso\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.optimizers import Adam\n",
    "from dieboldmariano import dm_test\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c074dc",
   "metadata": {},
   "source": [
    "### 0.2.1 Defining few functions (may not be relevant anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20228b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fw1(x):\n",
    "    # Find the maximum location of a vector\n",
    "    maximum = np.max(x)\n",
    "    p = np.where(x == maximum)[0]\n",
    "    if len(p) > 1:\n",
    "        p = p[0]\n",
    "    return p\n",
    "\n",
    "def pls(X, y, A):\n",
    "    \"\"\"\n",
    "    Partial Least Squares (PLS) regression\n",
    "    \n",
    "    Parameters:\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target values.\n",
    "    A : int\n",
    "        Number of components.\n",
    "        \n",
    "    Returns:\n",
    "    B : array, shape (n_features, A)\n",
    "        Coefficients for each component.\n",
    "    \"\"\"\n",
    "    # Initialize arrays to store intermediate results\n",
    "    s = X.T.dot(y)\n",
    "    R = np.zeros((X.shape[1], A))\n",
    "    TT = np.zeros((X.shape[0], A))\n",
    "    P = np.zeros((X.shape[1], A))\n",
    "    U = np.zeros((X.shape[0], A))\n",
    "    V = np.zeros((X.shape[1], A))\n",
    "    B = np.zeros((X.shape[1], A))\n",
    "    Q = np.zeros((1, A))\n",
    "\n",
    "    # Iterate through each component\n",
    "    for i in range(A):\n",
    "        # Calculate loading vector for X and Y\n",
    "        q = s.T.dot(s)\n",
    "        r = s.dot(q)\n",
    "        t = X.dot(r)\n",
    "        t = t - np.mean(t)\n",
    "        normt = np.sqrt(t.T.dot(t))\n",
    "        t = t / normt\n",
    "        r = r / normt\n",
    "        p = X.T.dot(t)\n",
    "        q = y.T.dot(t)\n",
    "        u = y * q\n",
    "        v = p\n",
    "\n",
    "        # Calculate deflation\n",
    "        if i > 0:\n",
    "            v = v - V[:, :i+1].dot(V[:, :i+1].T.dot(p))\n",
    "            u = u - TT[:, :i+1].dot(TT[:, :i+1].T.dot(u))\n",
    "        v = v / np.sqrt(v.T.dot(v))\n",
    "        s = s - v.dot(v.T.dot(s))\n",
    "\n",
    "        # Store results for current component\n",
    "        R[:, i] = r\n",
    "        TT[:, i] = t\n",
    "        P[:, i] = p\n",
    "        U[:, i] = u\n",
    "        V[:, i] = v\n",
    "        Q[:, i] = q\n",
    "\n",
    "    # Reconstruct the coefficient matrix B\n",
    "    for i in range(A - 1):\n",
    "        C = R[:, :i+1].dot(Q[:, :i+1].T)\n",
    "        B[:, i+1] = C[:, 0]\n",
    "\n",
    "    return B\n",
    "\n",
    "def soft_threshodl(groups, nc, w, mu):\n",
    "    \"\"\"\n",
    "    Soft thresholding operator\n",
    "    \n",
    "    Parameters:\n",
    "    groups : int\n",
    "        Not used in the function, placeholder for the MATLAB code.\n",
    "    nc : int\n",
    "        Not used in the function, placeholder for the MATLAB code.\n",
    "    w : array-like\n",
    "        Input array.\n",
    "    mu : float\n",
    "        Threshold parameter.\n",
    "    \n",
    "    Returns:\n",
    "    val : array-like\n",
    "        Soft thresholded array.\n",
    "    \"\"\"\n",
    "    val = np.sign(w) * np.maximum(np.abs(w) - mu, 0)\n",
    "    return val\n",
    "\n",
    "def lossh(y, yhat, mu):\n",
    "    \"\"\"\n",
    "    Loss function for proximalH\n",
    "    \n",
    "    Parameters:\n",
    "    y : array-like\n",
    "        True target values.\n",
    "    yhat : array-like\n",
    "        Predicted values.\n",
    "    mu : float\n",
    "        Threshold parameter.\n",
    "    \n",
    "    Returns:\n",
    "    m : float\n",
    "        Loss value.\n",
    "    \"\"\"\n",
    "    r = np.abs(yhat - y)\n",
    "    l = np.zeros(len(r))\n",
    "    ind = (r > mu)\n",
    "    l[ind] = 2 * mu * r[ind] - mu * mu\n",
    "    ind = (r <= mu)\n",
    "    l[ind] = r[ind] * r[ind]\n",
    "    m = np.mean(l)\n",
    "    return m\n",
    "\n",
    "def f_gradh(w, X, y, mu):\n",
    "    \"\"\"\n",
    "    Gradient of the loss function for proximalH\n",
    "    \n",
    "    Parameters:\n",
    "    w : array-like\n",
    "        Coefficients.\n",
    "    X : array-like\n",
    "        Training data.\n",
    "    y : array-like\n",
    "        Target values.\n",
    "    mu : float\n",
    "        Threshold parameter.\n",
    "    \n",
    "    Returns:\n",
    "    grad : array-like\n",
    "        Gradient.\n",
    "    \"\"\"\n",
    "    r = np.dot(X, w) - y\n",
    "    ind0 = np.where(np.abs(r) <= mu)[0]\n",
    "    ind1 = np.where(r > mu)[0]\n",
    "    indf1 = np.where(r < -mu)[0]\n",
    "    grad = np.dot(X[ind0, :].T, np.dot(X[ind0, :], w) - y[ind0]) + mu * np.dot(X[ind1, :].T, np.ones(len(ind1))) - mu * np.dot(X[indf1, :].T, np.ones(len(indf1)))\n",
    "    return grad\n",
    "\n",
    "def proximalH(groups, nc, xtest, mtrain, ytest, w, X, y, mu, tol, L, l2, func):\n",
    "    \"\"\"\n",
    "    Proximal operator using accelerated proximal gradient descent\n",
    "    \n",
    "    Parameters:\n",
    "    groups : int\n",
    "        Not used in the function, placeholder for the MATLAB code.\n",
    "    nc : int\n",
    "        Not used in the function, placeholder for the MATLAB code.\n",
    "    xtest : array-like\n",
    "        Test data.\n",
    "    mtrain : float\n",
    "        Mean of the training target values.\n",
    "    ytest : array-like\n",
    "        Test target values.\n",
    "    w : array-like\n",
    "        Initial guess of the coefficients.\n",
    "    X : array-like\n",
    "        Training data.\n",
    "    y : array-like\n",
    "        Target values.\n",
    "    mu : float\n",
    "        Threshold parameter.\n",
    "    tol : float\n",
    "        Tolerance parameter for convergence.\n",
    "    L : float\n",
    "        Lipschitz constant.\n",
    "    l2 : float\n",
    "        Regularization parameter.\n",
    "    func : function\n",
    "        Soft thresholding function.\n",
    "    \n",
    "    Returns:\n",
    "    a : array-like\n",
    "        Final coefficients after proximal gradient descent.\n",
    "    \"\"\"\n",
    "    dim = X.shape[1]\n",
    "    max_iter = 3000\n",
    "    gamma = 1 / L\n",
    "    l1 = l2\n",
    "    v = w.copy()\n",
    "    yhatbig1 = np.dot(xtest, w) + mtrain\n",
    "    r20 = lossh(yhatbig1, ytest, mu)\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        vold = v.copy()\n",
    "        w_perv = w.copy()\n",
    "        w = v - gamma * f_gradh(v, X, y, mu)\n",
    "        mu1 = l1 * gamma\n",
    "        w = func(groups, nc, w, mu1)\n",
    "        v = w + t / (t + 3) * (w - w_perv)\n",
    "        \n",
    "        if np.sum((v - vold) ** 2) < (np.sum(vold ** 2) * tol) or np.sum(np.abs(v - vold)) == 0:\n",
    "            break\n",
    "    \n",
    "    return v\n",
    "\n",
    "def proximal(groups, nc, XX, XY, tol, L, l2, func):\n",
    "    '''\n",
    "    groups: Group membership of features\n",
    "    nc: Number of classes\n",
    "    XX: Matrix of features\n",
    "    XY: Matrix of labels\n",
    "    tol: Tolerance for convergence\n",
    "    L: Lipschitz constant\n",
    "    l2: L2 regularization parameter\n",
    "    func: Proximal operator function\n",
    "    '''\n",
    "    dim = XX.shape[0]\n",
    "    max_iter = 30000\n",
    "    gamma = 1 / L\n",
    "    l1 = l2\n",
    "    w = np.zeros(dim)\n",
    "    v = w.copy()\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        vold = v.copy()\n",
    "        w_prev = w.copy()\n",
    "        w = v - gamma * f_grad(XX, XY, v)\n",
    "        w = func(groups, nc, w, l1 * gamma)\n",
    "        v = w + t / (t + 3) * (w - w_prev)\n",
    "        if (np.sum(np.power(v - vold, 2)) < (np.sum(np.power(vold, 2)) * tol)) or (np.sum(np.abs(v - vold)) == 0):\n",
    "            break\n",
    "\n",
    "    return v\n",
    "\n",
    "def f_grad(XX, XY, w):\n",
    "    \"\"\"\n",
    "    Gradient of the objective function.\n",
    "\n",
    "    Parameters:\n",
    "    XX (array): Design matrix.\n",
    "    XY (array): Target values.\n",
    "    w (array): Coefficients.\n",
    "\n",
    "    Returns:\n",
    "    grad (array): Gradient.\n",
    "    \"\"\"\n",
    "    grad = np.dot(XX, w) - XY\n",
    "    return grad\n",
    "\n",
    "def soft_threshodr(groups, nc, w, mu):\n",
    "    \"\"\"\n",
    "    Soft thresholding function for ridge regularization.\n",
    "\n",
    "    Parameters:\n",
    "    groups (array): Groups.\n",
    "    nc (int): Number of components.\n",
    "    w (array): Coefficients.\n",
    "    mu (float): Threshold parameter.\n",
    "\n",
    "    Returns:\n",
    "    val (array): Updated coefficients after soft thresholding.\n",
    "    \"\"\"\n",
    "    val = w / (1 + mu)\n",
    "    return val\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cut_knots_degree2(x, n, th):\n",
    "    '''\n",
    "    x: Input matrix\n",
    "    n: Degree of the polynomial\n",
    "    th: Threshold values for knot points\n",
    "    '''\n",
    "    a, b = x.shape  # Get the dimensions of the input matrix\n",
    "    resultfinal = np.zeros((a, b * (n + 1)))  # Initialize the final result matrix\n",
    "\n",
    "    for i in range(b):\n",
    "        xcut = x[:, i]  # Extract a column of the input matrix\n",
    "        xcutnona = np.copy(xcut)  # Create a copy of the column\n",
    "        xcutnona[np.isnan(xcutnona)] = 0  # Replace NaN values with 0\n",
    "        index = ((1 - 1 * np.isnan(xcut)) == 1)  # Find non-NaN indices\n",
    "\n",
    "        t = th[:, i]  # Get the threshold values for this column\n",
    "\n",
    "        x1 = xcutnona\n",
    "        resultfinal[:, (n + 1) * i - n] = x1 - np.mean(x1)  # Store the original values\n",
    "\n",
    "        x1 = np.power(xcutnona - t[0], 2)\n",
    "        resultfinal[:, (n + 1) * i - n + 1] = x1 - np.mean(x1)  # Store the squared differences with the first threshold\n",
    "\n",
    "        for j in range(n - 1):\n",
    "            x1 = np.power(xcutnona - t[j + 1], 2) * (xcutnona >= t[j + 1])  # Calculate squared differences with subsequent thresholds\n",
    "            resultfinal[:, (n + 1) * i - n + 1 + j] = x1 - np.mean(x1)  # Store the result in the final matrix\n",
    "\n",
    "    return resultfinal\n",
    "\n",
    "def soft_threshode(groups, nc, w, mu):\n",
    "    '''\n",
    "    groups: Group membership of features\n",
    "    nc: Number of classes\n",
    "    w: Input vector\n",
    "    mu: Threshold parameter\n",
    "    '''\n",
    "    # Apply soft thresholding operation element-wise\n",
    "    val = np.sign(w) * np.maximum(np.abs(w) - 0.5 * mu, 0) / (1 + 0.5 * mu)\n",
    "    \n",
    "    return val\n",
    "\n",
    "def soft_threshodg(groups, nc, w, mu):\n",
    "    '''\n",
    "    groups: Group membership of features\n",
    "    nc: Number of classes\n",
    "    w: Input vector\n",
    "    mu: Threshold parameter\n",
    "    '''\n",
    "    w1 = np.copy(w)  # Create a copy of the input vector\n",
    "    for i in range(1, nc + 1):  # Iterate over the number of classes\n",
    "        ind = (groups == i)  # Identify indices corresponding to the current class\n",
    "        wg = w1[ind]  # Extract the values of w corresponding to the current class\n",
    "        nn = len(wg)  # Get the number of elements in the current class\n",
    "        n2 = np.sqrt(np.sum(wg ** 2))  # Calculate the L2 norm of the current class\n",
    "        if n2 <= mu:  # Check if the L2 norm is less than or equal to the threshold parameter mu\n",
    "            w1[ind] = np.zeros(nn)  # Set the values of w corresponding to the current class to zero\n",
    "        else:\n",
    "            w1[ind] = wg - mu * wg / n2  # Apply the soft thresholding operation to the values of w corresponding to the current class\n",
    "    return w1  # Return the updated vector w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ee9c1",
   "metadata": {},
   "source": [
    "### 0.2.2. New set of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40a87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "# Huber objective function\n",
    "def huber(actual, predicted, xi):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, diff**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss)\n",
    "\n",
    "# Scoring Function\n",
    "# out-of-sample R squared\n",
    "def R_oos(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted).flatten()\n",
    "    predicted = np.where(predicted<0,0,predicted)\n",
    "    return 1 - (np.dot((actual-predicted),(actual-predicted)))/(np.dot(actual,actual))\n",
    "\n",
    "# Validation Function\n",
    "def val_fun(model, params: dict, X_trn, y_trn, X_vld, y_vld, illustration=True, sleep=0, is_NN=False):\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            if is_NN:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            else:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            best_mod = mod\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {best_ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "        else:\n",
    "            #time.sleep(sleep)\n",
    "            if is_NN:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            else:\n",
    "                mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_mod = mod\n",
    "                best_param = param\n",
    "    if illustration:\n",
    "        print('\\n'+'#'*60)\n",
    "        print('Tuning process finished!!!')\n",
    "        print(f'The best setting is: {best_param}')\n",
    "        print(f'with R2oos {best_ros*100:.2f}% on validation set.')\n",
    "        print('#'*60)\n",
    "    return best_mod\n",
    "    \n",
    "\n",
    "# Pairwise Comparison\n",
    "# Diebold-Mariano test statistics\n",
    "\n",
    "# Evaluation Output\n",
    "def evaluate(actual, predicted, insample=False):\n",
    "    if insample:\n",
    "        print('*'*15+'In-Sample Metrics'+'*'*15)\n",
    "        print(f'The in-sample R2 is {r2_score(actual,predicted)*100:.2f}%')\n",
    "        print(f'The in-sample MSE is {mean_squared_error(actual,predicted):.3f}')\n",
    "    else:\n",
    "        print('*'*15+'Out-of-Sample Metrics'+'*'*15)\n",
    "        print(f'The out-of-sample R2 is {R_oos(actual,predicted)*100:.2f}%')\n",
    "        print(f'The out-of-sample MSE is {mean_squared_error(actual,predicted):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1a2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCRegressor:\n",
    "    def __init__(self,n_PCs=1,loss='mse'):\n",
    "        self.n_PCs = n_PCs\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        X = np.array(X)\n",
    "        N,K = X.shape\n",
    "        y = np.array(y_trn).reshape((N,1))\n",
    "        self.mu = np.mean(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.std(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.where(self.sigma==0,1,self.sigma)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        pca = PCA()\n",
    "        X = pca.fit_transform(X)[:,:self.n_PCs]\n",
    "        self.pc_coef = pca.components_.T[:,:self.n_PCs]\n",
    "        if self.loss == 'mse':\n",
    "            self.model = LinearRegression().fit(X,y)\n",
    "        else:\n",
    "            self.model = HuberRegressor().fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X = np.array(X)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        X = X @ self.pc_coef\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def get_params(self, X,y):\n",
    "        X = np.array(X)\n",
    "        N,K = X.shape\n",
    "        y = np.array(y_trn).reshape((N,1))\n",
    "        self.mu = np.mean(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.std(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.where(self.sigma==0,1,self.sigma)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        pca = PCA()\n",
    "        X = pca.fit_transform(X)[:,:self.n_PCs]\n",
    "        self.pc_coef = pca.components_.T[:,:self.n_PCs]\n",
    "        return self.pc_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633450df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse\n",
    "def mse(actual, predicted):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    return np.mean(resid**2)\n",
    "\n",
    "# huber objective function\n",
    "def huber(actual, predicted, xi):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, diff**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss)\n",
    "\n",
    "# proximal operator\n",
    "def prox(theta,lmd,rho,gamma):\n",
    "    return (1/(1+lmd*gamma*rho))*softhred(theta,(1-rho)*gamma*lmd)\n",
    "\n",
    "# soft-thresholding operator\n",
    "def softhred(x,mu):\n",
    "    x = np.where(np.abs(x)<=mu, 0, x)\n",
    "    x = np.where((np.abs(x)>mu) & (x>0), x-mu, x)\n",
    "    x = np.where((np.abs(x)>mu) & (x<0), x+mu, x)\n",
    "    return x\n",
    "\n",
    "# penalized mse\n",
    "def mse_pnl(theta, X, y, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    resid = y - X@theta.reshape((K,1))\n",
    "    return np.mean(resid**2) + lmd*(1-rho)*np.sum(np.abs(theta)) + 0.5*lmd*rho*np.sum(theta**2)\n",
    "\n",
    "# penalized huber objective function\n",
    "def huber_pnl(theta, X, y, xi, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    resid = y - X@theta.reshape((K,1))\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, resid**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss) + lmd*(1-rho)*np.sum(np.abs(theta)) + 0.5*lmd*rho*np.sum(theta**2)\n",
    "\n",
    "# gradient of mse\n",
    "def grad_mse(theta, X, y, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    grad = (X.T @ (y - X@theta))/N + lmd*(1-rho)*np.where(theta>0,1,-1) + lmd*rho*theta\n",
    "    return grad.flatten()\n",
    "\n",
    "# gradient of huber loss\n",
    "def grad_huber(theta, X, y, xi, lmd, rho):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    resid = y - X@theta\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    try:\n",
    "        grad_m = X[ind_m].T @ (y[ind_m] - X[ind_m]@theta)\n",
    "    except:\n",
    "        grad_m = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_u = 2*xi* X[ind_u].T@np.ones((len(ind_u[0]),1))\n",
    "    except:\n",
    "        grad_u = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_l = -2*xi* X[ind_l].T@np.ones((len(ind_l[0]),1))\n",
    "    except:\n",
    "        grad_l = np.zeros((K,1))\n",
    "    grad = (grad_m+grad_u+grad_l)/N + lmd*(1-rho)*np.where(theta>0,1,-1) + lmd*rho*theta\n",
    "    return grad.flatten()\n",
    "\n",
    "# Elastic Net\n",
    "class ENet:\n",
    "    \n",
    "    def __init__(\n",
    "        self, lmd, rho, xi=1.35, loss='huber', random_state=None, fit_intercept=True\n",
    "    ):\n",
    "        self.lmd = lmd\n",
    "        self.rho = rho\n",
    "        self.xi = xi\n",
    "        self.random_state = random_state\n",
    "        self.fit_intercept = fit_intercept\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "            \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        K = X.shape[1]\n",
    "        X = np.array(X)\n",
    "        N = len(y)\n",
    "        y = np.array(y).reshape((N,1))\n",
    "        if self.fit_intercept:\n",
    "            K += 1\n",
    "            X = np.concatenate((np.ones((N,1)),X),axis=1)\n",
    "        # initialize theta\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "            theta = np.random.uniform(K)\n",
    "        else:\n",
    "            theta = np.zeros(K)\n",
    "        \n",
    "        if self.loss == 'huber':\n",
    "            res = minimize(\n",
    "                partial(huber_pnl, X=X, y=y, xi=self.xi, lmd=self.lmd, rho=self.rho), theta,\n",
    "                method='nelder-mead',\n",
    "                #method='BFGS',\n",
    "                #jac = partial(grad_huber, X=X, y=y, xi=self.xi, lmd=self.lmd, rho=self.rho),\n",
    "                options = {'disp': True}\n",
    "            )\n",
    "        else:\n",
    "            res = minimize(\n",
    "                partial(mse_pnl, X=X, y=y, lmd=self.lmd, rho=self.rho), theta, \n",
    "                method='nelder-mead',\n",
    "                #method='BFGS',\n",
    "                #jac = partial(grad_mse, X=X, y=y, lmd=self.lmd, rho=self.rho),\n",
    "                options = {'disp': True}\n",
    "            )\n",
    "        \n",
    "        self.theta = res.x.reshape((K,1))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        N = X.shape[0]\n",
    "        if self.fit_intercept:\n",
    "            X = np.concatenate((np.ones((N,1)),X),axis=1)\n",
    "        return X@self.theta\n",
    "    \n",
    "    def get_params():\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6224fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def SplineTransform(data,knots=3):\n",
    "    spline_data = pd.DataFrame(np.ones((data.shape[0],1)),index=data.index,columns=['const'])\n",
    "    for i in data.columns:\n",
    "        i_dat = data.loc[:,i]\n",
    "        i_sqr = i_dat**2\n",
    "        i_cut, bins = pd.cut(i_dat, 3, right=True, ordered=True, retbins=True)\n",
    "        i_dum = pd.get_dummies(i_cut)\n",
    "        for j in np.arange(knots):\n",
    "            i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
    "        i_dum.columns = [f\"{i}_{k}\" for k in np.arange(1,knots+1)]\n",
    "        spline_data = pd.concat((spline_data,i_dat,i_dum),axis=1)\n",
    "    return spline_data\n",
    "\n",
    "class GLMRegression:\n",
    "    \n",
    "    def __init__(self,knots=3,lmd=1e-4,l1_reg=1e-4,random_state=12308):\n",
    "        self.knots = knots\n",
    "        self.lmd = lmd\n",
    "        self.random_state = random_state\n",
    "        self.l1_reg = l1_reg\n",
    "        \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        groups = [0]+flatten([list(np.repeat(i,self.knots+1))[:] for i in np.arange(1,X.shape[1]+1)])\n",
    "        X = SplineTransform(X)\n",
    "        self.mod = GroupLasso(\n",
    "            groups=groups,group_reg=self.lmd,l1_reg=self.l1_reg,\n",
    "            fit_intercept=False,random_state=self.random_state\n",
    "        )\n",
    "        self.mod = self.mod.fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X = SplineTransform(X)\n",
    "        return self.mod.predict(X)\n",
    "    \n",
    "    def get_params(self,X,y):\n",
    "        groups = [0]+flatten([list(np.repeat(i,self.knots+1))[:] for i in np.arange(1,X.shape[1]+1)])\n",
    "        X_trans = SplineTransform(X)\n",
    "        indexes = [X_trans.columns.get_loc(col) for col in X.columns]\n",
    "        self.mod = GroupLasso(\n",
    "            groups=groups,group_reg=self.lmd,l1_reg=self.l1_reg,\n",
    "            fit_intercept=False,random_state=self.random_state\n",
    "        )\n",
    "        coeffis = self.mod.fit(X_trans,y).coef_\n",
    "        self.good = [coeffis[i] for i in indexes]\n",
    "        return self.good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e85a1",
   "metadata": {},
   "source": [
    "### 0.3 Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = pd.read_csv('datashare.csv')\n",
    "r1 = pd.read_csv('funny_data.csv')\n",
    "some = pd.read_csv('small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df154ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1['DATE'] = pd.to_datetime(c1['DATE'],format='%Y%m%d')+pd.offsets.MonthEnd(0)\n",
    "r1['DATE'] = r1['date']\n",
    "r1['DATE'] = pd.to_datetime(r1['DATE'])+pd.offsets.MonthEnd(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.to_csv('./Simu/Simu_p50/c1_1.csv', index=False)\n",
    "r1.to_csv('./Simu/Simu_p50/r1_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06800a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = pd.read_csv(\"./Simu/Simu_p50/c1_1.csv\")\n",
    "r1 = pd.read_csv(\"./Simu/Simu_p50/r1_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a02118",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.Categorical(c1['permno'])\n",
    "c1 = c1.set_index([\"permno\", \"DATE\"])\n",
    "c1['stock'] = stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3f1370",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.Categorical(r1[\"PERMNO\"])\n",
    "r1 = r1.set_index([\"PERMNO\", \"date\"])\n",
    "r1['stock'] = stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "066ce44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.index.names = ['permno', 'date']\n",
    "r1.index.names = ['permno', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "538a81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = c1.sort_index(level=['permno', 'date'])\n",
    "r1 = r1.sort_index(level=['permno', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fad9afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_date_index = pd.to_datetime(r1.index.get_level_values(level = \"date\"))\n",
    "new_index = pd.MultiIndex.from_tuples(zip(r1['stock'], new_date_index), names=['permno', 'date'])\n",
    "r1.index = new_index\n",
    "\n",
    "new_date_index = pd.to_datetime(c1.index.get_level_values(level = \"date\"))\n",
    "new_index = pd.MultiIndex.from_tuples(zip(c1['stock'], new_date_index), names=['permno', 'date'])\n",
    "c1.index = new_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26985922",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93c4fa",
   "metadata": {},
   "source": [
    "#### Set the prediod of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb81ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting = pd.to_datetime('2018-01-01')\n",
    "ending = pd.to_datetime('2022-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5c067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_small = c1[(c1.index.get_level_values(level = \"date\") >= starting) & (c1.index.get_level_values(level = \"date\") <= ending)]\n",
    "r1_small = r1[(r1.index.get_level_values(level = \"date\") >= starting) & (r1.index.get_level_values(level = \"date\") <= ending)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b9e00",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc0571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = c1_small.merge(r1_small, left_index=True, right_on=['permno', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26c2add",
   "metadata": {},
   "source": [
    "### 0.3.1 Cleaning the data from bad tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f33242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1['TICKER'] = pd.Categorical(b1['TICKER'])\n",
    "additional = pd.DataFrame(b1['TICKER'].value_counts())\n",
    "bad_spisok = additional[additional['count'] <= 5].index\n",
    "\n",
    "b1 = b1[~b1['TICKER'].isin(bad_spisok)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b5da551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalIndex(['CHAA', 'ACQR', 'BRSP', 'AAQC', 'ATVC', 'ACND', 'ACBA',\n",
       "                  'AADR', 'CIU', 'ZIP',\n",
       "                  ...\n",
       "                  'CCRD', 'SPCX', 'PEAR', 'WULF', 'GUG', 'LCW', 'SARK', 'CDRE',\n",
       "                  'WYIG', 'MYNA'],\n",
       "                 categories=['A', 'AA', 'AAAP', 'AABA', ..., 'ZY', 'ZYME', 'ZYNE', 'ZYXI'], ordered=False, dtype='category', name='TICKER', length=1362)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_spisok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a42e18",
   "metadata": {},
   "source": [
    "### 0.4 Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d45ddb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mvel1         44\n",
       "beta       14573\n",
       "betasq     14573\n",
       "chmom      12642\n",
       "dolvol      1356\n",
       "           ...  \n",
       "ASKHI          0\n",
       "PRC          689\n",
       "VOL            0\n",
       "RET          690\n",
       "SPREAD    189624\n",
       "Length: 104, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = b1.drop(columns = ['stock_x', 'stock_y', 'DATE', 'TICKER', 'RETX'])\n",
    "b1['RET'] = pd.to_numeric(b1['RET'], errors = \"coerce\")\n",
    "\n",
    "characteristics = list(set(b1.columns).difference({'SHROUT','mve0','sic2','prc', \"SICCD\"}))\n",
    "b1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86ce2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in enumerate(b1['RET']):\n",
    "    if isinstance(value, str):\n",
    "        b1.at[index, 'RET'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4149e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.66 s\n",
      "Wall time: 3.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fill na with cross-sectional median\n",
    "for ch in characteristics:\n",
    "     b1[ch] = b1.groupby('date')[ch].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37ed4ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sic2'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ch in characteristics:\n",
    "     b1[ch] = b1[ch].fillna(0)\n",
    "    \n",
    "b1.columns[b1.isnull().sum()!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72998ab",
   "metadata": {},
   "source": [
    "### 0.5 Adjusting Industrical Classification variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea4ad17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sic_dummies(data_ch):\n",
    "    sic_dummies = pd.get_dummies(b1['sic2'].fillna(999).astype(int),prefix='sic').drop('sic_999',axis=1)\n",
    "    b1_d = pd.concat([b1,sic_dummies],axis=1)\n",
    "    b1_d.drop(['sic2'],inplace=True,axis=1)\n",
    "    b1_d.drop(['SICCD'],inplace=True,axis=1)\n",
    "    return b1_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34475b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    101\n",
       "bool        68\n",
       "int64        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_d = get_sic_dummies(b1)\n",
    "b1_d.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2923801",
   "metadata": {},
   "source": [
    "### 0.6 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61c28249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "index_co = b1_d.index\n",
    "features = list(set(b1_d.columns).difference({'permno','DATE','RET'}))\n",
    "X = MinMaxScaler((-1,1)).fit_transform(b1_d[features])\n",
    "X = pd.DataFrame(X, columns=features, index=index_co)\n",
    "y = b1_d['RET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f3c5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stdt_vld = starting + (ending - starting)/3\n",
    "stdt_tst = starting + 2*(ending - starting)/3\n",
    "\n",
    "X_trn = X[X.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "y_trn = y[y.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "\n",
    "X_vld = X[(X.index.get_level_values(level = \"date\") >= stdt_vld) & (X.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "y_vld = y[(y.index.get_level_values(level = \"date\") >= stdt_vld) & (y.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "\n",
    "X_tst = X[X.index.get_level_values(level = \"date\") >= stdt_tst]\n",
    "y_tst = y[y.index.get_level_values(level = \"date\") >= stdt_tst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519147f4",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068809c5",
   "metadata": {},
   "source": [
    "## 1.0 Linear Model (Old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "890cf012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                    RET   R-squared:                        0.0308\n",
      "Estimator:                   PanelOLS   R-squared (Between):             -0.3514\n",
      "No. Observations:              192242   R-squared (Within):               0.0434\n",
      "Date:                Wed, Apr 24 2024   R-squared (Overall):              0.0308\n",
      "Time:                        08:02:17   Log-likelihood                 3.695e+04\n",
      "Cov. Estimator:            Unadjusted                                           \n",
      "                                        F-statistic:                      60.555\n",
      "Entities:                        7177   P-value                           0.0000\n",
      "Avg Obs:                       26.786   Distribution:              F(101,192140)\n",
      "Min Obs:                       1.0000                                           \n",
      "Max Obs:                       33.000   F-statistic (robust):             60.555\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                      33   Distribution:              F(101,192140)\n",
      "Avg Obs:                       5825.5                                           \n",
      "Min Obs:                       5484.0                                           \n",
      "Max Obs:                       6235.0                                           \n",
      "                                                                                \n",
      "                                Parameter Estimates                                \n",
      "===================================================================================\n",
      "                 Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-----------------------------------------------------------------------------------\n",
      "const               0.0123     0.0072     1.6948     0.0901     -0.0019      0.0265\n",
      "pchgm_pchsale      -0.0005     0.0007    -0.7128     0.4760     -0.0017      0.0008\n",
      "pchsaleinv          0.0004     0.0010     0.4026     0.6873     -0.0015      0.0023\n",
      "currat              0.0003     0.0002     1.4294     0.1529     -0.0001      0.0008\n",
      "sgr                -0.0031     0.0015    -2.0487     0.0405     -0.0060     -0.0001\n",
      "PERMCO            -1.7e-07  3.792e-08    -4.4836     0.0000  -2.443e-07  -9.569e-08\n",
      "stdacc          -4.902e-06  1.446e-05    -0.3390     0.7346  -3.325e-05   2.344e-05\n",
      "pchcurrat           0.0013     0.0034     0.3983     0.6904     -0.0053      0.0080\n",
      "convind            -0.0041     0.0020    -2.0930     0.0363     -0.0080     -0.0003\n",
      "zerotrade           0.0042     0.0007     6.3721     0.0000      0.0029      0.0055\n",
      "rd_sale           5.68e-05  5.976e-05     0.9505     0.3419  -6.033e-05      0.0002\n",
      "secured            -0.0115     0.0016    -7.1264     0.0000     -0.0146     -0.0083\n",
      "sin                -0.0068     0.0056    -1.1955     0.2319     -0.0178      0.0043\n",
      "mom6m              -0.0026     0.0037    -0.7078     0.4791     -0.0098      0.0046\n",
      "cfp_ia             -0.0010     0.0006    -1.8465     0.0648     -0.0021   6.401e-05\n",
      "baspread            0.8757     0.0393     22.256     0.0000      0.7985      0.9528\n",
      "cashpr           3.662e-05   1.22e-05     3.0010     0.0027    1.27e-05   6.053e-05\n",
      "lgr                 0.0021     0.0013     1.6356     0.1019     -0.0004      0.0046\n",
      "mvel1           -6.466e-11  1.471e-11    -4.3948     0.0000   -9.35e-11  -3.582e-11\n",
      "BIDLO            -1.24e-05  3.144e-06    -3.9449     0.0001  -1.857e-05  -6.241e-06\n",
      "pchsale_pchxsga    -0.0111     0.0027    -4.1271     0.0000     -0.0163     -0.0058\n",
      "ear                 0.0535     0.0063     8.4315     0.0000      0.0411      0.0660\n",
      "mve_ia          -8.866e-08  3.517e-08    -2.5207     0.0117  -1.576e-07  -1.972e-08\n",
      "betasq             -0.0045     0.0008    -5.9896     0.0000     -0.0060     -0.0030\n",
      "cfp                -0.0053     0.0033    -1.6073     0.1080     -0.0119      0.0012\n",
      "turn               -0.0014     0.0001    -11.141     0.0000     -0.0016     -0.0011\n",
      "chatoia             0.0179     0.0041     4.3325     0.0000      0.0098      0.0260\n",
      "retvol             -0.2331     0.0719    -3.2414     0.0012     -0.3741     -0.0922\n",
      "grltnoa             0.0068     0.0055     1.2309     0.2184     -0.0040      0.0176\n",
      "idiovol            -0.2904     0.0180    -16.126     0.0000     -0.3257     -0.2551\n",
      "indmom             -0.0203     0.0017    -12.316     0.0000     -0.0236     -0.0171\n",
      "bm                 -0.0002     0.0003    -0.5762     0.5644     -0.0007      0.0004\n",
      "cashdebt            0.0006     0.0005     1.2640     0.2062     -0.0003      0.0016\n",
      "ps                 -0.0022     0.0004    -5.2178     0.0000     -0.0030     -0.0014\n",
      "PRC              2.209e-05  3.894e-06     5.6723     0.0000   1.445e-05   2.972e-05\n",
      "absacc             -0.0027     0.0080    -0.3429     0.7316     -0.0183      0.0129\n",
      "saleinv         -2.072e-06  5.391e-06    -0.3843     0.7008  -1.264e-05   8.495e-06\n",
      "tb                 -0.0003     0.0003    -0.8180     0.4134     -0.0009      0.0004\n",
      "dy                  0.0053     0.0052     1.0317     0.3022     -0.0048      0.0154\n",
      "stdcf            3.648e-07  4.599e-06     0.0793     0.9368   -8.65e-06   9.379e-06\n",
      "HSICMG              0.0006  6.831e-05     8.7418     0.0000      0.0005      0.0007\n",
      "divo               -0.0012     0.0037    -0.3268     0.7438     -0.0086      0.0061\n",
      "maxret              0.1095     0.0186     5.8866     0.0000      0.0730      0.1460\n",
      "mom36m              0.0014     0.0009     1.4861     0.1373     -0.0004      0.0032\n",
      "salerec         -5.971e-05  2.529e-05    -2.3608     0.0182     -0.0001  -1.014e-05\n",
      "chinv              -0.1225     0.0275    -4.4553     0.0000     -0.1764     -0.0686\n",
      "chmom               0.0086     0.0019     4.4492     0.0000      0.0048      0.0124\n",
      "ep                  0.0064     0.0027     2.4063     0.0161      0.0012      0.0117\n",
      "pctacc           6.557e-05  8.448e-05     0.7762     0.4376     -0.0001      0.0002\n",
      "agr                -0.0014     0.0025    -0.5796     0.5622     -0.0063      0.0034\n",
      "rd_mve              0.0273     0.0070     3.8982     0.0001      0.0136      0.0410\n",
      "divi               -0.0097     0.0034    -2.8516     0.0044     -0.0164     -0.0030\n",
      "chempia             0.0006     0.0002     2.7982     0.0051      0.0002      0.0010\n",
      "operprof            0.0012     0.0010     1.1907     0.2338     -0.0008      0.0031\n",
      "invest              0.0335     0.0069     4.8347     0.0000      0.0199      0.0471\n",
      "sp                  0.0012     0.0004     3.1869     0.0014      0.0005      0.0019\n",
      "mom12m              0.0022     0.0017     1.3401     0.1802     -0.0010      0.0055\n",
      "pricedelay         -0.0011     0.0009    -1.1705     0.2418     -0.0028      0.0007\n",
      "quick              -0.0002     0.0003    -0.8897     0.3736     -0.0007      0.0003\n",
      "bm_ia               0.0004     0.0003     1.4605     0.1441     -0.0001      0.0009\n",
      "age                -0.0002  5.545e-05    -3.5735     0.0004     -0.0003  -8.946e-05\n",
      "cinvest             0.0003  4.478e-05     5.8666     0.0000      0.0002      0.0004\n",
      "roavol             -0.0287     0.0090    -3.1861     0.0014     -0.0464     -0.0110\n",
      "mom1m              -0.0644     0.0041    -15.845     0.0000     -0.0724     -0.0564\n",
      "ASKHI           -9.714e-06   2.82e-06    -3.4442     0.0006  -1.524e-05  -4.186e-06\n",
      "beta                0.0094     0.0021     4.4071     0.0000      0.0052      0.0136\n",
      "rd                  0.0020     0.0018     1.1336     0.2570     -0.0015      0.0055\n",
      "hire               -0.0084     0.0028    -3.0460     0.0023     -0.0139     -0.0030\n",
      "dolvol             -0.0022     0.0003    -7.7004     0.0000     -0.0028     -0.0017\n",
      "egr                -0.0014     0.0007    -2.0473     0.0406     -0.0028  -6.123e-05\n",
      "lev                 0.0003     0.0002     1.3500     0.1770     -0.0001      0.0007\n",
      "ms                  0.0015     0.0004     3.4767     0.0005      0.0006      0.0023\n",
      "roaq                0.0099     0.0116     0.8533     0.3935     -0.0128      0.0327\n",
      "chtx               -0.0482     0.0489    -0.9856     0.3243     -0.1440      0.0476\n",
      "chcsho             -0.0107     0.0022    -4.9169     0.0000     -0.0149     -0.0064\n",
      "VOL              1.939e-08  5.038e-10     38.477     0.0000    1.84e-08   2.037e-08\n",
      "grcapx             -0.0003     0.0002    -1.4129     0.1577     -0.0007      0.0001\n",
      "orgcap              0.1233     0.1474     0.8363     0.4030     -0.1657      0.4123\n",
      "chpmia           1.061e-05  4.606e-05     0.2304     0.8178  -7.966e-05      0.0001\n",
      "acc                -0.0072     0.0085    -0.8470     0.3970     -0.0239      0.0095\n",
      "pchdepr            -0.0116     0.0013    -9.1160     0.0000     -0.0141     -0.0091\n",
      "pchsale_pchrect     0.0013     0.0008     1.5545     0.1201     -0.0003      0.0030\n",
      "roic                0.0003     0.0003     0.7728     0.4397     -0.0004      0.0009\n",
      "aeavol             -0.0006     0.0004    -1.7139     0.0866     -0.0013   8.868e-05\n",
      "gma                 0.0063     0.0030     2.1283     0.0333      0.0005      0.0121\n",
      "herf                0.0029     0.0064     0.4542     0.6497     -0.0096      0.0155\n",
      "pchquick           -0.0007     0.0031    -0.2415     0.8092     -0.0068      0.0053\n",
      "std_turn        -6.936e-05  1.444e-05    -4.8032     0.0000  -9.766e-05  -4.106e-05\n",
      "depr                0.0012     0.0008     1.4454     0.1483     -0.0004      0.0028\n",
      "salecash        -9.571e-06  7.334e-06    -1.3051     0.1919  -2.395e-05   4.803e-06\n",
      "realestate          0.0283     0.0035     8.1371     0.0000      0.0215      0.0351\n",
      "nincr              -0.0045     0.0005    -8.7505     0.0000     -0.0055     -0.0035\n",
      "SPREAD             -0.0098     0.0010    -9.4024     0.0000     -0.0118     -0.0078\n",
      "rsup               -0.0483     0.0048    -10.177     0.0000     -0.0577     -0.0390\n",
      "pchcapx_ia      -1.785e-05  1.577e-05    -1.1322     0.2575  -4.875e-05   1.305e-05\n",
      "tang               -0.0125     0.0038    -3.3040     0.0010     -0.0200     -0.0051\n",
      "cash               -0.0006     0.0039    -0.1428     0.8865     -0.0081      0.0070\n",
      "securedind          0.0005     0.0012     0.4499     0.6528     -0.0018      0.0029\n",
      "roeq                0.0009     0.0025     0.3739     0.7085     -0.0040      0.0059\n",
      "ill              2.554e-07     124.56   2.05e-09     1.0000     -244.14      244.14\n",
      "std_dolvol         -0.0146     0.0017    -8.7721     0.0000     -0.0179     -0.0113\n",
      "pchsale_pchinvt    -0.0018     0.0011    -1.6776     0.0934     -0.0040      0.0003\n",
      "===================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PooledOLS - this is what I need to do. Plus add the Huber Func\n",
    "exog_vars = list(set(b1.columns).difference({'sic2','RET', \"SICCD\"}))\n",
    "exog = sm.add_constant(b1[exog_vars])\n",
    "mod = PanelOLS(b1.RET, exog, check_rank=False)\n",
    "pooled_res = mod.fit()\n",
    "print(pooled_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f7994e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some stocks and check how they behave. Check the data consostency and clear bad stocks (write them down in additional list.\n",
    "# Create a table with each bad ticker and associated problem (and check the paper for the advice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81854a",
   "metadata": {},
   "source": [
    "## 1.1 Linear Models (New)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4ca6c",
   "metadata": {},
   "source": [
    "### 1.1.1 OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a9854a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 2.77%\n",
      "The in-sample MSE is 0.029\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 1.28%\n",
      "The out-of-sample MSE is 0.050\n",
      "CPU times: total: 6.06 s\n",
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Simple OLS with MSE as a loss function\n",
    "\n",
    "# OLS with all features\n",
    "OLS = LinearRegression().fit(X_trn,y_trn)\n",
    "evaluate(y_trn, OLS.predict(X_trn), insample=True)\n",
    "evaluate(y_tst, OLS.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b428ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 1.65%\n",
      "The in-sample MSE is 0.029\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.83%\n",
      "The out-of-sample MSE is 0.050\n",
      "CPU times: total: 19.7 s\n",
      "Wall time: 9.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### OLS with a Huber loss function\n",
    "\n",
    "# OLS by Huber robust objective function with all features\n",
    "epsilon = np.max(((y_trn-OLS.predict(X_trn)).quantile(.999),1))\n",
    "OLS_H = HuberRegressor(epsilon=epsilon).fit(X_trn,y_trn)\n",
    "evaluate(y_trn, OLS_H.predict(X_trn), insample=True)\n",
    "evaluate(y_tst, OLS_H.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23de4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS with preselected size, bm, and momentum covariates\n",
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']\n",
    "OLS_3 = LinearRegression().fit(X_trn[features_3],y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e53195",
   "metadata": {},
   "source": [
    "### 1.1.2 PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22522759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'n_components': 1} finished.\n",
      "with out-of-sample R squared on validation set: 1.21289%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 5} finished.\n",
      "with out-of-sample R squared on validation set: 2.55827%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 10} finished.\n",
      "with out-of-sample R squared on validation set: 2.25932%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 50} finished.\n",
      "with out-of-sample R squared on validation set: 2.43065%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'n_components': 5}\n",
      "with R2oos 2.56% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 13.2 s\n",
      "Wall time: 6.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {'n_components': [1, 5, 10, 50]}\n",
    "PLS = val_fun(PLSRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a36089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 2.48%\n",
      "The in-sample MSE is 0.029\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 1.32%\n",
      "The out-of-sample MSE is 0.049\n",
      "CPU times: total: 219 ms\n",
      "Wall time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pls_pred_is = PLS.predict(X_trn)\n",
    "pls_pred_os = PLS.predict(X_tst)\n",
    "evaluate(y_trn, pls_pred_is, insample=True) \n",
    "evaluate(y_tst, pls_pred_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0c721",
   "metadata": {},
   "source": [
    "### 1.1.3 PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09ec5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'loss': 'mse', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: -0.08341%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: -0.08293%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: -0.03206%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: -0.00721%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: 0.28307%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 1.23369%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: -0.14658%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: -0.14463%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: -0.15016%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: -0.14419%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: -0.13010%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 0.17786%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'loss': 'mse', 'n_PCs': 50}\n",
      "with R2oos 1.23% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 2min 2s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {'n_PCs':[1,3,5,7,10,50],'loss':['mse','huber']}\n",
    "PCR = val_fun(PCRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,sleep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3921e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.86%\n",
      "The in-sample MSE is 0.030\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is -0.40%\n",
      "The out-of-sample MSE is 0.042\n",
      "CPU times: total: 1.06 s\n",
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, PCR.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, PCR.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651805eb",
   "metadata": {},
   "source": [
    "### 1.1.4 Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d36c2076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.023946\n",
      "         Iterations: 6547\n",
      "         Function evaluations: 8658\n",
      "CPU times: total: 9min 3s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "random.seed(12308)\n",
    "EN_my_mse_rdm = ENet(lmd=.01,rho=.5,loss='huber').fit(X_trn,y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a607409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is -0.07%\n",
      "The in-sample MSE is 0.030\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.03%\n",
      "The out-of-sample MSE is 0.041\n",
      "CPU times: total: 141 ms\n",
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, EN_my_mse_rdm.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, EN_my_mse_rdm.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441ee80",
   "metadata": {},
   "source": [
    "### 1.1.5 GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "977532cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: 0.15672%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: 0.21839%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'knots': 3, 'l1_reg': 0, 'lmd': 0.0001}\n",
      "with R2oos 0.22% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 11min 16s\n",
      "Wall time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'knots':[3],\n",
    "    'lmd':[1e-4,1e-1],#list(np.linspace(1e-4,1e-1,10)),\n",
    "    'l1_reg':[1e-4,0]\n",
    "}\n",
    "GLM = val_fun(GLMRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d7b6c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 0.24%\n",
      "The in-sample MSE is 0.030\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 0.46%\n",
      "The out-of-sample MSE is 0.040\n",
      "CPU times: total: 54.8 s\n",
      "Wall time: 54.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, GLM.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, GLM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c13f2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns = [\"OLS + H\", \"OLS-3 + H\", \"PCR\", \"PLS\", \"Enet + H\", \"GLM + H\", \"RF\", \"GBRT + H\"], data=np.zeros((3,8)))\n",
    "output.index = [\"All\", \"Top 1000\", \"Bottom 1000\"]\n",
    "output.iloc[0,0] =r2_score(y_tst, OLS_H.predict(X_tst))\n",
    "output.iloc[0,2] = r2_score(y_tst, PCR.predict(X_tst))\n",
    "output.iloc[0,3] = r2_score(y_tst, PLS.predict(X_tst))\n",
    "output.iloc[0,4] = r2_score(y_tst, EN_my_mse_rdm.predict(X_tst))\n",
    "output.iloc[0,5] = r2_score(y_tst, GLM.predict(X_tst))\n",
    "\n",
    "output.iloc[0,1] = r2_score(y_tst, OLS_3.predict(X_tst[features_3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eff6ce",
   "metadata": {},
   "source": [
    "### Diebold-Mariano tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8003b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "moriano = pd.DataFrame(columns = [\"OLS-3 + H\", \"PLS\", \"PCR\", \"Enet + H\", \"GLM + H\", \"RF\", \"GBRT + H\", \"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"], index = [\"OLS + H\", \"OLS-3 + H\", \"PLS\", \"PCR\", \"Enet + H\", \"GLM + H\", \"RF\", \"GBRT + H\", \"NN1\", \"NN2\", \"NN3\", \"NN4\"], data=np.zeros((12,12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7041ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "moriano.iloc[0,0] = dm_test(y_tst.values, OLS_H.predict(X_tst), OLS_3.predict(X_tst[features_3]), one_sided = False)[0]\n",
    "moriano.iloc[0,1] = dm_test(y_tst.values, OLS_H.predict(X_tst), PLS.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[0,2] = dm_test(y_tst.values, OLS_H.predict(X_tst), PCR.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[0,3] = dm_test(y_tst.values, OLS_H.predict(X_tst), EN_my_mse_rdm.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[0,4] = dm_test(y_tst.values,OLS_H.predict(X_tst), GLM.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[1,1] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), PLS.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[1,2] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), PCR.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[1,3] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), EN_my_mse_rdm.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[1,4] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), GLM.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[2,2] = dm_test(y_tst.values, PLS.predict(X_tst), PCR.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[2,3] = dm_test(y_tst.values, PLS.predict(X_tst), EN_my_mse_rdm.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[2,4] = dm_test(y_tst.values, PLS.predict(X_tst), GLM.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[3,3] = dm_test(y_tst.values, PCR.predict(X_tst), EN_my_mse_rdm.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[3,4] = dm_test(y_tst.values, PCR.predict(X_tst), GLM.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[4,4] = dm_test(y_tst.values, EN_my_mse_rdm.predict(X_tst), GLM.predict(X_tst), one_sided = False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "875766e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OLS-3 + H</th>\n",
       "      <th>PLS</th>\n",
       "      <th>PCR</th>\n",
       "      <th>Enet + H</th>\n",
       "      <th>GLM + H</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBRT + H</th>\n",
       "      <th>NN1</th>\n",
       "      <th>NN2</th>\n",
       "      <th>NN3</th>\n",
       "      <th>NN4</th>\n",
       "      <th>NN5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OLS + H</th>\n",
       "      <td>44.667607</td>\n",
       "      <td>13.617104</td>\n",
       "      <td>50.423308</td>\n",
       "      <td>47.538610</td>\n",
       "      <td>47.408230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS-3 + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-37.000430</td>\n",
       "      <td>-6.857542</td>\n",
       "      <td>21.822947</td>\n",
       "      <td>24.353913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.498729</td>\n",
       "      <td>41.597149</td>\n",
       "      <td>41.618567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.385630</td>\n",
       "      <td>21.651692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enet + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.784107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLM + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBRT + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           OLS-3 + H        PLS        PCR   Enet + H    GLM + H   RF  \\\n",
       "OLS + H    44.667607  13.617104  50.423308  47.538610  47.408230  0.0   \n",
       "OLS-3 + H   0.000000 -37.000430  -6.857542  21.822947  24.353913  0.0   \n",
       "PLS         0.000000   0.000000  42.498729  41.597149  41.618567  0.0   \n",
       "PCR         0.000000   0.000000   0.000000  20.385630  21.651692  0.0   \n",
       "Enet + H    0.000000   0.000000   0.000000   0.000000  17.784107  0.0   \n",
       "GLM + H     0.000000   0.000000   0.000000   0.000000   0.000000  0.0   \n",
       "RF          0.000000   0.000000   0.000000   0.000000   0.000000  0.0   \n",
       "GBRT + H    0.000000   0.000000   0.000000   0.000000   0.000000  0.0   \n",
       "NN1         0.000000   0.000000   0.000000   0.000000   0.000000  0.0   \n",
       "NN2         0.000000   0.000000   0.000000   0.000000   0.000000  0.0   \n",
       "NN3         0.000000   0.000000   0.000000   0.000000   0.000000  0.0   \n",
       "NN4         0.000000   0.000000   0.000000   0.000000   0.000000  0.0   \n",
       "\n",
       "           GBRT + H  NN1  NN2  NN3  NN4  NN5  \n",
       "OLS + H         0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "OLS-3 + H       0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "PLS             0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "PCR             0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "Enet + H        0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "GLM + H         0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "RF              0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "GBRT + H        0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "NN1             0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "NN2             0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "NN3             0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "NN4             0.0  0.0  0.0  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moriano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27cefc",
   "metadata": {},
   "source": [
    "### Coefficient Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "121aaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "significo = pd.DataFrame(columns = [\"PLS\", \"PCR\", \"Enet + H\", \"GLM + H\", \"RF\", \"GBRT + H\", \"NN1\", \"NN2\", \"NN3\", \"NN4\", \"NN5\"], index = X_tst.columns, data=np.zeros((169,11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "571e60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "significo[\"PLS\"] = PLS.coef_\n",
    "\n",
    "joe = PCRegressor()\n",
    "significo[\"PCR\"] = joe.get_params(X_trn, y_trn).flatten()\n",
    "\n",
    "#significo[\"Enet + H\"] = EN_my_mse_rdm.coef_\n",
    "\n",
    "joe = GLMRegression()\n",
    "flattened_list = []\n",
    "significo[\"GLM + H\"] = joe.get_params(X_trn,y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4efd14ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLS</th>\n",
       "      <th>PCR</th>\n",
       "      <th>Enet + H</th>\n",
       "      <th>GLM + H</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBRT + H</th>\n",
       "      <th>NN1</th>\n",
       "      <th>NN2</th>\n",
       "      <th>NN3</th>\n",
       "      <th>NN4</th>\n",
       "      <th>NN5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pchgm_pchsale</th>\n",
       "      <td>-0.002017</td>\n",
       "      <td>-0.058591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_80</th>\n",
       "      <td>0.000681</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_55</th>\n",
       "      <td>0.000914</td>\n",
       "      <td>-0.010684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currat</th>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.036253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgr</th>\n",
       "      <td>-0.000459</td>\n",
       "      <td>0.052028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_81</th>\n",
       "      <td>0.000663</td>\n",
       "      <td>-0.002312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ill</th>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.014122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_49</th>\n",
       "      <td>0.000419</td>\n",
       "      <td>-0.026146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.001054</td>\n",
       "      <td>-0.116730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pchsale_pchinvt</th>\n",
       "      <td>-0.001179</td>\n",
       "      <td>0.023227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      PLS       PCR  Enet + H GLM + H   RF  GBRT + H  NN1  \\\n",
       "pchgm_pchsale   -0.002017 -0.058591       0.0   [0.0]  0.0       0.0  0.0   \n",
       "sic_80           0.000681 -0.000182       0.0   [0.0]  0.0       0.0  0.0   \n",
       "sic_55           0.000914 -0.010684       0.0   [0.0]  0.0       0.0  0.0   \n",
       "currat           0.000368  0.036253       0.0   [0.0]  0.0       0.0  0.0   \n",
       "sgr             -0.000459  0.052028       0.0  [-0.0]  0.0       0.0  0.0   \n",
       "...                   ...       ...       ...     ...  ...       ...  ...   \n",
       "sic_81           0.000663 -0.002312       0.0  [-0.0]  0.0       0.0  0.0   \n",
       "ill              0.004975  0.014122       0.0   [0.0]  0.0       0.0  0.0   \n",
       "sic_49           0.000419 -0.026146       0.0   [0.0]  0.0       0.0  0.0   \n",
       "age             -0.001054 -0.116730       0.0   [0.0]  0.0       0.0  0.0   \n",
       "pchsale_pchinvt -0.001179  0.023227       0.0  [-0.0]  0.0       0.0  0.0   \n",
       "\n",
       "                 NN2  NN3  NN4  NN5  \n",
       "pchgm_pchsale    0.0  0.0  0.0  0.0  \n",
       "sic_80           0.0  0.0  0.0  0.0  \n",
       "sic_55           0.0  0.0  0.0  0.0  \n",
       "currat           0.0  0.0  0.0  0.0  \n",
       "sgr              0.0  0.0  0.0  0.0  \n",
       "...              ...  ...  ...  ...  \n",
       "sic_81           0.0  0.0  0.0  0.0  \n",
       "ill              0.0  0.0  0.0  0.0  \n",
       "sic_49           0.0  0.0  0.0  0.0  \n",
       "age              0.0  0.0  0.0  0.0  \n",
       "pchsale_pchinvt  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[169 rows x 11 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417f788",
   "metadata": {},
   "source": [
    "## 1.2 Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c5f113",
   "metadata": {},
   "source": [
    "### 1.2.0 Additional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efb18ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huber loss function for customized objective function\n",
    "\n",
    "# gradient of huber loss with respect to y_pred\n",
    "def grad_huber_obj(y_true, y_pred):\n",
    "    xi = 1.35 \n",
    "    # Though I do not want to make it hard-coded, lightgbm, behind the scene, evaluates the # of parameters\n",
    "    # of the objective function first, then pass according # of parameters. I tried to use partial to set \n",
    "    # the value of xi. It did not work.\n",
    "    # I refer the readers to the source code to have a better understanding of the issue:\n",
    "    # (https://github.com/microsoft/LightGBM/blob/master/python-package/lightgbm/sklearn.py)\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    N = len(y_true)\n",
    "    resid = y_true - y_pred\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    grad = np.zeros(N)\n",
    "    try:\n",
    "        grad[ind_m] = (-2*(y_true-y_pred))[ind_m]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grad[ind_u] = np.repeat(2*xi,N)[ind_u]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grad[ind_l] = np.repeat(-2*xi,N)[ind_l]\n",
    "    except:\n",
    "        pass\n",
    "    return grad/N\n",
    "\n",
    "# hessian of huber loss with respect to y_pred\n",
    "def hess_huber_obj(y_true, y_pred):\n",
    "    xi = 1.35\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    N = len(y_true)\n",
    "    resid = y_true - y_pred\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    hess = np.zeros(N)\n",
    "    try:\n",
    "        hess[ind_m] = np.repeat(2,N)[ind_m]\n",
    "    except:\n",
    "        pass\n",
    "    return hess/N\n",
    "\n",
    "# huber loss for lgbm\n",
    "def huber_obj(y_true, y_pred):\n",
    "    grad = grad_huber_obj(y_true, y_pred)\n",
    "    hess = hess_huber_obj(y_true, y_pred)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301dfe01",
   "metadata": {},
   "source": [
    "### 1.2.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85ea33ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.45356%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.14061%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.77046%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 3.29298%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 3.21163%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.04809%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'max_depth': 6, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308}\n",
      "with R2oos 3.29% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3, 6],\n",
    "    'max_features': [30, 50, 100],\n",
    "    'random_state': [12308]\n",
    "}\n",
    "RF = val_fun(RandomForestRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c706d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 31.20%\n",
      "The in-sample MSE is 0.021\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 1.42%\n",
      "The out-of-sample MSE is 0.040\n",
      "CPU times: total: 1.08 s\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate(y_trn, RF.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, RF.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fd448",
   "metadata": {},
   "source": [
    "### 1.2.2 Gradient Boosting (XGBRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33014eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.11101%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.19727%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.75432%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.07785%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 2.10425%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.05221%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 3.52072%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.19727%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 5.15652%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.07785%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 6.08117%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.05221%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 4.19659%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.03936%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 5.38658%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.03936%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 6.08740%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.03936%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 18.92054%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.03936%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 8.50320%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.03936%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 4.04798%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.03936%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308}\n",
      "with R2oos 18.92% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective':[None, huber_obj],\n",
    "    'max_depth':[1,2],\n",
    "    'n_estimators':[500, 800, 1000],\n",
    "    'random_state':[12308],\n",
    "    'learning_rate':[.01,.1]\n",
    "}\n",
    "LGBM = val_fun(XGBRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81190723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************In-Sample Metrics***************\n",
      "The in-sample R2 is 50.66%\n",
      "The in-sample MSE is 0.015\n",
      "***************Out-of-Sample Metrics***************\n",
      "The out-of-sample R2 is 18.57%\n",
      "The out-of-sample MSE is 0.032\n",
      "CPU times: total: 1.89 s\n",
      "Wall time: 265 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(y_trn, LGBM.predict(X_trn), insample=True) \n",
    "evaluate(y_tst, LGBM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b48c057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[0,6] = r2_score(y_tst, RF.predict(X_tst))\n",
    "output.iloc[0,7] = r2_score(y_tst, LGBM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d83367",
   "metadata": {},
   "source": [
    "### Diebold-Mariano tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "443b4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "moriano.iloc[0,5] = dm_test(y_tst.values, OLS_H.predict(X_tst), RF.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[0,6] = dm_test(y_tst.values, OLS_H.predict(X_tst), LGBM.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[1,5] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), RF.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[1,6] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), LGBM.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[2,5] = dm_test(y_tst.values, PLS.predict(X_tst), RF.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[2,6] = dm_test(y_tst.values, PLS.predict(X_tst), LGBM.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[3,5] = dm_test(y_tst.values, PCR.predict(X_tst), RF.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[3,6] = dm_test(y_tst.values, PCR.predict(X_tst), LGBM.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[4,5] = dm_test(y_tst.values, EN_my_mse_rdm.predict(X_tst), RF.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[4,6] = dm_test(y_tst.values, EN_my_mse_rdm.predict(X_tst), LGBM.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[5,5] = dm_test(y_tst.values, GLM.predict(X_tst), RF.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[5,6] = dm_test(y_tst.values, GLM.predict(X_tst), LGBM.predict(X_tst), one_sided = False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9fd83a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OLS-3 + H</th>\n",
       "      <th>PLS</th>\n",
       "      <th>PCR</th>\n",
       "      <th>Enet + H</th>\n",
       "      <th>GLM + H</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBRT + H</th>\n",
       "      <th>NN1</th>\n",
       "      <th>NN2</th>\n",
       "      <th>NN3</th>\n",
       "      <th>NN4</th>\n",
       "      <th>NN5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OLS + H</th>\n",
       "      <td>44.667607</td>\n",
       "      <td>13.617104</td>\n",
       "      <td>50.423308</td>\n",
       "      <td>47.538610</td>\n",
       "      <td>47.408230</td>\n",
       "      <td>36.402181</td>\n",
       "      <td>21.641169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS-3 + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-37.000430</td>\n",
       "      <td>-6.857542</td>\n",
       "      <td>21.822947</td>\n",
       "      <td>24.353913</td>\n",
       "      <td>11.264323</td>\n",
       "      <td>13.505666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.498729</td>\n",
       "      <td>41.597149</td>\n",
       "      <td>41.618567</td>\n",
       "      <td>33.649792</td>\n",
       "      <td>20.598327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.385630</td>\n",
       "      <td>21.651692</td>\n",
       "      <td>12.197810</td>\n",
       "      <td>13.569594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enet + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.784107</td>\n",
       "      <td>4.617354</td>\n",
       "      <td>12.171884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLM + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.408721</td>\n",
       "      <td>12.032375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBRT + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           OLS-3 + H        PLS        PCR   Enet + H    GLM + H         RF  \\\n",
       "OLS + H    44.667607  13.617104  50.423308  47.538610  47.408230  36.402181   \n",
       "OLS-3 + H   0.000000 -37.000430  -6.857542  21.822947  24.353913  11.264323   \n",
       "PLS         0.000000   0.000000  42.498729  41.597149  41.618567  33.649792   \n",
       "PCR         0.000000   0.000000   0.000000  20.385630  21.651692  12.197810   \n",
       "Enet + H    0.000000   0.000000   0.000000   0.000000  17.784107   4.617354   \n",
       "GLM + H     0.000000   0.000000   0.000000   0.000000   0.000000   3.408721   \n",
       "RF          0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "GBRT + H    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN1         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN2         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN3         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN4         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "            GBRT + H  NN1  NN2  NN3  NN4  NN5  \n",
       "OLS + H    21.641169  0.0  0.0  0.0  0.0  0.0  \n",
       "OLS-3 + H  13.505666  0.0  0.0  0.0  0.0  0.0  \n",
       "PLS        20.598327  0.0  0.0  0.0  0.0  0.0  \n",
       "PCR        13.569594  0.0  0.0  0.0  0.0  0.0  \n",
       "Enet + H   12.171884  0.0  0.0  0.0  0.0  0.0  \n",
       "GLM + H    12.032375  0.0  0.0  0.0  0.0  0.0  \n",
       "RF          0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "GBRT + H    0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "NN1         0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "NN2         0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "NN3         0.000000  0.0  0.0  0.0  0.0  0.0  \n",
       "NN4         0.000000  0.0  0.0  0.0  0.0  0.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moriano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d59c5",
   "metadata": {},
   "source": [
    "### Coefficient Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "273db626",
   "metadata": {},
   "outputs": [],
   "source": [
    "significo['RF'] = RF.feature_importances_\n",
    "significo['GBRT + H'] = LGBM.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f75de6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLS</th>\n",
       "      <th>PCR</th>\n",
       "      <th>Enet + H</th>\n",
       "      <th>GLM + H</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBRT + H</th>\n",
       "      <th>NN1</th>\n",
       "      <th>NN2</th>\n",
       "      <th>NN3</th>\n",
       "      <th>NN4</th>\n",
       "      <th>NN5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pchgm_pchsale</th>\n",
       "      <td>-0.002017</td>\n",
       "      <td>-0.058591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_80</th>\n",
       "      <td>0.000681</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_55</th>\n",
       "      <td>0.000914</td>\n",
       "      <td>-0.010684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currat</th>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.036253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.011125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgr</th>\n",
       "      <td>-0.000459</td>\n",
       "      <td>0.052028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.0]</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_81</th>\n",
       "      <td>0.000663</td>\n",
       "      <td>-0.002312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.0]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ill</th>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.014122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>0.008632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_49</th>\n",
       "      <td>0.000419</td>\n",
       "      <td>-0.026146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.001054</td>\n",
       "      <td>-0.116730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pchsale_pchinvt</th>\n",
       "      <td>-0.001179</td>\n",
       "      <td>0.023227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.0]</td>\n",
       "      <td>0.026324</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      PLS       PCR  Enet + H GLM + H        RF  GBRT + H  \\\n",
       "pchgm_pchsale   -0.002017 -0.058591       0.0   [0.0]  0.000892  0.000000   \n",
       "sic_80           0.000681 -0.000182       0.0   [0.0]  0.000063  0.000000   \n",
       "sic_55           0.000914 -0.010684       0.0   [0.0]  0.000000  0.000000   \n",
       "currat           0.000368  0.036253       0.0   [0.0]  0.003436  0.011125   \n",
       "sgr             -0.000459  0.052028       0.0  [-0.0]  0.003557  0.001681   \n",
       "...                   ...       ...       ...     ...       ...       ...   \n",
       "sic_81           0.000663 -0.002312       0.0  [-0.0]  0.000000  0.000000   \n",
       "ill              0.004975  0.014122       0.0   [0.0]  0.007285  0.008632   \n",
       "sic_49           0.000419 -0.026146       0.0   [0.0]  0.000023  0.000000   \n",
       "age             -0.001054 -0.116730       0.0   [0.0]  0.000656  0.000000   \n",
       "pchsale_pchinvt -0.001179  0.023227       0.0  [-0.0]  0.026324  0.008610   \n",
       "\n",
       "                 NN1  NN2  NN3  NN4  NN5  \n",
       "pchgm_pchsale    0.0  0.0  0.0  0.0  0.0  \n",
       "sic_80           0.0  0.0  0.0  0.0  0.0  \n",
       "sic_55           0.0  0.0  0.0  0.0  0.0  \n",
       "currat           0.0  0.0  0.0  0.0  0.0  \n",
       "sgr              0.0  0.0  0.0  0.0  0.0  \n",
       "...              ...  ...  ...  ...  ...  \n",
       "sic_81           0.0  0.0  0.0  0.0  0.0  \n",
       "ill              0.0  0.0  0.0  0.0  0.0  \n",
       "sic_49           0.0  0.0  0.0  0.0  0.0  \n",
       "age              0.0  0.0  0.0  0.0  0.0  \n",
       "pchsale_pchinvt  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[169 rows x 11 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7ed4ab95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baspread      0.009017\n",
       "sic_73        0.007239\n",
       "ill           0.004975\n",
       "maxret        0.004049\n",
       "sic_38        0.003435\n",
       "                ...   \n",
       "chpmia       -0.005962\n",
       "std_turn     -0.006613\n",
       "std_dolvol   -0.007444\n",
       "mom1m        -0.012592\n",
       "indmom       -0.018105\n",
       "Name: PLS, Length: 169, dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significo['PLS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbed12f",
   "metadata": {},
   "source": [
    "## 1.3 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56a86d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized metrics\n",
    "# out-of-sample r squared for keras\n",
    "def R_oos_tf(y_true, y_pred):\n",
    "    resid = tf.square(y_true-y_pred)\n",
    "    denom = tf.square(y_true)\n",
    "    return 1 - tf.divide(tf.reduce_mean(resid),tf.reduce_mean(denom))\n",
    "\n",
    "# data standardization\n",
    "# please standardize the data if BatchNormalization is not used\n",
    "def standardize(X_trn, X_vld, X_tst):\n",
    "    mu_trn = np.mean(np.array(X_trn),axis=0).reshape((1,X_trn.shape[1]))\n",
    "    sigma_trn = np.std(np.array(X_trn),axis=0).reshape((1,X_trn.shape[1]))\n",
    "\n",
    "    X_trn_std = (np.array(X_trn)-mu_trn)/sigma_trn\n",
    "    X_vld_std = (np.array(X_vld)-mu_trn)/sigma_trn\n",
    "    X_tst_std = (np.array(X_tst)-mu_trn)/sigma_trn\n",
    "    return X_trn_std, X_vld_std, X_tst_std\n",
    "\n",
    "# NN class\n",
    "class NN:\n",
    "    \n",
    "    def __init__(\n",
    "        self, n_layers=1, loss='mse', l1=1e-5, l2=0, learning_rate=.01, BatchNormalization=True, patience=5,\n",
    "        epochs=100, batch_size=3000, verbose=1, random_state=12308, monitor='val_R_oos_tf', base_neurons=5\n",
    "    ):\n",
    "        self.n_layers = n_layers\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.learning_rate = learning_rate\n",
    "        self.BatchNormalization = BatchNormalization\n",
    "        self.patience = patience\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.monitor = monitor\n",
    "        self.base_neurons = base_neurons\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X_trn, y_trn, X_vld, y_vld):\n",
    "        # fix random seed for reproducibility\n",
    "        random.seed(self.random_state)\n",
    "        np.random.seed(self.random_state)\n",
    "        tf.random.set_seed(self.random_state)\n",
    "        \n",
    "        # model construction\n",
    "        mod = Sequential()\n",
    "        mod.add(Input(shape=(X_trn.shape[1],)))\n",
    "        \n",
    "        for i in np.arange(self.n_layers,0,-1):\n",
    "            if self.n_layers>self.base_neurons:\n",
    "                if self.n_layers == i:\n",
    "                    mod.add(Dense(2**i, activation='relu'))\n",
    "                else:\n",
    "                    mod.add(Dense(2**i, activation='relu', kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "            else:\n",
    "                if self.n_layers == i:\n",
    "                    mod.add(Dense(2**(self.base_neurons-(self.n_layers-i)), activation='relu'))\n",
    "                else:\n",
    "                    mod.add(Dense(2**(self.base_neurons-(self.n_layers-i)), \n",
    "                                  activation='relu', kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "            if self.BatchNormalization:\n",
    "                mod.add(BatchNormalization())\n",
    "        \n",
    "        mod.add(Dense(1, kernel_regularizer=L1L2(self.l1,self.l2)))\n",
    "        \n",
    "        # early stopping\n",
    "        earlystop = tf.keras.callbacks.EarlyStopping(monitor=self.monitor, patience=self.patience, mode = 'max')\n",
    "\n",
    "        # Adam solver\n",
    "        opt = Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        # compile the model\n",
    "        mod.compile(loss=self.loss,\n",
    "                    optimizer=opt,\n",
    "                    metrics=[R_oos_tf])\n",
    "\n",
    "        # fit the model\n",
    "        mod.fit(X_trn, np.array(y_trn).reshape((len(y_trn),1)), epochs=self.epochs, batch_size=self.batch_size, \n",
    "                callbacks=[earlystop], verbose=self.verbose, \n",
    "                validation_data=(X_vld,np.array(y_vld).reshape((len(y_vld),1))))\n",
    "        \n",
    "        self.model = mod\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=self.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41711c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -119.88697%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.26530%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 2.42762%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.27474%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -125.87268%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.83116%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 2.49137%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.95371%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 2.49% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 2min 33s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN1-Regression-[32(relu)-1(linear)]\n",
    "\n",
    "params = {\n",
    "    'n_layers': [1],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf'],\n",
    "}\n",
    "NN1 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d362e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -184.38131%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.00880%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -73.63059%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -12.02550%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -82.36432%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 2.44245%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -13.87971%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.06153%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 2, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 2.44% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 2min 57s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [2],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN2 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "333b8520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -9.27175%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 2.80203%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.41440%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -72.56592%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -7.16014%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -2.53306%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.04141%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.08382%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 3, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 2.80% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 3min 19s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [3],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN3 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05ebe489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -12.60764%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -8.48823%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.25468%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -10.69177%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -8.98328%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.52169%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.62177%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.09278%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 4, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 1.52% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 3min 52s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [4],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN4 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e9184fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.08453%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.48976%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.02585%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.09425%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.38232%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -0.76868%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 0.19619%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 2.21515%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 1366, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 5, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 2.22% on validation set.\n",
      "############################################################\n",
      "CPU times: total: 4min 13s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [5],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN5 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76f20bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['NN1'] = np.array([0,0,0])\n",
    "output['NN2'] = np.array([0,0,0])\n",
    "output['NN3'] = np.array([0,0,0])\n",
    "output['NN4'] = np.array([0,0,0])\n",
    "output['NN5'] = np.array([0,0,0])\n",
    "output.iloc[0,8] = r2_score(y_tst, NN1.predict(X_tst))\n",
    "output.iloc[0,9] = r2_score(y_tst, NN2.predict(X_tst))\n",
    "output.iloc[0,10] = r2_score(y_tst, NN3.predict(X_tst))\n",
    "output.iloc[0,11] = r2_score(y_tst, NN4.predict(X_tst))\n",
    "output.iloc[0,12] = r2_score(y_tst, NN5.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2cd0a",
   "metadata": {},
   "source": [
    "### Diebold-Mariano tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0551f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "moriano.iloc[0,7] = dm_test(y_tst.values, OLS_H.predict(X_tst), NN1.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[0,8] = dm_test(y_tst.values, OLS_H.predict(X_tst), NN2.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[0,9] = dm_test(y_tst.values, OLS_H.predict(X_tst), NN3.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[0,10] = dm_test(y_tst.values, OLS_H.predict(X_tst), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[0,11] = dm_test(y_tst.values, OLS_H.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[1,7] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), NN1.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[1,8] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), NN2.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[1,9] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), NN3.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[1,10] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[1,11] = dm_test(y_tst.values, OLS_3.predict(X_tst[features_3]), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[2,7] = dm_test(y_tst.values, PLS.predict(X_tst), NN1.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[2,8] = dm_test(y_tst.values, PLS.predict(X_tst), NN2.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[2,9] = dm_test(y_tst.values, PLS.predict(X_tst), NN3.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[2,10] = dm_test(y_tst.values, PLS.predict(X_tst), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[2,11] = dm_test(y_tst.values, PLS.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[3,7] = dm_test(y_tst.values, PCR.predict(X_tst), NN1.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[3,8] = dm_test(y_tst.values, PCR.predict(X_tst), NN2.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[3,9] = dm_test(y_tst.values, PCR.predict(X_tst), NN3.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[3,10] = dm_test(y_tst.values, PCR.predict(X_tst), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[3,11] = dm_test(y_tst.values, PCR.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[4,7] = dm_test(y_tst.values, EN_my_mse_rdm.predict(X_tst), NN1.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[4,8] = dm_test(y_tst.values, EN_my_mse_rdm.predict(X_tst), NN2.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[4,9] = dm_test(y_tst.values, EN_my_mse_rdm.predict(X_tst), NN3.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[4,10] = dm_test(y_tst.values, EN_my_mse_rdm.predict(X_tst), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[4,11] = dm_test(y_tst.values, EN_my_mse_rdm.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[5,7] = dm_test(y_tst.values, GLM.predict(X_tst), NN1.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[5,8] = dm_test(y_tst.values, GLM.predict(X_tst), NN2.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[5,9] = dm_test(y_tst.values, GLM.predict(X_tst), NN3.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[5,10] = dm_test(y_tst.values, GLM.predict(X_tst), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[5,11] = dm_test(y_tst.values, GLM.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[6,7] = dm_test(y_tst.values, RF.predict(X_tst), NN1.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[6,8] = dm_test(y_tst.values, RF.predict(X_tst), NN2.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[6,9] = dm_test(y_tst.values, RF.predict(X_tst), NN3.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[6,10] = dm_test(y_tst.values, RF.predict(X_tst), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[6,11] = dm_test(y_tst.values, RF.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[7,7] = dm_test(y_tst.values, LGBM.predict(X_tst), NN1.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[7,8] = dm_test(y_tst.values, LGBM.predict(X_tst), NN2.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[7,9] = dm_test(y_tst.values, LGBM.predict(X_tst), NN3.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[7,10] = dm_test(y_tst.values, LGBM.predict(X_tst), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[7,11] = dm_test(y_tst.values, LGBM.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "\n",
    "moriano.iloc[8,8] = dm_test(y_tst.values, NN1.predict(X_tst), NN2.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[8,9] = dm_test(y_tst.values, NN1.predict(X_tst), NN3.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[8,10] = dm_test(y_tst.values, NN1.predict(X_tst), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[8,11] = dm_test(y_tst.values, NN1.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[9,9] = dm_test(y_tst.values, NN2.predict(X_tst), NN3.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[9,10] = dm_test(y_tst.values, NN2.predict(X_tst), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[9,11] = dm_test(y_tst.values, NN2.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[10,10] = dm_test(y_tst.values, NN3.predict(X_tst), NN4.predict(X_tst), one_sided = False)[0]\n",
    "moriano.iloc[10,11] = dm_test(y_tst.values, NN3.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]\n",
    "\n",
    "moriano.iloc[11,11] = dm_test(y_tst.values, NN4.predict(X_tst), NN5.predict(X_tst), one_sided = False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9809c9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OLS-3 + H</th>\n",
       "      <th>PLS</th>\n",
       "      <th>PCR</th>\n",
       "      <th>Enet + H</th>\n",
       "      <th>GLM + H</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBRT + H</th>\n",
       "      <th>NN1</th>\n",
       "      <th>NN2</th>\n",
       "      <th>NN3</th>\n",
       "      <th>NN4</th>\n",
       "      <th>NN5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OLS + H</th>\n",
       "      <td>44.667607</td>\n",
       "      <td>13.617104</td>\n",
       "      <td>50.423308</td>\n",
       "      <td>47.538610</td>\n",
       "      <td>47.408230</td>\n",
       "      <td>36.402181</td>\n",
       "      <td>21.641169</td>\n",
       "      <td>-11.697275</td>\n",
       "      <td>-49.387508</td>\n",
       "      <td>-0.363544</td>\n",
       "      <td>46.780575</td>\n",
       "      <td>49.806442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS-3 + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-37.000430</td>\n",
       "      <td>-6.857542</td>\n",
       "      <td>21.822947</td>\n",
       "      <td>24.353913</td>\n",
       "      <td>11.264323</td>\n",
       "      <td>13.505666</td>\n",
       "      <td>-56.849159</td>\n",
       "      <td>-80.963097</td>\n",
       "      <td>-31.158699</td>\n",
       "      <td>2.877297</td>\n",
       "      <td>-15.552996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.498729</td>\n",
       "      <td>41.597149</td>\n",
       "      <td>41.618567</td>\n",
       "      <td>33.649792</td>\n",
       "      <td>20.598327</td>\n",
       "      <td>-28.597974</td>\n",
       "      <td>-45.142408</td>\n",
       "      <td>-9.796310</td>\n",
       "      <td>39.354866</td>\n",
       "      <td>38.316115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.385630</td>\n",
       "      <td>21.651692</td>\n",
       "      <td>12.197810</td>\n",
       "      <td>13.569594</td>\n",
       "      <td>-66.263751</td>\n",
       "      <td>-86.115869</td>\n",
       "      <td>-34.302898</td>\n",
       "      <td>7.311327</td>\n",
       "      <td>-10.938444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enet + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.784107</td>\n",
       "      <td>4.617354</td>\n",
       "      <td>12.171884</td>\n",
       "      <td>-60.745200</td>\n",
       "      <td>-85.063556</td>\n",
       "      <td>-34.177638</td>\n",
       "      <td>-15.006216</td>\n",
       "      <td>-33.213926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLM + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.408721</td>\n",
       "      <td>12.032375</td>\n",
       "      <td>-60.085903</td>\n",
       "      <td>-84.415741</td>\n",
       "      <td>-34.179653</td>\n",
       "      <td>-17.615084</td>\n",
       "      <td>-33.122356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-44.734188</td>\n",
       "      <td>-65.644951</td>\n",
       "      <td>-29.271791</td>\n",
       "      <td>-10.572270</td>\n",
       "      <td>-16.893509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBRT + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-23.371767</td>\n",
       "      <td>-30.534307</td>\n",
       "      <td>-20.389906</td>\n",
       "      <td>-13.148064</td>\n",
       "      <td>-14.411870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-47.033726</td>\n",
       "      <td>4.256386</td>\n",
       "      <td>59.759953</td>\n",
       "      <td>68.758141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.243170</td>\n",
       "      <td>85.851082</td>\n",
       "      <td>97.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.052517</td>\n",
       "      <td>31.102022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.031160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           OLS-3 + H        PLS        PCR   Enet + H    GLM + H         RF  \\\n",
       "OLS + H    44.667607  13.617104  50.423308  47.538610  47.408230  36.402181   \n",
       "OLS-3 + H   0.000000 -37.000430  -6.857542  21.822947  24.353913  11.264323   \n",
       "PLS         0.000000   0.000000  42.498729  41.597149  41.618567  33.649792   \n",
       "PCR         0.000000   0.000000   0.000000  20.385630  21.651692  12.197810   \n",
       "Enet + H    0.000000   0.000000   0.000000   0.000000  17.784107   4.617354   \n",
       "GLM + H     0.000000   0.000000   0.000000   0.000000   0.000000   3.408721   \n",
       "RF          0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "GBRT + H    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN1         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN2         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN3         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN4         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "            GBRT + H        NN1        NN2        NN3        NN4        NN5  \n",
       "OLS + H    21.641169 -11.697275 -49.387508  -0.363544  46.780575  49.806442  \n",
       "OLS-3 + H  13.505666 -56.849159 -80.963097 -31.158699   2.877297 -15.552996  \n",
       "PLS        20.598327 -28.597974 -45.142408  -9.796310  39.354866  38.316115  \n",
       "PCR        13.569594 -66.263751 -86.115869 -34.302898   7.311327 -10.938444  \n",
       "Enet + H   12.171884 -60.745200 -85.063556 -34.177638 -15.006216 -33.213926  \n",
       "GLM + H    12.032375 -60.085903 -84.415741 -34.179653 -17.615084 -33.122356  \n",
       "RF          0.000000 -44.734188 -65.644951 -29.271791 -10.572270 -16.893509  \n",
       "GBRT + H    0.000000 -23.371767 -30.534307 -20.389906 -13.148064 -14.411870  \n",
       "NN1         0.000000   0.000000 -47.033726   4.256386  59.759953  68.758141  \n",
       "NN2         0.000000   0.000000   0.000000  27.243170  85.851082  97.941765  \n",
       "NN3         0.000000   0.000000   0.000000   0.000000  32.052517  31.102022  \n",
       "NN4         0.000000   0.000000   0.000000   0.000000   0.000000 -17.031160  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moriano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511b89a",
   "metadata": {},
   "source": [
    "## 1.3 Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d3c079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_top = b1_d.sort_values('mvel1',ascending=False).groupby('date').head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01f9800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_co = b1_top.index\n",
    "features = list(set(b1_top.columns).difference({'permno','DATE','RET'}))\n",
    "X = MinMaxScaler((-1,1)).fit_transform(b1_top[features])\n",
    "X = pd.DataFrame(X, columns=features, index=index_co)\n",
    "y = b1_top['RET']\n",
    "\n",
    "stdt_vld = starting + (ending - starting)/3\n",
    "stdt_tst = starting + 2*(ending - starting)/3\n",
    "\n",
    "X_trn = X[X.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "y_trn = y[y.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "\n",
    "X_vld = X[(X.index.get_level_values(level = \"date\") >= stdt_vld) & (X.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "y_vld = y[(y.index.get_level_values(level = \"date\") >= stdt_vld) & (y.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "\n",
    "X_tst = X[X.index.get_level_values(level = \"date\") >= stdt_tst]\n",
    "y_tst = y[y.index.get_level_values(level = \"date\") >= stdt_tst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c698b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'n_components': 1} finished.\n",
      "with out-of-sample R squared on validation set: 6.91720%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 5} finished.\n",
      "with out-of-sample R squared on validation set: 4.52028%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 10} finished.\n",
      "with out-of-sample R squared on validation set: 2.86558%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 50} finished.\n",
      "with out-of-sample R squared on validation set: 3.51104%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'n_components': 1}\n",
      "with R2oos 6.92% on validation set.\n",
      "############################################################\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: 1.30246%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: 1.73092%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: 1.84109%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: 2.03683%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: 2.99459%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 5.57466%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.59683%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: 0.92337%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: 1.30874%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: 1.47682%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: 2.61009%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 5.74889%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'loss': 'huber', 'n_PCs': 50}\n",
      "with R2oos 5.75% on validation set.\n",
      "############################################################\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.008622\n",
      "         Iterations: 5510\n",
      "         Function evaluations: 6312\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: 0.40302%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: 0.55507%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'knots': 3, 'l1_reg': 0, 'lmd': 0.0001}\n",
      "with R2oos 0.56% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "epsilon = np.max(((y_trn-OLS.predict(X_trn)).quantile(.999),1))\n",
    "OLS_H = HuberRegressor(epsilon=epsilon).fit(X_trn,y_trn)\n",
    "\n",
    "params = {'n_components': [1, 5, 10, 50]}\n",
    "PLS = val_fun(PLSRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)\n",
    "\n",
    "params = {'n_PCs':[1,3,5,7,10,50],'loss':['mse','huber']}\n",
    "PCR = val_fun(PCRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,sleep=3)\n",
    "\n",
    "EN_my_mse_rdm = ENet(lmd=.01,rho=.5,loss='huber').fit(X_trn,y_trn)\n",
    "\n",
    "params = {\n",
    "    'knots':[3],\n",
    "    'lmd':[1e-4,1e-1],#list(np.linspace(1e-4,1e-1,10)),\n",
    "    'l1_reg':[1e-4,0]\n",
    "}\n",
    "GLM = val_fun(GLMRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)\n",
    "\n",
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']\n",
    "OLS_3 = LinearRegression().fit(X_trn[features_3],y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5154272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[1,0] =r2_score(y_tst, OLS_H.predict(X_tst))\n",
    "output.iloc[1,2] = r2_score(y_tst, PCR.predict(X_tst))\n",
    "output.iloc[1,3] = r2_score(y_tst, PLS.predict(X_tst))\n",
    "output.iloc[1,4] = r2_score(y_tst, EN_my_mse_rdm.predict(X_tst))\n",
    "output.iloc[1,5] = r2_score(y_tst, GLM.predict(X_tst))\n",
    "\n",
    "output.iloc[1,1] = r2_score(y_tst, OLS_3.predict(X_tst[features_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07bfa454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.73180%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.70376%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -5.41997%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.69172%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.76169%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -5.41974%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308}\n",
      "with R2oos 0.73% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3, 6],\n",
    "    'max_features': [30, 50, 100],\n",
    "    'random_state': [12308]\n",
    "}\n",
    "RF = val_fun(RandomForestRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26fdb53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -4.72057%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -2.72356%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -3.75069%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.06010%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -3.37262%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.05057%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 1.39410%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -2.72356%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 3.82511%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.06010%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 5.22406%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.05057%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -1.61134%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.08030%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -1.20089%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.08030%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: -0.53776%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.08030%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 9.62570%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.08030%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 13.91972%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.08030%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 16.41981%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.08030%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308}\n",
      "with R2oos 16.42% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective':[None, huber_obj],\n",
    "    'max_depth':[1,2],\n",
    "    'n_estimators':[500, 800, 1000],\n",
    "    'random_state':[12308],\n",
    "    'learning_rate':[.01,.1]\n",
    "}\n",
    "LGBM = val_fun(XGBRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3662242",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[1,6] = r2_score(y_tst, RF.predict(X_tst))\n",
    "output.iloc[1,7] = r2_score(y_tst, LGBM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13ff717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -294.29593%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -30.53179%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 1.87961%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -8.96582%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -270.40952%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -3.28417%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 4.67746%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -32.19573%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 4.68% on validation set.\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN1-Regression-[32(relu)-1(linear)]\n",
    "\n",
    "params = {\n",
    "    'n_layers': [1],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf'],\n",
    "}\n",
    "NN1 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [2],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN2 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [3],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN3 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [4],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN4 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [5],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN5 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d9cb7043",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[1,8] = r2_score(y_tst, NN1.predict(X_tst))\n",
    "output.iloc[1,9] = r2_score(y_tst, NN2.predict(X_tst))\n",
    "output.iloc[1,10] = r2_score(y_tst, NN3.predict(X_tst))\n",
    "output.iloc[1,11] = r2_score(y_tst, NN4.predict(X_tst))\n",
    "output.iloc[1,12] = r2_score(y_tst, NN5.predict(X_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddb678",
   "metadata": {},
   "source": [
    "## 1.5 Bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ea07525",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_bot = b1_d.sort_values('mvel1',ascending=False).groupby('date').tail(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64cc7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_co = b1_bot.index\n",
    "features = list(set(b1_bot.columns).difference({'permno','DATE','RET'}))\n",
    "X = MinMaxScaler((-1,1)).fit_transform(b1_bot[features])\n",
    "X = pd.DataFrame(X, columns=features, index=index_co)\n",
    "y = b1_bot['RET']\n",
    "\n",
    "stdt_vld = starting + (ending - starting)/3\n",
    "stdt_tst = starting + 2*(ending - starting)/3\n",
    "\n",
    "X_trn = X[X.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "y_trn = y[y.index.get_level_values(level = \"date\") <= stdt_vld]\n",
    "\n",
    "X_vld = X[(X.index.get_level_values(level = \"date\") >= stdt_vld) & (X.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "y_vld = y[(y.index.get_level_values(level = \"date\") >= stdt_vld) & (y.index.get_level_values(level = \"date\") <= stdt_tst)]\n",
    "\n",
    "X_tst = X[X.index.get_level_values(level = \"date\") >= stdt_tst]\n",
    "y_tst = y[y.index.get_level_values(level = \"date\") >= stdt_tst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66690ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'n_components': 1} finished.\n",
      "with out-of-sample R squared on validation set: 1.18813%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 5} finished.\n",
      "with out-of-sample R squared on validation set: 3.50014%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 10} finished.\n",
      "with out-of-sample R squared on validation set: 3.54722%\n",
      "************************************************************\n",
      "Model with params: {'n_components': 50} finished.\n",
      "with out-of-sample R squared on validation set: 3.53315%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'n_components': 10}\n",
      "with R2oos 3.55% on validation set.\n",
      "############################################################\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00354%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: -0.00140%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: 0.02130%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: 0.07351%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: 0.12974%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'mse', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 1.37230%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 1} finished.\n",
      "with out-of-sample R squared on validation set: -0.00078%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 3} finished.\n",
      "with out-of-sample R squared on validation set: -0.00069%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 5} finished.\n",
      "with out-of-sample R squared on validation set: -0.00077%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 7} finished.\n",
      "with out-of-sample R squared on validation set: -0.00243%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 10} finished.\n",
      "with out-of-sample R squared on validation set: -0.00236%\n",
      "************************************************************\n",
      "Model with params: {'loss': 'huber', 'n_PCs': 50} finished.\n",
      "with out-of-sample R squared on validation set: 0.05338%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'loss': 'mse', 'n_PCs': 50}\n",
      "with R2oos 1.37% on validation set.\n",
      "############################################################\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.053711\n",
      "         Iterations: 4493\n",
      "         Function evaluations: 5817\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: 0.57002%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0.0001, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.0001} finished.\n",
      "with out-of-sample R squared on validation set: 0.63523%\n",
      "************************************************************\n",
      "Model with params: {'knots': 3, 'l1_reg': 0, 'lmd': 0.1} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'knots': 3, 'l1_reg': 0, 'lmd': 0.0001}\n",
      "with R2oos 0.64% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "epsilon = np.max(((y_trn-OLS.predict(X_trn)).quantile(.999),1))\n",
    "OLS_H = HuberRegressor(epsilon=epsilon).fit(X_trn,y_trn)\n",
    "\n",
    "params = {'n_components': [1, 5, 10, 50]}\n",
    "PLS = val_fun(PLSRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)\n",
    "\n",
    "params = {'n_PCs':[1,3,5,7,10,50],'loss':['mse','huber']}\n",
    "PCR = val_fun(PCRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,sleep=3)\n",
    "\n",
    "EN_my_mse_rdm = ENet(lmd=.01,rho=.5,loss='huber').fit(X_trn,y_trn)\n",
    "\n",
    "params = {\n",
    "    'knots':[3],\n",
    "    'lmd':[1e-4,1e-1],#list(np.linspace(1e-4,1e-1,10)),\n",
    "    'l1_reg':[1e-4,0]\n",
    "}\n",
    "GLM = val_fun(GLMRegression,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)\n",
    "\n",
    "features_3 = ['mvel1','bm','mom1m','mom6m','mom12m','mom36m']\n",
    "OLS_3 = LinearRegression().fit(X_trn[features_3],y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9432d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[2,0] =r2_score(y_tst, OLS_H.predict(X_tst))\n",
    "output.iloc[2,2] = r2_score(y_tst, PCR.predict(X_tst))\n",
    "output.iloc[2,3] = r2_score(y_tst, PLS.predict(X_tst))\n",
    "output.iloc[2,4] = r2_score(y_tst, EN_my_mse_rdm.predict(X_tst))\n",
    "output.iloc[2,5] = r2_score(y_tst, GLM.predict(X_tst))\n",
    "\n",
    "output.iloc[2,1] = r2_score(y_tst, OLS_3.predict(X_tst[features_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "269e8cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'max_depth': 3, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 8.14864%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 9.92383%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 3, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 13.10519%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 30, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 13.02828%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 50, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 15.79485%\n",
      "************************************************************\n",
      "Model with params: {'max_depth': 6, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 18.06984%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'max_depth': 6, 'max_features': 100, 'n_estimators': 300, 'random_state': 12308}\n",
      "with R2oos 18.07% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3, 6],\n",
    "    'max_features': [30, 50, 100],\n",
    "    'random_state': [12308]\n",
    "}\n",
    "RF = val_fun(RandomForestRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2d05e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 7.01012%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.20518%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 8.14844%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 8.71075%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 17.07783%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.20518%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 19.39189%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 20.76088%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 15.12166%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 17.26157%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 18.13021%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 29.05433%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 24.04753%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 800, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': None, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 18.30064%\n",
      "************************************************************\n",
      "Model with params: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000, 'objective': <function huber_obj at 0x0000025EFD008430>, 'random_state': 12308} finished.\n",
      "with out-of-sample R squared on validation set: 0.00000%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 500, 'objective': None, 'random_state': 12308}\n",
      "with R2oos 29.05% on validation set.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective':[None, huber_obj],\n",
    "    'max_depth':[1,2],\n",
    "    'n_estimators':[500, 800, 1000],\n",
    "    'random_state':[12308],\n",
    "    'learning_rate':[.01,.1]\n",
    "}\n",
    "LGBM = val_fun(XGBRegressor,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "340ac82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[2,6] = r2_score(y_tst, RF.predict(X_tst))\n",
    "output.iloc[2,7] = r2_score(y_tst, LGBM.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "201073a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 2.65815%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 3.99095%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 2.56912%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 1e-05, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -1.10149%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: -3.00115%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 4.03830%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_loss', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 2.70255%\n",
      "************************************************************\n",
      "Model with params: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.01, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0} finished.\n",
      "with out-of-sample R squared on validation set: 3.70471%\n",
      "************************************************************\n",
      "\n",
      "############################################################\n",
      "Tuning process finished!!!\n",
      "The best setting is: {'BatchNormalization': True, 'batch_size': 240, 'epochs': 100, 'l1': 0.001, 'learning_rate': 0.001, 'loss': 'mse', 'monitor': 'val_R_oos_tf', 'n_layers': 1, 'patience': 5, 'random_state': 12308, 'verbose': 0}\n",
      "with R2oos 4.04% on validation set.\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NN1-Regression-[32(relu)-1(linear)]\n",
    "\n",
    "params = {\n",
    "    'n_layers': [1],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf'],\n",
    "}\n",
    "NN1 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [2],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN2 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [3],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN3 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [4],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN4 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)\n",
    "\n",
    "%%time\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "params = {\n",
    "    'n_layers': [5],\n",
    "    'loss': ['mse'],\n",
    "    'l1': [1e-5,1e-3],\n",
    "    'learning_rate': [.001,.01],\n",
    "    'batch_size': [int(X_trn.shape[0]/50)],\n",
    "    'epochs': [100],\n",
    "    'random_state': [12308],\n",
    "    'BatchNormalization': [True],\n",
    "    'patience':[5],\n",
    "    'verbose': [0],\n",
    "    'monitor':['val_loss','val_R_oos_tf']\n",
    "}\n",
    "NN5 = val_fun(NN,params=params,X_trn=X_trn,y_trn=y_trn,X_vld=X_vld,y_vld=y_vld,is_NN=True,sleep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1973656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[2,8] = r2_score(y_tst, NN1.predict(X_tst))\n",
    "output.iloc[2,9] = r2_score(y_tst, NN2.predict(X_tst))\n",
    "output.iloc[2,10] = r2_score(y_tst, NN3.predict(X_tst))\n",
    "output.iloc[2,11] = r2_score(y_tst, NN4.predict(X_tst))\n",
    "output.iloc[2,12] = r2_score(y_tst, NN5.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bb45c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_excel(\"reality_here.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "684ca3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OLS + H</th>\n",
       "      <th>OLS-3 + H</th>\n",
       "      <th>PCR</th>\n",
       "      <th>PLS</th>\n",
       "      <th>Enet + H</th>\n",
       "      <th>GLM + H</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBRT + H</th>\n",
       "      <th>NN1</th>\n",
       "      <th>NN2</th>\n",
       "      <th>NN3</th>\n",
       "      <th>NN4</th>\n",
       "      <th>NN5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>-0.258922</td>\n",
       "      <td>-0.042222</td>\n",
       "      <td>-0.051462</td>\n",
       "      <td>-0.223856</td>\n",
       "      <td>-0.013536</td>\n",
       "      <td>-0.009523</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.201412</td>\n",
       "      <td>-0.278784</td>\n",
       "      <td>-0.420429</td>\n",
       "      <td>-0.260544</td>\n",
       "      <td>-0.036583</td>\n",
       "      <td>-0.068679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top 1000</th>\n",
       "      <td>-0.518628</td>\n",
       "      <td>-0.118402</td>\n",
       "      <td>-0.099896</td>\n",
       "      <td>-0.083866</td>\n",
       "      <td>-0.034174</td>\n",
       "      <td>-0.028305</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.136232</td>\n",
       "      <td>-1.449462</td>\n",
       "      <td>-1.488644</td>\n",
       "      <td>-2.641036</td>\n",
       "      <td>-0.080898</td>\n",
       "      <td>-0.245133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bottom 1000</th>\n",
       "      <td>-0.215506</td>\n",
       "      <td>-0.034741</td>\n",
       "      <td>-0.046066</td>\n",
       "      <td>-0.145577</td>\n",
       "      <td>-0.005900</td>\n",
       "      <td>-0.005751</td>\n",
       "      <td>0.122885</td>\n",
       "      <td>0.184621</td>\n",
       "      <td>-0.190637</td>\n",
       "      <td>-0.226046</td>\n",
       "      <td>-0.150685</td>\n",
       "      <td>-0.016387</td>\n",
       "      <td>-0.037959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              OLS + H  OLS-3 + H       PCR       PLS  Enet + H   GLM + H  \\\n",
       "All         -0.258922  -0.042222 -0.051462 -0.223856 -0.013536 -0.009523   \n",
       "Top 1000    -0.518628  -0.118402 -0.099896 -0.083866 -0.034174 -0.028305   \n",
       "Bottom 1000 -0.215506  -0.034741 -0.046066 -0.145577 -0.005900 -0.005751   \n",
       "\n",
       "                   RF  GBRT + H       NN1       NN2       NN3       NN4  \\\n",
       "All          0.000526  0.201412 -0.278784 -0.420429 -0.260544 -0.036583   \n",
       "Top 1000     0.002884  0.136232 -1.449462 -1.488644 -2.641036 -0.080898   \n",
       "Bottom 1000  0.122885  0.184621 -0.190637 -0.226046 -0.150685 -0.016387   \n",
       "\n",
       "                  NN5  \n",
       "All         -0.068679  \n",
       "Top 1000    -0.245133  \n",
       "Bottom 1000 -0.037959  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "39c05295",
   "metadata": {},
   "outputs": [],
   "source": [
    "significo.to_excel(\"model_significance.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e6256892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLS</th>\n",
       "      <th>PCR</th>\n",
       "      <th>Enet + H</th>\n",
       "      <th>GLM + H</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBRT + H</th>\n",
       "      <th>NN1</th>\n",
       "      <th>NN2</th>\n",
       "      <th>NN3</th>\n",
       "      <th>NN4</th>\n",
       "      <th>NN5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pchgm_pchsale</th>\n",
       "      <td>-0.002017</td>\n",
       "      <td>-0.058591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_80</th>\n",
       "      <td>0.000681</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_55</th>\n",
       "      <td>0.000914</td>\n",
       "      <td>-0.010684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currat</th>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.036253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.011125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgr</th>\n",
       "      <td>-0.000459</td>\n",
       "      <td>0.052028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.0]</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_81</th>\n",
       "      <td>0.000663</td>\n",
       "      <td>-0.002312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.0]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ill</th>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.014122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>0.008632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sic_49</th>\n",
       "      <td>0.000419</td>\n",
       "      <td>-0.026146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.001054</td>\n",
       "      <td>-0.116730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pchsale_pchinvt</th>\n",
       "      <td>-0.001179</td>\n",
       "      <td>0.023227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.0]</td>\n",
       "      <td>0.026324</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      PLS       PCR  Enet + H GLM + H        RF  GBRT + H  \\\n",
       "pchgm_pchsale   -0.002017 -0.058591       0.0   [0.0]  0.000892  0.000000   \n",
       "sic_80           0.000681 -0.000182       0.0   [0.0]  0.000063  0.000000   \n",
       "sic_55           0.000914 -0.010684       0.0   [0.0]  0.000000  0.000000   \n",
       "currat           0.000368  0.036253       0.0   [0.0]  0.003436  0.011125   \n",
       "sgr             -0.000459  0.052028       0.0  [-0.0]  0.003557  0.001681   \n",
       "...                   ...       ...       ...     ...       ...       ...   \n",
       "sic_81           0.000663 -0.002312       0.0  [-0.0]  0.000000  0.000000   \n",
       "ill              0.004975  0.014122       0.0   [0.0]  0.007285  0.008632   \n",
       "sic_49           0.000419 -0.026146       0.0   [0.0]  0.000023  0.000000   \n",
       "age             -0.001054 -0.116730       0.0   [0.0]  0.000656  0.000000   \n",
       "pchsale_pchinvt -0.001179  0.023227       0.0  [-0.0]  0.026324  0.008610   \n",
       "\n",
       "                 NN1  NN2  NN3  NN4  NN5  \n",
       "pchgm_pchsale    0.0  0.0  0.0  0.0  0.0  \n",
       "sic_80           0.0  0.0  0.0  0.0  0.0  \n",
       "sic_55           0.0  0.0  0.0  0.0  0.0  \n",
       "currat           0.0  0.0  0.0  0.0  0.0  \n",
       "sgr              0.0  0.0  0.0  0.0  0.0  \n",
       "...              ...  ...  ...  ...  ...  \n",
       "sic_81           0.0  0.0  0.0  0.0  0.0  \n",
       "ill              0.0  0.0  0.0  0.0  0.0  \n",
       "sic_49           0.0  0.0  0.0  0.0  0.0  \n",
       "age              0.0  0.0  0.0  0.0  0.0  \n",
       "pchsale_pchinvt  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[169 rows x 11 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "69ba491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "moriano.to_excel(\"dieboldmariano_test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a61dbe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OLS-3 + H</th>\n",
       "      <th>PLS</th>\n",
       "      <th>PCR</th>\n",
       "      <th>Enet + H</th>\n",
       "      <th>GLM + H</th>\n",
       "      <th>RF</th>\n",
       "      <th>GBRT + H</th>\n",
       "      <th>NN1</th>\n",
       "      <th>NN2</th>\n",
       "      <th>NN3</th>\n",
       "      <th>NN4</th>\n",
       "      <th>NN5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OLS + H</th>\n",
       "      <td>44.667607</td>\n",
       "      <td>13.617104</td>\n",
       "      <td>50.423308</td>\n",
       "      <td>47.538610</td>\n",
       "      <td>47.408230</td>\n",
       "      <td>36.402181</td>\n",
       "      <td>21.641169</td>\n",
       "      <td>-11.697275</td>\n",
       "      <td>-49.387508</td>\n",
       "      <td>-0.363544</td>\n",
       "      <td>46.780575</td>\n",
       "      <td>49.806442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS-3 + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-37.000430</td>\n",
       "      <td>-6.857542</td>\n",
       "      <td>21.822947</td>\n",
       "      <td>24.353913</td>\n",
       "      <td>11.264323</td>\n",
       "      <td>13.505666</td>\n",
       "      <td>-56.849159</td>\n",
       "      <td>-80.963097</td>\n",
       "      <td>-31.158699</td>\n",
       "      <td>2.877297</td>\n",
       "      <td>-15.552996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLS</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.498729</td>\n",
       "      <td>41.597149</td>\n",
       "      <td>41.618567</td>\n",
       "      <td>33.649792</td>\n",
       "      <td>20.598327</td>\n",
       "      <td>-28.597974</td>\n",
       "      <td>-45.142408</td>\n",
       "      <td>-9.796310</td>\n",
       "      <td>39.354866</td>\n",
       "      <td>38.316115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.385630</td>\n",
       "      <td>21.651692</td>\n",
       "      <td>12.197810</td>\n",
       "      <td>13.569594</td>\n",
       "      <td>-66.263751</td>\n",
       "      <td>-86.115869</td>\n",
       "      <td>-34.302898</td>\n",
       "      <td>7.311327</td>\n",
       "      <td>-10.938444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enet + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.784107</td>\n",
       "      <td>4.617354</td>\n",
       "      <td>12.171884</td>\n",
       "      <td>-60.745200</td>\n",
       "      <td>-85.063556</td>\n",
       "      <td>-34.177638</td>\n",
       "      <td>-15.006216</td>\n",
       "      <td>-33.213926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLM + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.408721</td>\n",
       "      <td>12.032375</td>\n",
       "      <td>-60.085903</td>\n",
       "      <td>-84.415741</td>\n",
       "      <td>-34.179653</td>\n",
       "      <td>-17.615084</td>\n",
       "      <td>-33.122356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-44.734188</td>\n",
       "      <td>-65.644951</td>\n",
       "      <td>-29.271791</td>\n",
       "      <td>-10.572270</td>\n",
       "      <td>-16.893509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBRT + H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-23.371767</td>\n",
       "      <td>-30.534307</td>\n",
       "      <td>-20.389906</td>\n",
       "      <td>-13.148064</td>\n",
       "      <td>-14.411870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-47.033726</td>\n",
       "      <td>4.256386</td>\n",
       "      <td>59.759953</td>\n",
       "      <td>68.758141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.243170</td>\n",
       "      <td>85.851082</td>\n",
       "      <td>97.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.052517</td>\n",
       "      <td>31.102022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.031160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           OLS-3 + H        PLS        PCR   Enet + H    GLM + H         RF  \\\n",
       "OLS + H    44.667607  13.617104  50.423308  47.538610  47.408230  36.402181   \n",
       "OLS-3 + H   0.000000 -37.000430  -6.857542  21.822947  24.353913  11.264323   \n",
       "PLS         0.000000   0.000000  42.498729  41.597149  41.618567  33.649792   \n",
       "PCR         0.000000   0.000000   0.000000  20.385630  21.651692  12.197810   \n",
       "Enet + H    0.000000   0.000000   0.000000   0.000000  17.784107   4.617354   \n",
       "GLM + H     0.000000   0.000000   0.000000   0.000000   0.000000   3.408721   \n",
       "RF          0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "GBRT + H    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN1         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN2         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN3         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "NN4         0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "            GBRT + H        NN1        NN2        NN3        NN4        NN5  \n",
       "OLS + H    21.641169 -11.697275 -49.387508  -0.363544  46.780575  49.806442  \n",
       "OLS-3 + H  13.505666 -56.849159 -80.963097 -31.158699   2.877297 -15.552996  \n",
       "PLS        20.598327 -28.597974 -45.142408  -9.796310  39.354866  38.316115  \n",
       "PCR        13.569594 -66.263751 -86.115869 -34.302898   7.311327 -10.938444  \n",
       "Enet + H   12.171884 -60.745200 -85.063556 -34.177638 -15.006216 -33.213926  \n",
       "GLM + H    12.032375 -60.085903 -84.415741 -34.179653 -17.615084 -33.122356  \n",
       "RF          0.000000 -44.734188 -65.644951 -29.271791 -10.572270 -16.893509  \n",
       "GBRT + H    0.000000 -23.371767 -30.534307 -20.389906 -13.148064 -14.411870  \n",
       "NN1         0.000000   0.000000 -47.033726   4.256386  59.759953  68.758141  \n",
       "NN2         0.000000   0.000000   0.000000  27.243170  85.851082  97.941765  \n",
       "NN3         0.000000   0.000000   0.000000   0.000000  32.052517  31.102022  \n",
       "NN4         0.000000   0.000000   0.000000   0.000000   0.000000 -17.031160  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moriano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a11410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tursday Morning 2nd of May 8:00 a.m. => 10 p.m Wednesday my time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
